[
  {
    "objectID": "extra/logistic-regression-housing.html",
    "href": "extra/logistic-regression-housing.html",
    "title": "Logistic regression - house pricing",
    "section": "",
    "text": "What is logistic regression?\nLogistic regression is a tool that can be used for solving classification problems, in particular, predicting the probability of a binary outcome. For example:\n\nPredicting whether “it will rain or not” (yes/no)\n\nPredicting whether “a house is worth buying” (worth/not worth)\n\n\n\nWhen is logistic regression Suitable?\nLogistic regression is useful when you need to make a “two-choice” decision (though it can also be extended to more categories).\nIt’s suitable in scenarios like:\n\nClassification problems: Like predicting “will a student pass or fail?” or “does a patient have a disease?”\n\nQuantifiable features: When you have numerical inputs (like house size, income) to base your prediction on.\n\nNeed probabilities: When you want not just a “yes or no” answer, but also “how likely is it?”\n\nSome real-life examples are:\n\nPredicting “should I bring an umbrella today?”: Using temperature, humidity, and wind speed, logistic regression might tell you “70% chance of rain”.\n\nPredicting “is this house worth buying?”: Using size, location, and price, it might say “80% worth buying”.\n\nIn this notebook, we’ll use the California Housing dataset to try predicting whether a house in a certain area has a “high” or “low” price using logistic regression!\n\n\nPre-requisites\nBefore we dive into building our logistic regression model, we need to gather some tools—like setting up our workbench before starting a project! These tools (called “libraries” in programming) help us load data, do math, draw pictures, and build the model. Each library has a special job, and together they make our work easier.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n\n\n\nStep 1: Loading and Exploring the California Housing Dataset\n\nWhat Are We Doing Here?\nWe’re starting by loading the California Housing dataset, which tells us about small areas in California (not individual houses, but regions called “block groups”). We’ll peek at the data, set up our prediction task (high-priced or low-priced areas), and draw a picture to see how income relates to house prices.\n\n\nWhy This Step?\nWe need to understand our data before predicting anything—it’s like checking your ingredients before cooking! Here, we’ll turn house prices into a “yes or no” question: is this area high-priced?\n\n# load california housing dataset\ncalifornia = fetch_california_housing()\ndata = pd.DataFrame(california.data, columns=california.feature_names)\ndata['MedHouseVal'] = california.target  # old target is the MedHouseVal\n\n# Let us look at the head of the data\nprint(\"head of housing dataset：\")\nprint(data.head())\n\n# here we set a new target 'HIGH_PRICE' that is whether the price is higher than the median of all block groups' MedHouseVal, if higher, 1, otherwise, 0\nmedian_price = data['MedHouseVal'].median()\ndata['HIGH_PRICE'] = np.where(data['MedHouseVal'] &gt; median_price, 1, 0)\n\n# plt to see the relationship between MedInc and MedHouseVal\nplt.figure(figsize=(10, 6))\nplt.scatter(data['MedInc'], data['MedHouseVal'], c=data['HIGH_PRICE'], cmap='coolwarm', alpha=0.5)\nplt.xlabel('MedInc')\nplt.ylabel('MedHouseVal')\nplt.title('relationships between MedInc and MedHouseVal')\nplt.show()\n\nprint(\"red is High price house，blue is Low price house，we need to predict whether a house is high price or low price\")\n\nhead of housing dataset：\n   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n\n   Longitude  MedHouseVal  \n0    -122.23        4.526  \n1    -122.22        3.585  \n2    -122.24        3.521  \n3    -122.25        3.413  \n4    -122.25        3.422  \n\n\n\n\n\n\n\n\n\nred is High price house，blue is Low price house，we need to predict whether a house is high price or low price\n\n\n\n\n\nStep 2: A First Look at logistic regression with a Simple Example\n\nWhat Are We Doing Here?\nBefore using all the data, let’s try logistic regression with a tiny example. We’ll use just one piece of info—income—to predict if an area has high-priced houses. It’s a quick way to see how the model learns and makes guesses.\n\n\nWhy This Step?\nStarting small helps us understand how logistic regression works—like practicing with a toy car before driving a real one! We’ll see how it predicts “yes” or “no” based on income.\n\nprint(\"Logistic regression is a tool that help us predict true or false\")\nprint(\"eg: we want to predict whether a house is high price or low price according to the income\")\n\n# simple data：MedInc and HIGH_PRICE\nsimple_data = pd.DataFrame({\n    'MedInc': [2, 4, 6, 8],\n    'HIGH_PRICE': [0, 0, 1, 1]\n})\n\n# train a simple model\nsimple_model = LogisticRegression()\nsimple_model.fit(simple_data[['MedInc']], simple_data['HIGH_PRICE'])\n\n#predict\npredictions = simple_model.predict([[3], [7]])\nprint(f\"prediction of 3：{predictions[0]}（0=low-price，1=high-price）\")\nprint(f\"prediction of 7：{predictions[1]}\")\nprint(\"The model learned that the higher the income, the more likely housing prices will be!\")\n\nLogistic regression is a tool that help us predict true or false\neg: we want to predict whether a house is high price or low price according to the income\nprediction of 3：0（0=low-price，1=high-price）\nprediction of 7：1\nThe model learned that the higher the income, the more likely housing prices will be!\n\n\n/home/runner/work/data-analysis-python-2/data-analysis-python-2/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n  warnings.warn(\n\n\n\n\n\nStep 3: Preparing Data and Training the logistic regression Model\n\nWhat Are We Doing Here?\nNow we’ll get our data ready and teach our logistic regression model to make predictions. We’ll pick a few key features (like income and number of rooms), clean things up, and split the data into practice and test sets—like preparing flashcards for studying and saving some for a quiz.\n\n\nWhy This Step?\nData needs a bit of prep to work well with our model, just like washing veggies before cooking. Then we’ll train the model to spot patterns, like “high-income areas often have pricier houses,” and test it to see what it learned.\n\n# select some key features，like (MedInc)、(AveRooms)、(Population)\nX = data[['MedInc', 'AveRooms', 'Population']]\ny = data['HIGH_PRICE']\n\n# Standardize features (allow different ranges of numbers to be compared fairly)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Divide training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n\n# Train a logistic regression model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\nprint(\"Prediction results of the first 5 test samples：\", y_pred[:5])\nprint(\"Top 5 real results：\", y_test.values[:5])\n\nPrediction results of the first 5 test samples： [0 0 1 1 0]\nTop 5 real results： [0 0 1 1 1]\n\n\n\n\n\nStep 4: Checking How Well Our Model Did\n\nWhat Are We Doing Here?\nLet’s see how good our logistic regression model is! We’ll check how many predictions it got right, draw a “confusion matrix” to see where it messed up, and figure out which features (like income or rooms) mattered most to its decisions.\n\n\nWhy This Step?\nIt’s like grading a test — we want to know if our model learned well or if it needs more practice. This helps us understand what it’s good at and where it struggles.\n\n# Calculate model accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Model accuracy：{accuracy:.2f}（Proportion of correct predictions）\")\n\n# Draw confusion matrix\ncm = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['low price', 'high price'])\ndisp.plot(cmap='Blues')\nplt.title('matrix')\nplt.show()\n\n# See which features are important\nfeature_importance = pd.DataFrame({\n    'features': ['(MedInc)', '(AveRooms)', '(Population)'],\n    'weights': model.coef_[0]\n})\nprint(\"importance of features：\")\nprint(feature_importance)\n\nModel accuracy：0.76（Proportion of correct predictions）\n\n\n\n\n\n\n\n\n\nimportance of features：\n       features   weights\n0      (MedInc)  2.338169\n1    (AveRooms) -0.902615\n2  (Population) -0.091441\n\n\n\n\n\nStep 5: Ideas for Improving Our Model’s Accuracy\n\nWhat Are We Doing Here?\nOur model achieved an accuracy of 0.76, which means it got 76% of its predictions right—not bad for a start! We also saw the importance of features: MedInc (income) has a big positive weight (2.34), while AveRooms and Population have smaller or negative weights. But can we do better than 0.76? In this step, we’ll suggest some ideas for improving the model and let you try them out.\n\n\nWhy This Step?\nBuilding a model is just the beginning. Improving it is like tuning a guitar—you try different things to get the best sound. Here are some ideas to explore, and we’ll give you a starting point to experiment with!\n\n\nSuggestions for Improvement\nHere are a few ways you can try to boost the model’s accuracy. Pick one (or more!) and see what happens:\n\nAdd More Features: We only used MedInc, AveRooms, and Population. What if we include other features like HouseAge, Latitude, or Longitude? Maybe they’ll help the model learn more patterns.\nTune the Model’s Settings: logistic regression has a setting called C (it controls how strict the model is). The default is 1.0, but you could try smaller (like 0.1) or larger (like 10) values to see if it improves accuracy.\nAdjust the Decision Threshold: Right now, we predict “high price” if the probability is above 0.5 (default by LogisticRegression). What if we change it to 0.7 or 0.3? This might catch more high-priced areas (or fewer false positives).\n\n\n\nTry It Out!\nBelow is a basic code framework to get you started. Pick one idea, tweak the code, and see if the accuracy improves. Don’t be afraid to experiment!\n\n# Start experimenting with improving the model\nprint(\"Try one of the suggestions to improve the model!\")\n\n# Example 1: Add more features (uncomment and modify)\n# X = data[['MedInc', 'AveRooms', 'Population', 'HouseAge', 'Latitude', 'Longitude']]\n# X_scaled = scaler.fit_transform(X)\n# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n# model = LogisticRegression()\n# model.fit(X_train, y_train)\n# y_pred = model.predict(X_test)\n# accuracy = accuracy_score(y_test, y_pred)\n# print(f\"Accuracy with more features: {accuracy:.2f}\")\n\n# Example 2: Tune the C parameter (uncomment and modify)\n# C = 0.1\n# model = LogisticRegression(C=C)  # Try different values like 0.1, 10, etc.\n# model.fit(X_train, y_train)\n# y_pred = model.predict(X_test)\n# accuracy = accuracy_score(y_test, y_pred)\n# print(f\"Accuracy with new C={C}: {accuracy:.2f}\")\n\n# Example 3: Adjust the decision threshold (uncomment and modify)\n# y_prob = model.predict_proba(X_test)[:, 1]  # Get probabilities for class 1\n# threshold = 0.7  # Try 0.7 or 0.3\n# y_pred_custom = (y_prob &gt; threshold).astype(int)\n# accuracy_custom = accuracy_score(y_test, y_pred_custom)\n# print(f\"Accuracy with threshold {threshold}: {accuracy_custom:.2f}\")\n\n# Add more experiments as you like!\nprint(\"Pick an idea, tweak the code, and see if you can beat the original accuracy of 0.76!\")\n\nTry one of the suggestions to improve the model!\nPick an idea, tweak the code, and see if you can beat the original accuracy of 0.76!",
    "crumbs": [
      "Discussion",
      "Logistic regression - house pricing"
    ]
  },
  {
    "objectID": "extra/feature-importance-analysis.html",
    "href": "extra/feature-importance-analysis.html",
    "title": "Feature Importance Analysis",
    "section": "",
    "text": "Introduction\nFeature importance analysis is used to understand the usefulness or value of each feature in making predictions. The goal is to identify the most influential features that have the greatest impact on the model’s output. Feature Importance Analysis is widely used in machine learning to enhance model interpretability and performance.\n\n\nWhy is Feature Importance Analysis Important?\nIn a dataset with dozens or even hundreds of features, each feature may contribute to the performance of your machine learning model. However, not all features are equally valuable. Some may be redundant or irrelevant, increasing the complexity of the model and potentially leading to overfitting. Feature importance analysis helps identify and prioritize the most influential features, improving model efficiency, interpretability, and generalization.\nFeature importance analysis helps identify and focus on the most informative features, leading to several key advantages:\n\nImproved Model Performance. By selecting the most relevant features, the model can make more accurate predictions. Removing irrelevant or redundant features helps the model learn more effectively and generalize better to unseen data.\nReduced Overfitting. Using too many features, especially those that are not informative, can cause the model to learn noise rather than meaningful patterns. Feature importance analysis helps eliminate unnecessary features, reducing the risk of overfitting and improving the model’s ability to perform well on new data.\nFaster Training and Inference. With fewer but more relevant features, the computational cost of training the model decreases. This leads to faster model training and inference times, making it more efficient for large-scale applications.\nEnhanced Interpretability. Understanding which features contribute the most to predictions allows data scientists and domain experts to interpret model decisions more easily. This is particularly important in high-stakes applications like healthcare and finance, where model transparency is crucial.\n\nNow, let’s explore some common methods for feature importance analysis in scikit-learn.\n\n\nFeature Importance Analysis Methods\nLet’s explore Breast cancer wisconsin (diagnostic) dataset. This dataset includes information from 569 instances, with 30 numeric, predictive attributes such as radius, texture, perimeter, etc. Each sample is also labeled with a class that indicates the diagnosis, which can either be malignant (M) or benign (B). This dataset is commonly used for classification tasks, where the goal is to predict whether a tumor is malignant or benign based on the attributes.\n\nfrom sklearn.datasets import load_breast_cancer\n\nX, y = load_breast_cancer (as_frame=True, return_X_y=True)\ny = y.astype(\"category\")\nX\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst radius\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.30010\n0.14710\n0.2419\n0.07871\n...\n25.380\n17.33\n184.60\n2019.0\n0.16220\n0.66560\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.08690\n0.07017\n0.1812\n0.05667\n...\n24.990\n23.41\n158.80\n1956.0\n0.12380\n0.18660\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.19740\n0.12790\n0.2069\n0.05999\n...\n23.570\n25.53\n152.50\n1709.0\n0.14440\n0.42450\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.24140\n0.10520\n0.2597\n0.09744\n...\n14.910\n26.50\n98.87\n567.7\n0.20980\n0.86630\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.19800\n0.10430\n0.1809\n0.05883\n...\n22.540\n16.67\n152.20\n1575.0\n0.13740\n0.20500\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n564\n21.56\n22.39\n142.00\n1479.0\n0.11100\n0.11590\n0.24390\n0.13890\n0.1726\n0.05623\n...\n25.450\n26.40\n166.10\n2027.0\n0.14100\n0.21130\n0.4107\n0.2216\n0.2060\n0.07115\n\n\n565\n20.13\n28.25\n131.20\n1261.0\n0.09780\n0.10340\n0.14400\n0.09791\n0.1752\n0.05533\n...\n23.690\n38.25\n155.00\n1731.0\n0.11660\n0.19220\n0.3215\n0.1628\n0.2572\n0.06637\n\n\n566\n16.60\n28.08\n108.30\n858.1\n0.08455\n0.10230\n0.09251\n0.05302\n0.1590\n0.05648\n...\n18.980\n34.12\n126.70\n1124.0\n0.11390\n0.30940\n0.3403\n0.1418\n0.2218\n0.07820\n\n\n567\n20.60\n29.33\n140.10\n1265.0\n0.11780\n0.27700\n0.35140\n0.15200\n0.2397\n0.07016\n...\n25.740\n39.42\n184.60\n1821.0\n0.16500\n0.86810\n0.9387\n0.2650\n0.4087\n0.12400\n\n\n568\n7.76\n24.54\n47.92\n181.0\n0.05263\n0.04362\n0.00000\n0.00000\n0.1587\n0.05884\n...\n9.456\n30.37\n59.16\n268.6\n0.08996\n0.06444\n0.0000\n0.0000\n0.2871\n0.07039\n\n\n\n\n569 rows × 30 columns\n\n\n\nLet’s get the attribute names.\n\nfrom sklearn.datasets import load_breast_cancer\n\n# load dataset\nX, y = load_breast_cancer (as_frame=True, return_X_y=True)\n\n# get attribute information\ncolumn_names = X.columns\nfor index, column_name in enumerate(column_names):\n  print(index, column_name)\n\n0 mean radius\n1 mean texture\n2 mean perimeter\n3 mean area\n4 mean smoothness\n5 mean compactness\n6 mean concavity\n7 mean concave points\n8 mean symmetry\n9 mean fractal dimension\n10 radius error\n11 texture error\n12 perimeter error\n13 area error\n14 smoothness error\n15 compactness error\n16 concavity error\n17 concave points error\n18 symmetry error\n19 fractal dimension error\n20 worst radius\n21 worst texture\n22 worst perimeter\n23 worst area\n24 worst smoothness\n25 worst compactness\n26 worst concavity\n27 worst concave points\n28 worst symmetry\n29 worst fractal dimension\n\n\n\n\nBuilt-in Feature Importance\nMachine learning models such as linear regression and random forests have built-in capabilities to directly output feature importance scores. These scores provide insight into the contribution of each feature to the final prediction made by the model. Understanding feature importance is crucial, as it helps identify which input variables are the most influential in determining the model’s output.\nFor example, Random Forests Model evaluates how much each feature contributes to reducing the error in the model’s predictions. The attribute feature_importances_ can be used to access this information. In Random Forests, features that lead to larger reductions in prediction error across the trees are assigned higher importance scores. The importance score for a feature is typically calculated by aggregating the reduction in impurity (e.g., Gini impurity or entropy) across all the trees in the forest, weighted by the number of samples each tree uses.\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt\n\n# load dataset\nX, y = load_breast_cancer(return_X_y=True)\n\n# train model\nrf = RandomForestClassifier(n_estimators=100, random_state=1)\nrf.fit(X, y)\n\n# get feature importances\nimportances = rf.feature_importances_\n\n# plot importances\nplt.bar(range(X.shape[1]), importances)\nplt.xlabel('Feature Index')\nplt.ylabel('Feature Importance')\nplt.show()\n\n\n\n\n\n\n\n\nFor features with high feature importance, it indicates that the feature is very useful in distinguishing between malignant and benign tumors, and the model relies on it to make predictions.\nFor features with low feature importance, it suggests that the feature has a minimal impact on the model’s predictions. This feature may not contribute significantly to distinguishing the target variable (whether the tumor is malignant or benign).\n\n\nRecursive Feature Elimination (RFE)\nRecursive Feature Elimination (RFE) is a feature selection method that recursively removes features and evaluates the impact on the performance of the model. The process works by fitting the model multiple times and progressively eliminating the least important features based on a certain criterion, such as the feature’s weight or coefficient. By repeatedly evaluating the model’s performance, RFE identifies which features contribute the most to the model’s predictive power.\nIn sklearn, RFE can be implemented using the RFE class. This function allows users to specify the estimator (such as a linear regression or decision tree model) that will be used to evaluate the importance of each feature. The n_features_to_select parameter specifies how many features to keep, and the process will stop once the specified number of features is reached.\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import RFE\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# load dataset\nX, y = load_breast_cancer(as_frame=True,return_X_y=True)\n\n# train model\nrf = RandomForestClassifier()\nrfe = RFE(rf, n_features_to_select=10)\nrfe.fit(X, y)\n\n# print importance\nprint(rfe.ranking_)\n\n[ 3  4  1  1 10 11  1  1 19 20  6 13 12  2 17 15 21 16 18 14  1  1  1  1\n  5  8  1  1  7  9]\n\n\nFor the output array, each element corresponds to the ranking of the respective feature, with a higher rank indicating greater feature importance.\n\n\nPermutation Importance\nPermutation Importance evaluates the importance of each feature by randomly shuffling its values and observing how the model’s performance decreases. This method works by permuting (randomizing) the values of each feature one at a time and measuring the impact on the model’s accuracy.\nIn sklearn, the permutation_importance function can be used to compute permutation importance scores. This function takes the fitted model, the validation data, and the target values as inputs and returns the importance scores for each feature based on how much the model’s performance drops when the feature is permuted.\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\n# load dataset\ncancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=1)\n\n# train model\nrf = RandomForestClassifier(n_estimators=100, random_state=1)\nrf.fit(X_train, y_train)\n\n# get permutation importance\nresult = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=1, scoring='accuracy')\nimportances = result.importances_mean\n\n# plot importances\nplt.bar(range(len(importances)), importances)\nplt.xlabel('Feature Index')\nplt.ylabel('Permutation Importance')\nplt.show()\n\n\n\n\n\n\n\n\nA feature with high permutation importance indicates that when its values are randomly shuffled, the model’s performance (e.g., accuracy, mean squared error) drops significantly. This suggests that the feature contains crucial information that strongly influences the target variable.\nA feature with low permutation importance means that when its values are randomly shuffled, the model’s performance remains nearly unchanged. This implies that the feature may be irrelevant, redundant, or contain too much noise. Such features can be considered for removal or further analysis to assess their necessity.\n\n\nAnalysis of Variance (ANOVA)\nAnalysis of Variance (ANOVA) is a statistical method used to analyze the differences among group means and their associated variances. In the context of feature importance analysis, ANOVA is used to determine how strongly each feature is related to the target variable.\nIn sklearn, the f_classif() function is commonly used to perform ANOVA for classification tasks. It calculates the F-statistic for each feature, which measures the ratio of variance between groups to the variance within the groups.\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.feature_selection import f_classif\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# load dataset\nX, y = load_breast_cancer(as_frame=True,return_X_y=True)\n\n# ANOVA\ndf = pd.DataFrame(X, columns=range(len(X.columns)))\ndf['y'] = y\nfval = f_classif(X, y)\nfval = pd.Series(fval[0], index=range(X.shape[1]))\n\n# plot importances\nplt.bar(range(X.shape[1]), fval)\nplt.xlabel('Feature Index')\nplt.ylabel('F-value')\nplt.show()\n\n\n\n\n\n\n\n\nA feature with a higher F-value indicates that it provides more information about the target variable. It suggests that the feature is more discriminative in separating different classes.\nA low F-value means that the feature is not very useful for distinguishing between the target variable’s categories, suggesting a weaker relationship.\n\n\nChi-Square Test\nThe Chi-Square Test is a statistical method used to assess whether there is a significant relationship between two categorical variables. In the context of feature importance analysis, the Chi-Square test is commonly used to evaluate the independence of each feature from the target variable.\nIn sklearn, the chi2() function is used to perform the Chi-Square test for feature selection. It calculates the Chi-Square statistic for each feature, which measures how much the observed frequency of feature categories deviates from the expected frequency if the feature and target were independent.\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.feature_selection import chi2\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# load dataset\nX, y = load_breast_cancer(as_frame=True,return_X_y=True)\n\n# chi-2\ndf = pd.DataFrame(X, columns=range(len(X.columns)))\ndf['y'] = y\nchi_scores = chi2(X, y)\nchi_scores = pd.Series(chi_scores[0], index=range(X.shape[1]))\n\n# plot importances\nplt.bar(range(X.shape[1]), chi_scores)\nplt.xlabel('Feature Index')\nplt.ylabel('chi-2')\nplt.show()\n\n\n\n\n\n\n\n\nA feature with a high Chi-Square statistic suggests that the feature is dependent on the target variable. This means the feature has a significant relationship with the target and is likely useful for prediction.\nA low Chi-Square statistic indicates that the feature is independent of the target variable, meaning it does not provide useful information for prediction.",
    "crumbs": [
      "Discussion",
      "Feature Importance Analysis"
    ]
  },
  {
    "objectID": "extra/logistic-regression-cancer.html",
    "href": "extra/logistic-regression-cancer.html",
    "title": "Logistic regression - breast cancer classification",
    "section": "",
    "text": "Let us explore another regression model and apply that to the breast cancer dataset from sklearn. But first let us import the necessary libraries we need for this exercise.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nBreast Cancer Dataset\nThe dataset contains 569 samples with 30 features derived from the images of breast mass cells. Each sample is labeled as either malignant (0) or benign (1).\nFor each feature, the dataset provides the mean, standard error, and “worst” (mean of the three largest values) measurements, resulting in 30 total features.\n\n\nLogistic Regression\nLogistic regression functions as a supervised machine learning algorithm specifically designed for classification problems, commonly of a binary outcome (true/false, yes/no, 0/1). The primary purpose is to estimate the likelihood of an instance belonging to a particular class.\n\nLogistic Function\nThe core function used in logistic regression is the sigmoid function:\n\\[\\sigma(z)=\\frac{1}{1+e^{-z}}\\]\nThe logistic regression model calculates a linear combination of the \\(n\\) input features: \\(z = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n\\), where \\(w_0\\) is the intercept and \\(w_1 \\dots w_n\\) are regression coefficients multiplied by some predictor values. It applies the sigmoid function to map this value to a probability: \\(P(y=1|x) = \\sigma(z)\\) and classifies an instance as positive (1) if the probability is greater than 0.5, and negative (0) otherwise.\n\n\nAdvantages of Logistic Regression for Medical Diagnostics\nLogistic regression is particularly useful for medical applications because:\n\nInterpretability: The coefficients tell us how each feature influences the prediction.\nProbabilistic output: Rather than just providing a binary prediction, it gives a probability score that can be used to assess confidence in the diagnosis.\nEfficiency: It works well with limited data and is computationally efficient.\nRegularization: This can be regularized to prevent overfitting, especially important when dealing with many features.\n\n\n\n\nLoad dataset\nLet us now load the dataset from sklearn along with some important in-built functions for analysing and training our model.\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n\n\nbreast_cancer = load_breast_cancer()\nX = breast_cancer.data\ny = breast_cancer.target\nfeature_names = breast_cancer.feature_names\ntarget_names = breast_cancer.target_names\n\n# print to check the overall structure of our dataset\n# and also to find how many classes we have\n\nprint(f\"Dataset dimensions: {X.shape}\")\nprint(f\"Target classes: {target_names}\")\n\nDataset dimensions: (569, 30)\nTarget classes: ['malignant' 'benign']\n\n\nThis shows that our dataset contains \\(569\\) samples and \\(30\\) features (rows and columns, respectively).\nThe target class is malignant and benign, which means our goal should be to predict whether a sample is malignant or benign.\n\n\nData Visualisation\nNow that we have loaded our dataset let us inspect how they look like.\n\ndf = pd.DataFrame(X, columns=feature_names)\ndf['target'] = y\ndf['diagnosis'] = df['target'].map({0: 'malignant', 1: 'benign'})\n\nprint(\"\\nData overview:\")\nprint(df.head())\n\nprint(\"\\nTarget distribution:\")\nprint(df['diagnosis'].value_counts())\n\n\nData overview:\n   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n0        17.99         10.38          122.80     1001.0          0.11840   \n1        20.57         17.77          132.90     1326.0          0.08474   \n2        19.69         21.25          130.00     1203.0          0.10960   \n3        11.42         20.38           77.58      386.1          0.14250   \n4        20.29         14.34          135.10     1297.0          0.10030   \n\n   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n0           0.27760          0.3001              0.14710         0.2419   \n1           0.07864          0.0869              0.07017         0.1812   \n2           0.15990          0.1974              0.12790         0.2069   \n3           0.28390          0.2414              0.10520         0.2597   \n4           0.13280          0.1980              0.10430         0.1809   \n\n   mean fractal dimension  ...  worst perimeter  worst area  worst smoothness  \\\n0                 0.07871  ...           184.60      2019.0            0.1622   \n1                 0.05667  ...           158.80      1956.0            0.1238   \n2                 0.05999  ...           152.50      1709.0            0.1444   \n3                 0.09744  ...            98.87       567.7            0.2098   \n4                 0.05883  ...           152.20      1575.0            0.1374   \n\n   worst compactness  worst concavity  worst concave points  worst symmetry  \\\n0             0.6656           0.7119                0.2654          0.4601   \n1             0.1866           0.2416                0.1860          0.2750   \n2             0.4245           0.4504                0.2430          0.3613   \n3             0.8663           0.6869                0.2575          0.6638   \n4             0.2050           0.4000                0.1625          0.2364   \n\n   worst fractal dimension  target  diagnosis  \n0                  0.11890       0  malignant  \n1                  0.08902       0  malignant  \n2                  0.08758       0  malignant  \n3                  0.17300       0  malignant  \n4                  0.07678       0  malignant  \n\n[5 rows x 32 columns]\n\nTarget distribution:\ndiagnosis\nbenign       357\nmalignant    212\nName: count, dtype: int64\n\n\n\nplt.figure(figsize=(10, 6))\nsns.countplot(x='diagnosis', data=df)\nplt.title('Distribution of Diagnosis Classes')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSplitting dataset for training\nTo train the dataset, first we need to split it into two parts: training dataset and testing/validating dataset.\nWe can do so by using train_test_split and defining the size of test dataset as well as the random state to shuffle the dataset.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\nprint(f\"Training set shape: {X_train.shape}\")\nprint(f\"Testing set shape: {X_test.shape}\")\n\n# check feature distributions by class\nplt.figure(figsize=(15, 10))\nfeatures_to_plot = ['mean radius', 'mean texture', 'mean perimeter']\nfor i, feature in enumerate(features_to_plot):\n    plt.subplot(2, 3, i+1)\n    sns.boxplot(x='target', y=feature, data=df)\n    plt.title(f'{feature} by Class')\n    plt.xlabel('Class (0: Malignant, 1: Benign)')\nplt.tight_layout()\nplt.show()\n\nTraining set shape: (426, 30)\nTesting set shape: (143, 30)\n\n\n\n\n\n\n\n\n\nTo verify if the training and testing data were split correctly (\\(75:25\\) ratio), you can check is \\(143\\) is \\(25\\%\\) of \\(569\\).\nAnother approach to understand the relationship between features and tumour classifications (malignant vs. benign) is to visualise the means of each feature across both classes.\nIf you’d like to explore additional features, you can refer to the complete feature set available in the Wisconsin Breast Cancer Diagnostic dataset.\n\n\nScaling the dataset and fitting the model\nNow that we have our dataset ready for training, let us fit the logistic regression model.\n\n# initialise the scaler\n# this is basically pre-processing of the data to standardise the features\nscaler = StandardScaler()\n\n# fit the scaler on training data\nscaler.fit(X_train)\n\n# transform both training and test data\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# fit the model and initialise and train a basic logistic regression model\nmodel = LogisticRegression(random_state=42, max_iter=1000)\nmodel.fit(X_train_scaled, y_train)\n\n# evaluate the basic model\ny_pred = model.predict(X_test_scaled)\nprint(\"\\nLogistic Regression Results:\")\nprint(classification_report(y_test, y_pred, target_names=['Malignant', 'Benign']))\n\n\nLogistic Regression Results:\n              precision    recall  f1-score   support\n\n   Malignant       0.96      0.98      0.97        54\n      Benign       0.99      0.98      0.98        89\n\n    accuracy                           0.98       143\n   macro avg       0.98      0.98      0.98       143\nweighted avg       0.98      0.98      0.98       143\n\n\n\nHere, we see four classification matrices of the model: precision, recall, f1-score, and support.\n\nPrecision: Measures the accuracy of positive predictions. It is the ratio of true positives to all predicted positives. Here, a precision of \\(0.96\\) for malignant means \\(96\\%\\) of tumors predicted as malignant were actually malignant.\nRecall: It is the ratio of true positives to all actual positives. A recall of \\(0.98\\) for malignant means the model identified \\(98\\%\\) of all malignant tumors.\nF1-Score: The harmonic mean of precision and recall provides a balance between these two sometimes competing metrics. An F1-score of \\(0.97\\) for malignant indicates an excellent balance between identifying positive cases and avoiding false alarms.\nSupport: The actual number of samples in each class within the dataset being evaluated. The support values (\\(54\\) malignant, \\(89\\) benign) provide context for interpreting the other metrics and indicate the relative frequency of each class in the test data.\n\n\n\nCross Validation\nCross-validation helps us estimate how well our model will generalise to new data by the following measures:\n\nDividing the training data into multiple subsets (folds)\nTraining and testing/validating the model on different combinations of these folds\nAveraging the results to get a more reliable performance estimate\n\n\ncv_scores = cross_val_score(LogisticRegression(random_state=42, max_iter=1000), \n                           X_train_scaled, y_train, cv=5, scoring='accuracy')\nprint(f\"Cross-validation scores: {cv_scores}\")\nprint(f\"Mean CV accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n\nCross-validation scores: [0.98837209 0.96470588 1.         0.96470588 0.95294118]\nMean CV accuracy: 0.9741 ± 0.0173\n\n\nAs we had 5-fold cross-validation, we see \\(5\\) different scores in each fold. And the average accuracy is around \\(97\\%\\).\n\n\nVisualise Model Performance\nA confusion matrix is a performance evaluation tool that provides a detailed breakdown of correct and incorrect predictions for each class, allowing you to assess the performance of your classification model. The rows represent the actual classes the outcomes should have been. While the columns represent the predictions we have made. Using this table it is easy to see which predictions are wrong.\n\nplt.figure(figsize=(8, 6))\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['Malignant', 'Benign'],\n            yticklabels=['Malignant', 'Benign'])\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\n\n\n\n\n\n\n\n\nFrom this confusion matrix, we learn how the binary classification model predicts cases of malignant and benign.\nIn true positive category we have \\(53\\) cases as malignant while \\(1\\) malignant case was incorrectly classified as benign. But we don’t have any false positive or true negative predictions.",
    "crumbs": [
      "Discussion",
      "Logistic regression - breast cancer classification"
    ]
  },
  {
    "objectID": "pages/aside_non_linear_regression.html",
    "href": "pages/aside_non_linear_regression.html",
    "title": "Introduction to Data Anlysis in Python (Part 2)",
    "section": "",
    "text": "Non-linear regression\nSince we’re fitting a straight line, it may seem impossible for this technique to be able to correctly fit more complicated relationships. In general that is true but scikit-learn provides preprocessing tools which can automatically transform your data into something which can be understood linearly.\nLet’s start by making some definitely non-linear data from a sine curve:\nimport numpy as np\nfrom pandas import DataFrame\n\nrng = np.random.RandomState(42)\n\nnumber_of_points = 50\nx_scale = 10\n\nx = x_scale * rng.rand(number_of_points)\ny = np.sin(x) + 0.1 * rng.normal(size=number_of_points)\n\nsin_data = DataFrame({\"x\": x, \"y\": y})\nsin_data.plot.scatter(\"x\", \"y\")\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb3b56fe580&gt;\n\nOf course a linear fit will not work for this data (actually not true, a linear fit will give an answer for this data without trouble, it’s just that the fit will be useless) so let’s allow the data to be transformed by a polynomial basis-function before the linear regression. We put these together into a pipeline using scikit-learns’s make_pipeline() function. This output of this function can be used in the same way as the standard LinearRegression model.\nLet’s make a pipeline which first applies a 7th-order polynomial and then fits it with a linear regression:\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly_model = make_pipeline(PolynomialFeatures(7), LinearRegression())\nThis poly_model can then be used like the plain LinearRegression model before and have the fit() method called on it.\npoly_model.fit(sin_data[[\"x\"]], sin_data[\"y\"])\n\nxfit = np.linspace(0, x_scale)\nyfit = poly_model.predict(xfit[:, np.newaxis])\n\nax = sin_data.plot.scatter(\"x\", \"y\")\nax.plot(xfit, yfit, linestyle=\":\")\n[&lt;matplotlib.lines.Line2D at 0x7fb3a5bd2f70&gt;]\n\nYou can think of the combined “transform and linear fit” as being a single fit with a 7th-order polynomial.\n\nExercise\n\nChange the order of the polynomial. When does the data start to fit well. What happens when you make the polynomial order very large?"
  },
  {
    "objectID": "pages/091-loading-data.html",
    "href": "pages/091-loading-data.html",
    "title": "Loading your data",
    "section": "",
    "text": "In this section…\n\n\n\nThis are the key points of the section:\n- Pandas and Seaborn\n- Exploratory data analysis with correlation\nFirst we shall need some data. For this example we shall be pretending that we have measured these data from an experiment and we want to extract the underlying parameters of the system that generated them.\nI have prepared a CSV file which you can read into pandas using read_csv:\nx\ny\n\n\n\n\n0\n3.745401\n3.229269\n\n\n1\n9.507143\n14.185654\n\n\n2\n7.319939\n9.524231\n\n\n3\n5.986585\n6.672066\n\n\n4\n1.560186\n-3.358149\nWe can see here that the data has two columns, x and y. Traditionally, \\(x\\) is used for the things we can easily measure in the world and use as inputs, and \\(y\\) is used for the thing we want to predict. In our case, we want to work out what the value of \\(y\\) should be for any given \\(x\\).\nIn this case we have one single \\(x\\) column but in a more complicated data set we may have multiple \\(x_1\\), \\(x_2\\) etc. in which case the set together is sometimes given the upper-case letter \\(X\\). Each \\(x_n\\) column is called a feature. Features are usually things that you have measured as part of a experiment (e.g. height of a person, temperature of a room, size of a garden etc.).\nOur \\(y\\) column is the thing that we are going to create a model to predict the value of. The \\(y\\) column of the input data is often called the label or the target.\nLet’s check how many rows we have:\ndata.count()\n\nx    50\ny    50\ndtype: int64\nWe have 50 rows here. In the input data, each row is often called a sample (though sometimes also called an instance, example or observation). For example, it could be the information about a single person from a census or the measurements at a particular time from a weather station.\nLet’s have a look at what the data looks like when plotted:\nimport seaborn as sns\n\nsns.relplot(data=data, x=\"x\", y=\"y\")\nWe can clearly visually see here that there is a linear relationship between the \\(x\\) and \\(y\\) values but we need to be able to extract the exact parameters programmatically.",
    "crumbs": [
      "Working framework",
      "Loading your data"
    ]
  },
  {
    "objectID": "pages/091-loading-data.html#exploratory-data-analysis-using-correlation",
    "href": "pages/091-loading-data.html#exploratory-data-analysis-using-correlation",
    "title": "Loading your data",
    "section": "Exploratory data analysis using Correlation",
    "text": "Exploratory data analysis using Correlation\nWhen presented with a new collection of data, one of the first questions you may ask is how they are related to each other. This can involve deep study of how one parameter is likely to vary as you change another but the simplest start is to look a the linear correlation between them.\nCorrelation is usually taught as being the degree to which two variables are linearly related, that is as one increases, on average how much does the other one increase. This is a useful measure because it’s easy to calculate and most data only have either linear relationships or no relationship at all.\n\nHowever, correlation is a much broader idea than that and when doing machine learning, it’s worth understanding the bigger picture. At its core, correlation is a measure of how related two data sets are. The way I like to think of it is, if I know the value of one of the two ariables, how much information do I have about the value of the other.\nTo highlight this, consider the following two variables, \\(x\\) and \\(y\\):\n\nThey have a linear correlation of zero (on average as \\(x\\) increases, \\(y\\) stays the same) but if you know the value of \\(X\\), you clearly have information about what the value of \\(y\\) is likely to be.\nThe other way to think about it is in terms of mutual information. \\(y\\) is clearly sharing information with \\(x\\), otherwise there would be no visible pattern.\n\nMultiple cross-correlation\nIt’s very common when working on real data that you have more than two figures of interest.\nTo get a sense of some real data, let’s look at a housing dataset provided by scikit-learn.\n\nfrom sklearn.datasets import fetch_california_housing\n\nhousing, target = fetch_california_housing(as_frame=True, return_X_y=True)\n\n\nhousing.head()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n\n\n\n\n\n\n\nIt has a row for each census block and a column for each feature, e.g. “median income of the block”, “average house age of the block” etc.\nTo get the linear correlation between all these features, we call the corr() method on the DataFrame:\n\nhousing.corr()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\n\n\n\n\nMedInc\n1.000000\n-0.119034\n0.326895\n-0.062040\n0.004834\n0.018766\n-0.079809\n-0.015176\n\n\nHouseAge\n-0.119034\n1.000000\n-0.153277\n-0.077747\n-0.296244\n0.013191\n0.011173\n-0.108197\n\n\nAveRooms\n0.326895\n-0.153277\n1.000000\n0.847621\n-0.072213\n-0.004852\n0.106389\n-0.027540\n\n\nAveBedrms\n-0.062040\n-0.077747\n0.847621\n1.000000\n-0.066197\n-0.006181\n0.069721\n0.013344\n\n\nPopulation\n0.004834\n-0.296244\n-0.072213\n-0.066197\n1.000000\n0.069863\n-0.108785\n0.099773\n\n\nAveOccup\n0.018766\n0.013191\n-0.004852\n-0.006181\n0.069863\n1.000000\n0.002366\n0.002476\n\n\nLatitude\n-0.079809\n0.011173\n0.106389\n0.069721\n-0.108785\n0.002366\n1.000000\n-0.924664\n\n\nLongitude\n-0.015176\n-0.108197\n-0.027540\n0.013344\n0.099773\n0.002476\n-0.924664\n1.000000\n\n\n\n\n\n\n\nHere we see the features in our data set along both the rows and the columns. The correlation between each pair is given as a number between -1.0 and 1.0 where -1.0 is absolute inverse linear correlation, 1.0 is absolute positive linear correlation and zero is no linear correlation.\nWe see the the 1.0 occuring on the diagonal (because a variable is always completely correlated with itself) and a whole range of values between -1.0 and 1.0 off-diagonal.\nIf we want the correlation between two specific columns then we can request it from this object:\n\ncorr = housing.corr()\ncorr[\"MedInc\"][\"AveRooms\"]\n\nnp.float64(0.32689543164129786)\n\n\n\n\n\n\n\n\nExercise\n\n\n\nLook through the table manually and see if you can find the most negative and most positive correlations.\nBonus: Try to automate that search using Python code. - Hint: To find the minimum, use the min() and idxmin() methods. To find the maximum, hide the diagonals first using np.fill_diagonal(corr.values, np.nan)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nLet’s find the most negative and the most positive (ignoring self-correlation) values\nfrom pandas import DataFrame\nfrom sklearn.datasets import fetch_california_housing\n\nhousing_data = fetch_california_housing()\nhousing = DataFrame(housing_data.data, columns=housing_data.feature_names)\n\ncorr = housing.corr()\n\ncorr\n\n\n\n\n\n\n\n\nMedInc\n\n\nHouseAge\n\n\nAveRooms\n\n\nAveBedrms\n\n\nPopulation\n\n\nAveOccup\n\n\nLatitude\n\n\nLongitude\n\n\n\n\n\n\nMedInc\n\n\n1.000000\n\n\n-0.119034\n\n\n0.326895\n\n\n-0.062040\n\n\n0.004834\n\n\n0.018766\n\n\n-0.079809\n\n\n-0.015176\n\n\n\n\nHouseAge\n\n\n-0.119034\n\n\n1.000000\n\n\n-0.153277\n\n\n-0.077747\n\n\n-0.296244\n\n\n0.013191\n\n\n0.011173\n\n\n-0.108197\n\n\n\n\nAveRooms\n\n\n0.326895\n\n\n-0.153277\n\n\n1.000000\n\n\n0.847621\n\n\n-0.072213\n\n\n-0.004852\n\n\n0.106389\n\n\n-0.027540\n\n\n\n\nAveBedrms\n\n\n-0.062040\n\n\n-0.077747\n\n\n0.847621\n\n\n1.000000\n\n\n-0.066197\n\n\n-0.006181\n\n\n0.069721\n\n\n0.013344\n\n\n\n\nPopulation\n\n\n0.004834\n\n\n-0.296244\n\n\n-0.072213\n\n\n-0.066197\n\n\n1.000000\n\n\n0.069863\n\n\n-0.108785\n\n\n0.099773\n\n\n\n\nAveOccup\n\n\n0.018766\n\n\n0.013191\n\n\n-0.004852\n\n\n-0.006181\n\n\n0.069863\n\n\n1.000000\n\n\n0.002366\n\n\n0.002476\n\n\n\n\nLatitude\n\n\n-0.079809\n\n\n0.011173\n\n\n0.106389\n\n\n0.069721\n\n\n-0.108785\n\n\n0.002366\n\n\n1.000000\n\n\n-0.924664\n\n\n\n\nLongitude\n\n\n-0.015176\n\n\n-0.108197\n\n\n-0.027540\n\n\n0.013344\n\n\n0.099773\n\n\n0.002476\n\n\n-0.924664\n\n\n1.000000\n\n\n\n\n\n\nMost negative correlation\nFind the most negative correlation for each column:\ncorr.min()\nMedInc       -0.119034\nHouseAge     -0.296244\nAveRooms     -0.153277\nAveBedrms    -0.077747\nPopulation   -0.296244\nAveOccup     -0.006181\nLatitude     -0.924664\nLongitude    -0.924664\ndtype: float64\nFind the column which has the lowest correlation:\ncorr.min().idxmin()\n'Latitude'\nExtract the Latitude column and get the index of the most negative value in it:\ncorr[corr.min().idxmin()].idxmin()\n'Longitude'\nThe most negative correlation is therefore between:\ncorr.min().idxmin(), corr[corr.min().idxmin()].idxmin()\n('Latitude', 'Longitude')\nwith the value:\ncorr.min().min()\n-0.9246644339150366\n\n\nMost positive correlation\nFirst we need to remove the 1.0 values on the diagonal:\nimport numpy as np\n\nnp.fill_diagonal(corr.values, np.nan)\ncorr\n\n\n\n\n\n\n\n\nMedInc\n\n\nHouseAge\n\n\nAveRooms\n\n\nAveBedrms\n\n\nPopulation\n\n\nAveOccup\n\n\nLatitude\n\n\nLongitude\n\n\n\n\n\n\nMedInc\n\n\nNaN\n\n\n-0.119034\n\n\n0.326895\n\n\n-0.062040\n\n\n0.004834\n\n\n0.018766\n\n\n-0.079809\n\n\n-0.015176\n\n\n\n\nHouseAge\n\n\n-0.119034\n\n\nNaN\n\n\n-0.153277\n\n\n-0.077747\n\n\n-0.296244\n\n\n0.013191\n\n\n0.011173\n\n\n-0.108197\n\n\n\n\nAveRooms\n\n\n0.326895\n\n\n-0.153277\n\n\nNaN\n\n\n0.847621\n\n\n-0.072213\n\n\n-0.004852\n\n\n0.106389\n\n\n-0.027540\n\n\n\n\nAveBedrms\n\n\n-0.062040\n\n\n-0.077747\n\n\n0.847621\n\n\nNaN\n\n\n-0.066197\n\n\n-0.006181\n\n\n0.069721\n\n\n0.013344\n\n\n\n\nPopulation\n\n\n0.004834\n\n\n-0.296244\n\n\n-0.072213\n\n\n-0.066197\n\n\nNaN\n\n\n0.069863\n\n\n-0.108785\n\n\n0.099773\n\n\n\n\nAveOccup\n\n\n0.018766\n\n\n0.013191\n\n\n-0.004852\n\n\n-0.006181\n\n\n0.069863\n\n\nNaN\n\n\n0.002366\n\n\n0.002476\n\n\n\n\nLatitude\n\n\n-0.079809\n\n\n0.011173\n\n\n0.106389\n\n\n0.069721\n\n\n-0.108785\n\n\n0.002366\n\n\nNaN\n\n\n-0.924664\n\n\n\n\nLongitude\n\n\n-0.015176\n\n\n-0.108197\n\n\n-0.027540\n\n\n0.013344\n\n\n0.099773\n\n\n0.002476\n\n\n-0.924664\n\n\nNaN\n\n\n\n\n\ncorr.max().idxmax(), corr[corr.max().idxmax()].idxmax()\n('AveRooms', 'AveBedrms')\ncorr.max().max()\n0.8476213257130424\n\n\n\n\n\n\nPlotting the correlation\nViewing the correlation coefficients as a table is useful if you want the precise value of the correlation but often you want a visual overview which can give you the information you want at a glance.\nThe easiest way to view it is as a heat map where each cell has a colour showing the value of the correlation using Seaborn which is a visualisation library that provides a higher-level interface to Matplotlib.\n\nimport seaborn as sns\n\nsns.heatmap(corr, vmin=-1.0, vmax=1.0, square=True, cmap=\"RdBu\")\n\n\n\n\n\n\n\n\nThis gives us a sense of which parameters are strongly correlated with each other. Very blue squares are positively correlated, for example the average number of rooms and the average number of bedrooms. That correlation makes sense as they definitely have mutual information.\nOthers perhaps make less sense at a glance. We see that the latitude is very strongly negatively correlated with the longitude. Why on earth should there be any relationship between those two? Let’s take a look at another view on the data to see if we can discover why.\n\n\nMulti-variable scatter matrix\nPandas also provides a quick method of looking at a large number of data parameters at once and looking visually at which might be worth investigating. If you pass any pandas DataFrame to the scatter_matrix() function then it will plot all the pairs of parameters in the data.\nThe produced graph has a lot of information in it so it’s worth taking some time to make sure you understand these plots. The plot is arranged with all the variables of interest from top to bottom and then repeated from left to right so that any one square in the grid is defined by the intersection of two variables.\nEach box that is an intersection of a variable with another (e.g. row three, column one is the intersection between “AveRooms” and “MedInc”) shows the scatter plot of how the values of those variables relate to each other. If you see a strong diagonal line it means that those variables are correlated in this data set. It it’s more of a blob or a flat horizontal or vertical line then that suggests a low correlation.\nThe top-right triangle of the plot is a repeat of the bottom-left triangle, just with the items in the pair reversed (i.e. row one, column three is the intersection between “MedInc” and “AveRooms”).\nThe square boxes along the diagonal from the top-left to the bottom-right are those intersections of a variable with itself and so are used, not to show correlation, but to show the distribution of values of each single variable as a histogram.\n\nfrom pandas.plotting import scatter_matrix\n\na = scatter_matrix(housing, figsize=(16, 16))\n\n\n\n\n\n\n\n\nIn general, when calculating a regression, you want your features to be as uncorrelated with each other as possible. This is because if two features, \\(x_1\\) and \\(x_2\\) are strongly correlated with each other then it’s possible to predict the value of \\(x_2\\) from the value of \\(x_1\\) with high confidence. This means that \\(x_2\\) is not providing any additional predictive power.\nIn some cases this is not a problem as adding one extra variable does not slow down or harm the algorithm used but some methods benefit from choosing carefully the parameters which are being fitted over.\nIt’s also possible in some cases to transform the data in some way to reduce the correlation between variables. One example of a method which does this is principle component analysis (PCA).\nOn the other hand, you do want correlation between \\(X\\) and \\(y\\) as if there is no mutual information then there is no predictive power.\n\n\n\n\n\n\nExercise\n\n\n\nTry running through the above step using a different dataset from sklearn. You can find them listed at https://scikit-learn.org/stable/datasets/toy_dataset.html. Iris is a classic dataset used in machine learning which it is worth being aware of.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nfrom pandas import DataFrame\nfrom sklearn.datasets import load_iris\n\niris, iris_target = load_iris(as_frame=True, return_X_y=True)\niris.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\n\n\nsepal width (cm)\n\n\npetal length (cm)\n\n\npetal width (cm)\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\n\n\n2\n\n\n4.7\n\n\n3.2\n\n\n1.3\n\n\n0.2\n\n\n\n\n3\n\n\n4.6\n\n\n3.1\n\n\n1.5\n\n\n0.2\n\n\n\n\n4\n\n\n5.0\n\n\n3.6\n\n\n1.4\n\n\n0.2\n\n\n\n\n\ncorr = iris.corr()\n%matplotlib inline\n\nimport seaborn as sns\n\nsns.heatmap(corr, vmin=-1.0, vmax=1.0, cmap=\"RdBu\")\n&lt;AxesSubplot: &gt;\n\nfrom pandas.plotting import scatter_matrix\n\na = scatter_matrix(iris, figsize=(16, 16), c=iris_target)",
    "crumbs": [
      "Working framework",
      "Loading your data"
    ]
  },
  {
    "objectID": "pages/answer_blob_inertia.html",
    "href": "pages/answer_blob_inertia.html",
    "title": "Introduction to Data Anlysis in Python (Part 2)",
    "section": "",
    "text": "%matplotlib inline\n\nfrom pandas import Series, DataFrame\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\ndata, true_labels = make_blobs(n_samples=500, centers=4, random_state=6)\npoints = DataFrame(data, columns=[\"x\", \"y\"])\n\ninertia_values = []\nr = pd.RangeIndex(2, 8)\nfor n_clusters in r:\n    kmeans = KMeans(n_clusters=n_clusters).fit(points)\n    inertia_values.append(kmeans.inertia_)\n\ninertia = Series(inertia_values, name=\"inertia\", index=r)\ninertia.plot()\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f374762f198&gt;"
  },
  {
    "objectID": "pages/990-what-next.html",
    "href": "pages/990-what-next.html",
    "title": "What is next?",
    "section": "",
    "text": "Questions for discussion\nThe below questions for discussion can help consolidate your learning. How many can you answer?\n\nWhat is the “right” way to split data for machine learning?\n\nWould that work for time-series data?\nWhat if you had parameters you wanted to learn?\nWhat if you wanted to compare models?\n\nIf we had just one number to score a model, what would you choose?\n\nWhen is it good?\nWhen is it bad?\n\nHow do you find out more information about a model?\nIf you run a model and get an answer, and a colleague ran an analysis independently and came to a different conclusion, how would you go about explaining the discrepancy?\nWhat makes you think that the analysis is “right”? How would you know if something was wrong?\nDid you test a hypothesis? If so which hypothesis?\n\nIs there an area of your research where this is applicable?\nWhat about related work on slightly different problems? Are the hypotheses the same?\n\nHow to handle missing data\n\n\n\nFurther topics\nHere are some additional chapters to work through on a number of different topics. Choose the ones that interest you.\nLogistic regression for cancer classification\nLogistic regression for housing price classification\nPCA of morphological traits\nPrediction Alzheimer’s disease\nFeature Importance Analysis\nMedical image clustering\nFeature scaling and principal component analysis are important parts of many data analysis pipelines.\nClustering is an unsupervised classification method.\nImage Clustering uses clustering techniques to demonstrate a form of image compression.",
    "crumbs": [
      "Discussion",
      "What is next?"
    ]
  },
  {
    "objectID": "pages/answer_find_bluest_pixel.html",
    "href": "pages/answer_find_bluest_pixel.html",
    "title": "Introduction to Data Anlysis in Python (Part 2)",
    "section": "",
    "text": "import numpy as np\nfrom pandas import DataFrame, Series\nfrom skimage import io\n\nphoto = io.imread(\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/97/Swallow-tailed_bee-eater_%28Merops_hirundineus_chrysolaimus%29.jpg/768px-Swallow-tailed_bee-eater_%28Merops_hirundineus_chrysolaimus%29.jpg\")\n\nphoto = np.array(photo, dtype=np.float64) / 255  # Scale values\nw, h, d = original_shape = tuple(photo.shape)  # Get the current shape\nimage_array = np.reshape(photo, (w * h, d))  # Reshape to to 2D\n\npixels = DataFrame(image_array, columns=[\"Red\", \"Green\", \"Blue\"])\nTo find the index of the pixel with the largest blue value, we use idxmax() on the Blue column.\nbluest_index = pixels[\"Blue\"].idxmax()\nWe use the width, w, of the original image as the denominator for a division and a modulo.\nx = bluest_index % w\ny = bluest_index // w\nx, y\n(537, 150)"
  },
  {
    "objectID": "pages/answer_gridsearch_knn_iris.html",
    "href": "pages/answer_gridsearch_knn_iris.html",
    "title": "Introduction to Data Anlysis in Python (Part 2)",
    "section": "",
    "text": "As usual, grab the data. The difference this time is that are only going to grab two of the features in order to make it a 2D problem which is easier to visualise.\nfrom pandas import DataFrame\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\nX = DataFrame(load_iris().data, columns=load_iris().feature_names)\nX = X[[\"sepal length (cm)\", \"sepal width (cm)\"]]  # Grab just two of the features\ny = load_iris().target\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, random_state=42)\nWe will look at values of n_neighbors from 0 to 59.\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\n\nhyperparameters = {\n    \"n_neighbors\" : range(1, 40),\n}\nclf = GridSearchCV(KNeighborsClassifier(), hyperparameters).fit(train_X, train_y)\nTo easily look at the results, we put the output into a DataFrame, sort it by the test score (how well that value did against its validation set) and grab the top few rows.\ncv_results = DataFrame(clf.cv_results_)\ncv_results = cv_results.sort_values([\"rank_test_score\", \"mean_test_score\"])\ncv_results.head()[[\"param_n_neighbors\", \"mean_test_score\", \"std_test_score\", \"rank_test_score\"]]\n\n\n\n\n\n\n\n\nparam_n_neighbors\n\n\nmean_test_score\n\n\nstd_test_score\n\n\nrank_test_score\n\n\n\n\n\n\n30\n\n\n31\n\n\n0.795652\n\n\n0.056227\n\n\n1\n\n\n\n\n28\n\n\n29\n\n\n0.795257\n\n\n0.042471\n\n\n2\n\n\n\n\n17\n\n\n18\n\n\n0.786561\n\n\n0.048231\n\n\n3\n\n\n\n\n27\n\n\n28\n\n\n0.786561\n\n\n0.048231\n\n\n3\n\n\n\n\n29\n\n\n30\n\n\n0.786561\n\n\n0.048231\n\n\n3\n\n\n\n\n\nIt looks like the best one is n_neighbors=31 but let’s look on a plot to see how it varies:\ncv_results.plot.scatter(\"param_n_neighbors\", \"mean_test_score\", yerr=\"std_test_score\")\n&lt;Axes: xlabel='param_n_neighbors', ylabel='mean_test_score'&gt;\n\nIndeed n_neighbors=31 is the best in the range but they all have large standard deviations. It’s worth plotting it like this so that you might want to pick a lower mean in order to get a tighter distribution.\nfrom sklearn.inspection import DecisionBoundaryDisplay\nimport seaborn as sns\n\nDecisionBoundaryDisplay.from_estimator(clf, X, cmap=\"Pastel2\")\nsns.scatterplot(data=X, x=\"sepal length (cm)\", y=\"sepal width (cm)\", hue=y, palette=\"Dark2\")\n&lt;Axes: xlabel='sepal length (cm)', ylabel='sepal width (cm)'&gt;\n\nclf.score(test_X, test_y)\n0.868421052631579"
  },
  {
    "objectID": "pages/500-hyperparameters.html",
    "href": "pages/500-hyperparameters.html",
    "title": "Choosing hyperparameters",
    "section": "",
    "text": "Learning outcomes\n\n\n\nBy the end of this section you should:\n\nUnderstand the concept of hyperparameters and their impact on model performance\nImplement techniques like grid search for hyperparameter tuning\nVisualize model results using seaborn\n\n\n\n\n\n\n\n\n\nTrainer notes\n\n\n\n\n\nRecommended time: 30 minutes\nThis sections introduces to hyperparameters and their optimization.\nNote that GridSearchCV does split the data again.\n\n\n\nAfter the exercise in the last chapter, you’re hopefully thinking “why am I spending my time trying different values of n_neighbors, can’t it do this automatically?” If so, you’re in luck!\nIt is a very common thing to need to try out a bunch of different values for your hyperparameters and so scikit-learn provides us with some tools to help out.\nLet’s do a similar thing to the last chapter, but load a different file this time. One with four different classes and follow through the usual steps:\nimport pandas as pd\n\ndata = pd.read_csv(\"https://bristol-training.github.io/data-analysis-python-2/data/blobs.csv\")\nX = data[[\"x1\", \"x2\"]]\ny = data[\"y\"]\n\nimport seaborn as sns\n\nsns.scatterplot(data=data, x=\"x1\", y=\"x2\", hue=\"y\", palette=\"Dark2\")\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y)\n\nThe tools that allows us to do the hyper-parameter searching is called GridSearchCV which will rerun the model training for every possible hyperparameter that we pass it.\nThe GridSearchCV constructor takes two things: 1. the model that we want to explore, 2. a dictionary containing the hyper-parameter values we want to test.\nIn this case, we are asking it to try every value of n_neighbors from 1 to 49 and it will use the training data to choose the best value.\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\n\nhyperparameters = {\n    \"n_neighbors\" : range(1, 175),\n}\nmodel = GridSearchCV(KNeighborsClassifier(), hyperparameters)\nmodel.fit(train_X, train_y)\n\nGridSearchCV(estimator=KNeighborsClassifier(),\n             param_grid={'n_neighbors': range(1, 175)})In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFitted\n        \n            \n                Parameters\n                \n\n\n\n\nestimator \nKNeighborsClassifier()\n\n\n\nparam_grid \n{'n_neighbors': range(1, 175)}\n\n\n\nscoring \nNone\n\n\n\nn_jobs \nNone\n\n\n\nrefit \nTrue\n\n\n\ncv \nNone\n\n\n\nverbose \n0\n\n\n\npre_dispatch \n'2*n_jobs'\n\n\n\nerror_score \nnan\n\n\n\nreturn_train_score \nFalse\n\n\n\n\n            \n        \n    best_estimator_: KNeighborsClassifierKNeighborsClassifier(n_neighbors=39)KNeighborsClassifier?Documentation for KNeighborsClassifier\n        \n            \n                Parameters\n                \n\n\n\n\nn_neighbors \n39\n\n\n\nweights \n'uniform'\n\n\n\nalgorithm \n'auto'\n\n\n\nleaf_size \n30\n\n\n\np \n2\n\n\n\nmetric \n'minkowski'\n\n\n\nmetric_params \nNone\n\n\n\nn_jobs \nNone\n\n\n\n\n            \n        \n    \n\n\nThe best way to visualise the data is to plot it. We can do this by grabbing the cv_results_ attribute of GridSearchCV and plotting the mean_test_score against the value of n_neighbors. GridSearchCV will run each experiment multiple times with different splits of training and validation data to provide some measure of uncertainty of the score:\n\ncv_results = pd.DataFrame(model.cv_results_)\ncv_results.plot.scatter(\"param_n_neighbors\", \"mean_test_score\", yerr=\"std_test_score\", figsize=(10,8))\n\n\n\n\n\n\n\n\nOne thing that GridSearchCV does, once it has scanned through all the parameters, is do a final fit using the whole training data set using the best hyperparameters from the search. This allows you to use the GridSearchCV object model as if it were a KNeighborsClassifier object.\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nDecisionBoundaryDisplay.from_estimator(model, X, cmap=\"Pastel2\")\nsns.scatterplot(data=X, x=\"x1\", y=\"x2\", hue=y, palette=\"Dark2\")\n\n/home/runner/work/data-analysis-python-2/data-analysis-python-2/.venv/lib/python3.12/site-packages/sklearn/inspection/_plot/decision_boundary.py:226: UserWarning: 'cmap' is ignored in favor of 'multiclass_colors' in the multiclass case when the response method is 'decision_function' or 'predict_proba'.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nor use it directly with predict:\n\nnew_X = pd.DataFrame({\n    \"x1\": [0, -10, 5, -5],\n    \"x2\": [10, 5, 0, -10],\n})\n\nmodel.predict(new_X)\n\narray([0, 3, 1, 2])\n\n\nor measure its performance against the test data set:\n\nmodel.score(test_X, test_y)\n\n0.96\n\n\nUsing something like GridSearchCV allows you to find the best hyperparameters for your models while keeping them working most generally.\n\n\n\n\n\n\nExercise\n\n\n\nGrab the Iris data set from sklearn. This time we will simplify it down to only two features (sepal length and sepal width) to simplity the visualisation.\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\nX, y = load_iris(as_frame=True, return_X_y=True)\nX = X[[\"sepal length (cm)\", \"sepal width (cm)\"]]  # Grab just two of the features\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, random_state=42)\nUsing GridSearchCV, find the best value of n_neighbors and print the score of that model when run over the test data set.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nAs usual, grab the data. The difference this time is that are only going to grab two of the features in order to make it a 2D problem which is easier to visualise.\nfrom pandas import DataFrame\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\nX = DataFrame(load_iris().data, columns=load_iris().feature_names)\nX = X[[\"sepal length (cm)\", \"sepal width (cm)\"]]  # Grab just two of the features\ny = load_iris().target\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, random_state=42)\nWe will look at values of n_neighbors from 0 to 59.\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\n\nhyperparameters = {\n    \"n_neighbors\" : range(1, 40),\n}\nclf = GridSearchCV(KNeighborsClassifier(), hyperparameters).fit(train_X, train_y)\nTo easily look at the results, we put the output into a DataFrame, sort it by the test score (how well that value did against its validation set) and grab the top few rows.\ncv_results = DataFrame(clf.cv_results_)\ncv_results = cv_results.sort_values([\"rank_test_score\", \"mean_test_score\"])\ncv_results.head()[[\"param_n_neighbors\", \"mean_test_score\", \"std_test_score\", \"rank_test_score\"]]\n\n\n\n\n\n\n\n\nparam_n_neighbors\n\n\nmean_test_score\n\n\nstd_test_score\n\n\nrank_test_score\n\n\n\n\n\n\n30\n\n\n31\n\n\n0.795652\n\n\n0.056227\n\n\n1\n\n\n\n\n28\n\n\n29\n\n\n0.795257\n\n\n0.042471\n\n\n2\n\n\n\n\n17\n\n\n18\n\n\n0.786561\n\n\n0.048231\n\n\n3\n\n\n\n\n27\n\n\n28\n\n\n0.786561\n\n\n0.048231\n\n\n3\n\n\n\n\n29\n\n\n30\n\n\n0.786561\n\n\n0.048231\n\n\n3\n\n\n\n\n\nIt looks like the best one is n_neighbors=31 but let’s look on a plot to see how it varies:\ncv_results.plot.scatter(\"param_n_neighbors\", \"mean_test_score\", yerr=\"std_test_score\")\n&lt;Axes: xlabel='param_n_neighbors', ylabel='mean_test_score'&gt;\n\nIndeed n_neighbors=31 is the best in the range but they all have large standard deviations. It’s worth plotting it like this so that you might want to pick a lower mean in order to get a tighter distribution.\nfrom sklearn.inspection import DecisionBoundaryDisplay\nimport seaborn as sns\n\nDecisionBoundaryDisplay.from_estimator(clf, X, cmap=\"Pastel2\")\nsns.scatterplot(data=X, x=\"sepal length (cm)\", y=\"sepal width (cm)\", hue=y, palette=\"Dark2\")\n&lt;Axes: xlabel='sepal length (cm)', ylabel='sepal width (cm)'&gt;\n\nclf.score(test_X, test_y)\n0.868421052631579",
    "crumbs": [
      "Model fitting",
      "Choosing hyperparameters"
    ]
  },
  {
    "objectID": "pages/300-machine-learning.html",
    "href": "pages/300-machine-learning.html",
    "title": "Machine Learning",
    "section": "",
    "text": "In this section…\n\n\n\n\nUnderstand different types of machine learning models (e.g., regression, classification)\nSelect suitable models based on the problem type and data characteristics\n\n\n\n\n\n\n\n\n\nTrainer notes\n\n\n\n\n\nRecommended time: 15 minutes\nThis section introduces the concept of machine learning and different models. To be read by the student.\n\n\n\nAs explained at the beginning of the last chapter, machine learning is the name given to the tools, techniques and algorithms which are used to extract information from data.\nThere are two main classes of machine learning:\n\nSupervised Learning\nSupervised learning algorithms are trained on labeled data, where both input features and corresponding output labels are provided. The goal is to learn a function that maps inputs to outputs. This type of learning is further divided into:\n\nClassification: Predicting discrete categories or classes (e.g., spam detection, image recognition)\nRegression: Predicting continuous values (e.g., house price prediction, temperature forecasting)\n\nPopular supervised learning algorithms include: - Linear Regression - Logistic Regression - Support Vector Machines (SVM) - Decision Trees - Random Forests\n\nUnsupervised This is where you don’t have any label associated with the data and the algorithm will need to extract features of interest itself. Examples of this are:\n\nClustering: Grouping similar data points together (e.g., customer segmentation, anomaly detection)\nDimensionality reduction: Reducing the number of features while preserving important information (e.g., Principal Component Analysis)\n\n\nCommon unsupervised learning algorithms include: - K-means clustering - Hierarchical clustering - DBSCAN - Gaussian Mixture Models (GMM)\n\n\nThe supervised learning process\n\nThere is a standard process that you go through with most supervised learning problems which is worth understanding as it affects how you should go about data collection as well as the types of problems you can use it to solve. For this explanation, let’s pretend that we want to create a model which can predict the price of a house based on its age, the number of rooms it has and the size of its garden.\nThey all start with the initial data collection. You go out into “the wild” and collect some data, \\(X\\), this could be people’s heights or lengths of leaves of trees or anything in your field which you consider easy to collect/measure. In our example here we would measure a large number of houses’ ages, room counts and garden sizes. We put these data into a table with one row per house and three columns, one for each feature.\nAlongside that you, as an expert, label each data point you collected with some extra data and call that \\(y\\). In our case \\(y\\) is the price of the house. This is something which requires an expert’s eye to assess and we are trying to create a predictive model which can replace the job of the expert house-price surveyor.\nOnce you have collected your data, you need to choose which model best represents the relationship between \\(X\\) and \\(y\\). Perhaps a linear regression is sufficient or maybe you need something more complex. There is no magic solution to knowing which model to use, it comes from experience and experimentation.\nUsing \\(X\\) and \\(y\\) we train (or “fit”) our model to predict the relationship between those two (making sure to split them into train and test subsets to validate our model). After this point the parameters of our model are fixed and can be detached from the data that were used to train it. It is now a “black box” which can make predictions.\nImagine now that a client comes to us and asks us to estimate how much they might be able to sell their house for. They tell us how old the house is, how many rooms it has and the size of its garden. We put these three numbers (\\(x^\\prime\\)) into our model and it returns for us a prediction, \\(\\hat{y}\\).\nOf course, \\(\\hat{y}\\) is just a prediction, it is not necessarily correct. There is usually a true value that we are hoping that we are near to, \\(y\\). We can measure the quality of our model by seeing how close we are to to the true value by subtracting one from the other: \\(y - \\hat{y}\\). This is called the residual.\n\n\nAdvanced AI Techniques\n\nNeural Networks and Deep Learning\nNeural networks, inspired by the human brain, consist of interconnected layers of artificial neurons. Deep learning, a subset of neural networks with multiple hidden layers, has shown remarkable success in various domains, including:\n\nComputer Vision\nNatural Language Processing\nSpeech Recognition\n\n\n\nLanguage Models\nLarge language models, such as GPT (Generative Pre-trained Transformer), have revolutionized natural language processing tasks. These models are trained on vast amounts of text data and can generate human-like text, answer questions, and perform various language-related tasks.",
    "crumbs": [
      "Model fitting",
      "Machine Learning"
    ]
  },
  {
    "objectID": "pages/980-summary.html",
    "href": "pages/980-summary.html",
    "title": "Summary",
    "section": "",
    "text": "That’s all we have for this workshop. By now you should have a better understanding of how you can make your code more easily shared and reusable. In this workshow we have covered:\n\nHow to make you life easier with string formatting\nHow to use dictionaries\nUsing the Python standard library to find and use useful functions\nWays of bundling up your code into reusable units with functions\nMaking it possible to share your code with others by moving code into modules\nHow to produce custom errors\nHow to compactly generate lists with list comprehensions"
  },
  {
    "objectID": "pages/answer_iris_clustering.html",
    "href": "pages/answer_iris_clustering.html",
    "title": "Introduction to Data Anlysis in Python (Part 2)",
    "section": "",
    "text": "%matplotlib inline\n\nfrom pandas import DataFrame\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import load_iris\n\niris = DataFrame(load_iris().data, columns=load_iris().feature_names)\n\nnum_iris_species = len(load_iris().target_names)\n\nkmeans = KMeans(n_clusters=num_iris_species, n_init=\"auto\").fit(iris)\n\na = scatter_matrix(iris, figsize=(16, 16), c=kmeans.labels_)"
  },
  {
    "objectID": "pages/answer_find_largest_correlations.html",
    "href": "pages/answer_find_largest_correlations.html",
    "title": "Introduction to Data Anlysis in Python (Part 2)",
    "section": "",
    "text": "Let’s find the most negative and the most positive (ignoring self-correlation) values\nfrom pandas import DataFrame\nfrom sklearn.datasets import fetch_california_housing\n\nhousing_data = fetch_california_housing()\nhousing = DataFrame(housing_data.data, columns=housing_data.feature_names)\n\ncorr = housing.corr()\n\ncorr\n\n\n\n\n\n\n\n\nMedInc\n\n\nHouseAge\n\n\nAveRooms\n\n\nAveBedrms\n\n\nPopulation\n\n\nAveOccup\n\n\nLatitude\n\n\nLongitude\n\n\n\n\n\n\nMedInc\n\n\n1.000000\n\n\n-0.119034\n\n\n0.326895\n\n\n-0.062040\n\n\n0.004834\n\n\n0.018766\n\n\n-0.079809\n\n\n-0.015176\n\n\n\n\nHouseAge\n\n\n-0.119034\n\n\n1.000000\n\n\n-0.153277\n\n\n-0.077747\n\n\n-0.296244\n\n\n0.013191\n\n\n0.011173\n\n\n-0.108197\n\n\n\n\nAveRooms\n\n\n0.326895\n\n\n-0.153277\n\n\n1.000000\n\n\n0.847621\n\n\n-0.072213\n\n\n-0.004852\n\n\n0.106389\n\n\n-0.027540\n\n\n\n\nAveBedrms\n\n\n-0.062040\n\n\n-0.077747\n\n\n0.847621\n\n\n1.000000\n\n\n-0.066197\n\n\n-0.006181\n\n\n0.069721\n\n\n0.013344\n\n\n\n\nPopulation\n\n\n0.004834\n\n\n-0.296244\n\n\n-0.072213\n\n\n-0.066197\n\n\n1.000000\n\n\n0.069863\n\n\n-0.108785\n\n\n0.099773\n\n\n\n\nAveOccup\n\n\n0.018766\n\n\n0.013191\n\n\n-0.004852\n\n\n-0.006181\n\n\n0.069863\n\n\n1.000000\n\n\n0.002366\n\n\n0.002476\n\n\n\n\nLatitude\n\n\n-0.079809\n\n\n0.011173\n\n\n0.106389\n\n\n0.069721\n\n\n-0.108785\n\n\n0.002366\n\n\n1.000000\n\n\n-0.924664\n\n\n\n\nLongitude\n\n\n-0.015176\n\n\n-0.108197\n\n\n-0.027540\n\n\n0.013344\n\n\n0.099773\n\n\n0.002476\n\n\n-0.924664\n\n\n1.000000\n\n\n\n\n\n\nMost negative correlation\nFind the most negative correlation for each column:\ncorr.min()\nMedInc       -0.119034\nHouseAge     -0.296244\nAveRooms     -0.153277\nAveBedrms    -0.077747\nPopulation   -0.296244\nAveOccup     -0.006181\nLatitude     -0.924664\nLongitude    -0.924664\ndtype: float64\nFind the column which has the lowest correlation:\ncorr.min().idxmin()\n'Latitude'\nExtract the Latitude column and get the index of the most negative value in it:\ncorr[corr.min().idxmin()].idxmin()\n'Longitude'\nThe most negative correlation is therefore between:\ncorr.min().idxmin(), corr[corr.min().idxmin()].idxmin()\n('Latitude', 'Longitude')\nwith the value:\ncorr.min().min()\n-0.9246644339150366\n\n\nMost positive correlation\nFirst we need to remove the 1.0 values on the diagonal:\nimport numpy as np\n\nnp.fill_diagonal(corr.values, np.nan)\ncorr\n\n\n\n\n\n\n\n\nMedInc\n\n\nHouseAge\n\n\nAveRooms\n\n\nAveBedrms\n\n\nPopulation\n\n\nAveOccup\n\n\nLatitude\n\n\nLongitude\n\n\n\n\n\n\nMedInc\n\n\nNaN\n\n\n-0.119034\n\n\n0.326895\n\n\n-0.062040\n\n\n0.004834\n\n\n0.018766\n\n\n-0.079809\n\n\n-0.015176\n\n\n\n\nHouseAge\n\n\n-0.119034\n\n\nNaN\n\n\n-0.153277\n\n\n-0.077747\n\n\n-0.296244\n\n\n0.013191\n\n\n0.011173\n\n\n-0.108197\n\n\n\n\nAveRooms\n\n\n0.326895\n\n\n-0.153277\n\n\nNaN\n\n\n0.847621\n\n\n-0.072213\n\n\n-0.004852\n\n\n0.106389\n\n\n-0.027540\n\n\n\n\nAveBedrms\n\n\n-0.062040\n\n\n-0.077747\n\n\n0.847621\n\n\nNaN\n\n\n-0.066197\n\n\n-0.006181\n\n\n0.069721\n\n\n0.013344\n\n\n\n\nPopulation\n\n\n0.004834\n\n\n-0.296244\n\n\n-0.072213\n\n\n-0.066197\n\n\nNaN\n\n\n0.069863\n\n\n-0.108785\n\n\n0.099773\n\n\n\n\nAveOccup\n\n\n0.018766\n\n\n0.013191\n\n\n-0.004852\n\n\n-0.006181\n\n\n0.069863\n\n\nNaN\n\n\n0.002366\n\n\n0.002476\n\n\n\n\nLatitude\n\n\n-0.079809\n\n\n0.011173\n\n\n0.106389\n\n\n0.069721\n\n\n-0.108785\n\n\n0.002366\n\n\nNaN\n\n\n-0.924664\n\n\n\n\nLongitude\n\n\n-0.015176\n\n\n-0.108197\n\n\n-0.027540\n\n\n0.013344\n\n\n0.099773\n\n\n0.002476\n\n\n-0.924664\n\n\nNaN\n\n\n\n\n\ncorr.max().idxmax(), corr[corr.max().idxmax()].idxmax()\n('AveRooms', 'AveBedrms')\ncorr.max().max()\n0.8476213257130424"
  },
  {
    "objectID": "pages/400-nearest-neighbours.html",
    "href": "pages/400-nearest-neighbours.html",
    "title": "Nearest Neighbours",
    "section": "",
    "text": "In the first chapters we were using linear regression which is a supervised regression technique. We’re going to carry on with supervised techniques but look instead at how we can classify or categorise data using an algorithm called K-nearest neighbours.\nWhen you ask for a prediction from k-NN for a data point \\(x\\), the algorithm looks at all the training data that was passed in and finds the \\(k\\) nearest (e.g. the 5 nearest) data points. The label of \\(x\\) will then be based on whichever label was found most frequently within the \\(k\\) neighbours.\n\nWe’ll start by grabbing some data which represents a interesting case for some classification methods. Start by loading the data file:\nimport pandas as pd\n\ndata = pd.read_csv(\"https://bristol-training.github.io/data-analysis-python-2/data/moons.csv\")\nIf we look at the first few rows of this data, we can see it has two features (x1 and x2) and one target column (y). The target column is an integer, which suggests it will work well with a classification algorithm:\n\ndata.head()\n\n\n\n\n\n\n\n\nx1\nx2\ny\n\n\n\n\n0\n0.830858\n-0.334342\n1\n\n\n1\n0.991710\n0.879000\n0\n\n\n2\n1.107245\n-0.470344\n1\n\n\n3\n-0.140899\n1.033148\n0\n\n\n4\n0.405592\n1.328529\n0\n\n\n\n\n\n\n\nLet’s also have a look at the data visually to see what we’re working with:\n\nimport seaborn as sns\n\nsns.scatterplot(data=data, x=\"x1\", y=\"x2\", hue=\"y\", palette=\"Dark2\")\n\n\n\n\n\n\n\n\nWe can grab out the features and target parts now to use in scikit-learn shortly:\n\nX = data[[\"x1\", \"x2\"]]\ny = data[\"y\"]\n\nAs ever, we need to split our data into a training data set and a test data set:\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y)\n\nNow we have our data and it’s all in the correct format (\\(X\\) is a 2D table of data and \\(y\\) is a single column of labels), we can go ahead and use our model.\nAs usual it works by importing the model, KNeighborsClassifier, making an instance of it (setting the hyperparameters) and then fitting it by passing it \\(X\\) and \\(y\\) for the training data set.\nThe most important hyperparameter for k-NN is \\(k\\), or the number of neighbours. The model defaults to 5 but you can set it to any integer you wish.\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier(n_neighbors=5)\nmodel.fit(train_X, train_y)\n\nKNeighborsClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifier?Documentation for KNeighborsClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_neighbors \n5\n\n\n\nweights \n'uniform'\n\n\n\nalgorithm \n'auto'\n\n\n\nleaf_size \n30\n\n\n\np \n2\n\n\n\nmetric \n'minkowski'\n\n\n\nmetric_params \nNone\n\n\n\nn_jobs \nNone\n\n\n\n\n            \n        \n    \n\n\nAt this point, our model is ready to use. I’ll point out one important difference between k-NN and other algorithms and that is how it stores the information you have given it.\nThinking back to the example of the linear regression from the first chapter, in that case we gave the model some data (50 \\(x\\),\\(y\\) values) and based on those it calculated two parameters of interest, the gradient and the y-intercept. No matter how many training examples we give it, it will always generalise those data down to two parameters.\nK-nearest neighbours is different in that it is a non-generalising learning algorithm (also referred to as instance-based learning). It doesn’t simplify down the training data we pass in, it actually stores all of it internally. Thinking about it, this makes sense as when we ask it to make a prediction it needs to actually go and find the data points that are near the prediction site. This means that if we train a model on more data, the model becomes more heavyweight (i.e. may use more memory) and will likely become slower (as it needs to check more points to find the neighbours).\nIn general, this will not cause a problem but it’s something that you should be aware of.\nWe can use our model in the same way as in the past to, for example, check the performance against the test data set:\n\nmodel.score(test_X, test_y)\n\n0.976\n\n\nThat looks like a very good score indeed. Is that believable or do you think we’ve done something wrong?\nLet’s take a look at the distribution of predictions compared to the input data.\nWe’ll use a built-in function from sklearn called DecisionBoundaryDisplay.from_estimator to plot the predictions of the model, compared to the input data.:\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nDecisionBoundaryDisplay.from_estimator(model, X, cmap=\"PRGn\")\nsns.scatterplot(data=X, x=\"x1\", y=\"x2\", hue=y, palette=\"Dark2\")\n\n\n\n\n\n\n\n\nHere we can see that all of the green points are sitting in the one background area and the orange points are all in their area. In this case, it makes sense that it’s got a very good score since the data are not overlapping much.\n\n\n\n\n\n\nExercise\n\n\n\nRun the code above and make sure you see the same output.\nExperiment with different values of n_neighbors when creating the model (between 1 and 200).\nHow does varying this value affect the prediction map or the model score?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThis answer page shows the results of trying different values of noise and n_neighbors when fitting k-NN to a dummy data set. For you to complete the exercise I would just expect you to maually change the values and rerun the cells to look at the differences. On this page I have done something a little more complicated in order to visualise all the combinations in one plot.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.inspection import DecisionBoundaryDisplay\nWe’ll loop over a range of neighbour counts:\nneighbours = [1, 5, 10, 100, 150]\nStart by grabbing the data:\ndata = pd.read_csv(\"https://bristol-training.github.io/applied-data-analysis-in-python/data/moons.csv\")\nX = data[[\"x1\", \"x2\"]]\ny = data[\"y\"]\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, random_state=42)\nWe then loop over the range of values we want, fit the model and plot the result for each.\n# We'll plot the results in a grid of subplots\nfig, axs = plt.subplots(\n    nrows=len(neighbours),\n    ncols=1,\n    figsize=(8, 30),\n    constrained_layout=True,\n    sharex=True,\n    sharey=True\n)\n\nfor row, n_neighbors in enumerate(neighbours):\n    # Fit and score the model (uses the `n_neighbors` variable)\n    model = KNeighborsClassifier(n_neighbors=n_neighbors).fit(train_X, train_y)\n    score = model.score(test_X, test_y)\n\n    # Plot the results in the grid of subplots\n    ax = axs[row]\n    ax.set_xlim(-2, 3)\n    ax.set_ylim(-1.5, 2)\n    ax.set_title(f\"k: {n_neighbors}, score={score:.2}\")\n    DecisionBoundaryDisplay.from_estimator(model, X, cmap=\"PRGn\", ax=ax)\n    sns.scatterplot(data=X, x=\"x1\", y=\"x2\", hue=y, ax=ax, palette=\"Dark2\")\n    ax.get_legend().set_visible(False)"
  },
  {
    "objectID": "pages/generate_knn_animation.html",
    "href": "pages/generate_knn_animation.html",
    "title": "Introduction to Data Anlysis in Python (Part 2)",
    "section": "",
    "text": "from pandas import DataFrame\nimport seaborn as sns\nfrom sklearn.datasets import make_blobs\n\nN_CLUSTERS = 2\n\ndata, true_labels = make_blobs(n_samples=500, centers=N_CLUSTERS, random_state=3)\n\npoints = DataFrame(data, columns=[\"x1\", \"x2\"])\nax = sns.scatterplot(data=points, x=\"x1\", y=\"x2\", hue=true_labels, palette=\"Dark2\")\nax.get_legend().set_visible(False)\n\nfrom matplotlib.patches import Circle\nimport numpy as np\n\n#circle_centre = (-2.2, 3.5)\ncircle_centre = (-1.4, 2.6)\npoint_distances = np.sqrt(((points[\"x1\"] - circle_centre[0]).pow(2) + (points[\"x2\"] - circle_centre[1]).pow(2)))\nclosest_point_distances = point_distances.sort_values().iloc[0:5]\nclosest_points = points.loc[closest_point_distances.index]\nclosest_points[\"distance\"] = closest_point_distances\nclosest_points[\"label\"] = true_labels[closest_point_distances.index]\n\nax = sns.scatterplot(data=points, x=\"x1\", y=\"x2\", hue=true_labels, palette=\"Dark2\")\nsns.scatterplot(data=closest_points, x=\"x1\", y=\"x2\", hue=\"label\", palette=\"Dark2\", s=100, alpha=0.5)\nax.get_legend().set_visible(False)\nax.scatter([circle_centre[0]], [circle_centre[1]], s=5)\nax.add_patch(Circle(circle_centre, radius=closest_points[\"distance\"].iloc[-1], facecolor=\"None\", edgecolor=(1, 0.5, 0.5), linewidth=2, alpha=0.8))\nax.axis(\"equal\")\nclosest_points\n\n\n\n\n\n\n\n\nx1\n\n\nx2\n\n\ndistance\n\n\nlabel\n\n\n\n\n\n\n286\n\n\n-0.929114\n\n\n3.003234\n\n\n0.619944\n\n\n0\n\n\n\n\n30\n\n\n-1.014436\n\n\n3.202181\n\n\n0.715040\n\n\n0\n\n\n\n\n75\n\n\n-1.899780\n\n\n3.191116\n\n\n0.774079\n\n\n0\n\n\n\n\n247\n\n\n-2.147328\n\n\n1.727181\n\n\n1.149049\n\n\n1\n\n\n\n\n230\n\n\n-1.660563\n\n\n3.732856\n\n\n1.162435\n\n\n0\n\n\n\n\n\n\nfrom collections import defaultdict\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib import animation\nfrom matplotlib import cm\nfrom sklearn.cluster import KMeans\n\n# First set up the figure, the axis, and the plot element we want to animate\nwith plt.xkcd():\n    fig, ax = plt.subplots(constrained_layout=True)\n#fig.set_constrained_layout_pads(w_pad=10/72, h_pad=10/72)\nax.tick_params(\n    axis='both',\n    which='both',\n    bottom=False,\n    left=False,\n    labelbottom=False,\n    labelleft=False,\n)\nax.axis(\"equal\")\n\nax.set_xlim(-5, 2)\nax.set_ylim(0, 5)\n\ncmap = cm.Dark2\n\n# Here we create the initial path objects which will be altered each frame.\np = sns.scatterplot(data=points, x=\"x1\", y=\"x2\", hue=true_labels, palette=\"Dark2\", s=40, alpha=0.8, ax=ax).get_children()[0]\nax.get_legend().set_visible(False)\n#x = ax.scatter([circle_centre[0]], [circle_centre[1]], c=\"red\", s=200, marker=\"x\")\nc = ax.add_patch(Circle(circle_centre, radius=0.0, facecolor=\"None\", edgecolor=\"black\", linewidth=2, alpha=0.6))\n\nFPS = 30\nANIMATION_LENGTH = 6.0  # seconds\n\ntimes_drawn = defaultdict(int)\n\ndef animate(i):\n    max_radius = closest_points.iloc[-1][\"distance\"]*1.01\n    end_pause = 2.0  # seconds\n    max_radius_time = ANIMATION_LENGTH - end_pause  # seconds\n    \n    time=i/FPS\n    new_radius = min(max_radius*(time/max_radius_time), max_radius)  # grow up to max_radius in max_radius_time\n    \n    c.set_radius(new_radius)\n    \n    for index, row in closest_points.iterrows():\n        if new_radius &gt;= row[\"distance\"]:\n            if times_drawn[index] &gt; 0:\n                continue\n            color = cmap.colors[int(round(row[\"label\"]))]\n            ax.scatter([row[\"x1\"]], [row[\"x2\"]], s=160, color=color, alpha=0.5)\n            with plt.xkcd():\n                ax.plot([circle_centre[0], row[\"x1\"]], [circle_centre[1], row[\"x2\"]], color=color, lw=2, alpha=0.6, zorder=-10)\n            times_drawn[index] += 1\n\n    return p,\n\nanim = animation.FuncAnimation(fig, animate, frames=int(FPS*ANIMATION_LENGTH), interval=1000/FPS, blit=True)\n\n#anim.save(\"kmeans.mp4\", extra_args=[\"-vcodec\", \"libx264\"])\nanim.save(\"../img/knn.gif\", writer=\"imagemagick\")\n\n!gifsicle -b -O3 knn.gif\ngifsicle:knn.gif: warning: too many colors, using local colormaps\n  (You may want to try ‘--colors 256’.)"
  },
  {
    "objectID": "pages/answer_knn_moons.html",
    "href": "pages/answer_knn_moons.html",
    "title": "Introduction to Data Anlysis in Python (Part 2)",
    "section": "",
    "text": "This answer page shows the results of trying different values of noise and n_neighbors when fitting k-NN to a dummy data set. For you to complete the exercise I would just expect you to maually change the values and rerun the cells to look at the differences. On this page I have done something a little more complicated in order to visualise all the combinations in one plot.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.inspection import DecisionBoundaryDisplay\nWe’ll loop over a range of neighbour counts:\nneighbours = [1, 5, 10, 100, 150]\nStart by grabbing the data:\ndata = pd.read_csv(\"https://bristol-training.github.io/applied-data-analysis-in-python/data/moons.csv\")\nX = data[[\"x1\", \"x2\"]]\ny = data[\"y\"]\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, random_state=42)\nWe then loop over the range of values we want, fit the model and plot the result for each.\n# We'll plot the results in a grid of subplots\nfig, axs = plt.subplots(\n    nrows=len(neighbours),\n    ncols=1,\n    figsize=(8, 30),\n    constrained_layout=True,\n    sharex=True,\n    sharey=True\n)\n\nfor row, n_neighbors in enumerate(neighbours):\n    # Fit and score the model (uses the `n_neighbors` variable)\n    model = KNeighborsClassifier(n_neighbors=n_neighbors).fit(train_X, train_y)\n    score = model.score(test_X, test_y)\n\n    # Plot the results in the grid of subplots\n    ax = axs[row]\n    ax.set_xlim(-2, 3)\n    ax.set_ylim(-1.5, 2)\n    ax.set_title(f\"k: {n_neighbors}, score={score:.2}\")\n    DecisionBoundaryDisplay.from_estimator(model, X, cmap=\"PRGn\", ax=ax)\n    sns.scatterplot(data=X, x=\"x1\", y=\"x2\", hue=y, ax=ax, palette=\"Dark2\")\n    ax.get_legend().set_visible(False)"
  },
  {
    "objectID": "pages/002-jupyter-notebooks.html",
    "href": "pages/002-jupyter-notebooks.html",
    "title": "Getting familiar with Notebooks",
    "section": "",
    "text": "In this section…\n\n\n\n\nLearn how to use Jupyter Notebooks, including code execution, markdown documentation, and inline visualizations\n\n\n\n\n\n\n\n\n\nTrainer notes\n\n\n\n\n\nRecommended time: 15 minutes\nEven if some students may not understand some of the python code below, they can copy-paste it for now. The key point is getting familiar with Jupyter Notebooks interface.\n\n\n\nThis course will use a tool called Jupyter Notebooks to run your Python code. Once Anaconda is installed, start “Anaconda Navigator” and press the JupyterLab button on the main screen:\nTo open a notebook, follow our setup instructions. Once you have JupyterLab running, you can create a new notebook by clicking on the Python icon under the “Notebook” section of the launcher tab\nOnce the notebook is launched, you will see a wide grey box with a blue [ ]: to the left. The grey box is an input cell where you type any Python code you want to run:\n\n# Python code can be written in 'Code' cells\nprint(\"Output appears below when the cell is run\")\nprint(\"To run a cell, press Ctrl-Enter or Shift-Enter with the cursor inside\")\nprint(\"or use the run button (▶) in the toolbar at the top\")\n\nOutput appears below when the cell is run\nTo run a cell, press Ctrl-Enter or Shift-Enter with the cursor inside\nor use the run button (▶) in the toolbar at the top\n\n\nIn your notebook, type the following in the first cell and then run it with Shift-Enter, you should see the same output:\n\na = 5\nb = 7\na + b\n\n12\n\n\nThe cells in a notebook are linked together so a variable defined in one is available in all the cells from that point on so in the second cell you can use the variables a and b:\n\na - b\n\n-2\n\n\nSome Python libraries have special integration with Jupyter notebooks and so can display their output directly into the page. For example pandas will format tables of data nicely and matplotlib will embed graphs directly:\n\nimport pandas as pd\ntemp = pd.DataFrame(\n    [3.1, 2.4, 4.8, 4.1, 3.4, 4.2],\n    columns=[\"temp (°C)\"],\n    index=pd.RangeIndex(2000, 2006, name=\"year\")\n)\ntemp\n\n\n\n\n\n\n\n\ntemp (°C)\n\n\nyear\n\n\n\n\n\n2000\n3.1\n\n\n2001\n2.4\n\n\n2002\n4.8\n\n\n2003\n4.1\n\n\n2004\n3.4\n\n\n2005\n4.2\n\n\n\n\n\n\n\n\ntemp.plot()\n\n\n\n\n\n\n\n\n\nMarkdown\nIf you want to write some text as documentation (like these words here) then you should label the cell as being a Markdown cell. Do that by selecting the cell and going to the dropdown at the top of the page labelled Code and changing it to Markdown.\nIt is becomming common for people to use Jupyter notebooks as a sort of lab notebook where they document their processes, interspersed with code. This style of working where you give prose and code equal weight is sometimes called literate programming.\n\n\n\n\n\n\nExercise\n\n\n\nTake the following code and break it down, chunk by chunk, interspersing it with documentation explaining what each part does using Markdown blocks:\n\nprices = {\n    \"apple\": 0.40,\n    \"banana\": 0.50,\n}\n\nmy_basket = {\n    \"apple\": 1,\n    \"banana\": 6,\n}\n\ntotal_grocery_bill = 0\nfor fruit, count in my_basket.items():\n    total_grocery_bill += prices[fruit] * count\n\nprint(f\"I owe the grocer £{total_grocery_bill:.2f}\")\n\nYou don’t need to put only one line of code per cell, it makes sense sometimes to group some lines together.\nThroughout this course, use the Jupyter Notebook to solve the problems. Follow along with the examples, typing them into your own notebooks and see how they work.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nYour notebook could look similar to:\n\nMy first notebook\nWe first define the price of apples and bananas.\n\nprices = {\n    \"apple\": 0.40,\n    \"banana\": 0.50,\n}\n\nThen we state how many apples and bananas we have in the basket.\n\nmy_basket = {\n    \"apple\": 1,\n    \"banana\": 6,\n}\n\nFor each fruit in the basket we calculate the price to pay and add it to the total bill.\n\ntotal_grocery_bill = 0\nfor fruit, count in my_basket.items():\n    total_grocery_bill += prices[fruit] * count\n\nWe print a message with the total owed to the shop.\n\nprint(f\"I owe the grocer £{total_grocery_bill:.2f}\")\n\nI owe the grocer £3.40",
    "crumbs": [
      "Working framework",
      "Getting familiar with Notebooks"
    ]
  },
  {
    "objectID": "pages/aside_one_two_dimensional.html",
    "href": "pages/aside_one_two_dimensional.html",
    "title": "Introduction to Data Anlysis in Python (Part 2)",
    "section": "",
    "text": "scitkit-learn requires the X parameter of the fit() function to be two-dimensional and the y parameter to be one-dimensional.\nX must be two-dimensional, even if there is only one feature (column) present in your data. This can sometimes be a bit confusing as to humans there’s little difference between a table with one column and a simple list of values. Computers, however are very explicit about this difference and so we need to make sure we’re doing the right thing.\nFirst, let’s grab the data we were working with:\nfrom pandas import read_csv\n\ndata = read_csv(\"https://bristol-training.github.io/applied-data-analysis-in-python/data/linear.csv\")\n\n2D DataFrames\nIf we look at it, we see it’s a pandas DataFrame which is always inherently two-dimensional:\ndata.head()\n\n\n\n\n\n\n\n\nx\n\n\ny\n\n\n\n\n\n\n0\n\n\n3.745401\n\n\n3.229269\n\n\n\n\n1\n\n\n9.507143\n\n\n14.185654\n\n\n\n\n2\n\n\n7.319939\n\n\n9.524231\n\n\n\n\n3\n\n\n5.986585\n\n\n6.672066\n\n\n\n\n4\n\n\n1.560186\n\n\n-3.358149\n\n\n\n\n\nTo get a more specific idea of the shape of the data structure, we can use the shape attribute:\ndata.shape\n(50, 2)\nThis tell us that it’s a \\((50 \\times 2)\\) structure so is two dimensional.\nTo be explicit, we can also query its dimensionality directly with ndim:\ndata.ndim\n2\n\n\n1D Series\nIf we ask a DataFrame for one of its columns, it returns it to us as a pandas Series. These objects are always one-dimensional (ignoring the potential for multi-indexes):\ndata[\"x\"].head()\n0    3.745401\n1    9.507143\n2    7.319939\n3    5.986585\n4    1.560186\nName: x, dtype: float64\ntype(data[\"x\"])\npandas.core.series.Series\ndata[\"x\"].shape\n(50,)\nNote that the shape is (50,). This might look like it could have multiple values but this is just how Python represents a tuple with one value. To check the dimensionality explicitly, we can peek at ndim again:\ndata[\"x\"].ndim\n1\n\n\n2D subsets of DataFrames\nIf we want to ask a DataFrame for a subset of its columns, it will return the answer to us as a another DataFrame as this is the only way to represent data with multiple columns.\nWe can ask for multiple columns by passing a list of column names to the DataFrame indexing operator.\nPay attention here as the outer pair of square brackets are denoting the indexing operator being called while the inner pair denotes the list being created.\ndata[[\"x\", \"y\"]].head()\n\n\n\n\n\n\n\n\nx\n\n\ny\n\n\n\n\n\n\n0\n\n\n3.745401\n\n\n3.229269\n\n\n\n\n1\n\n\n9.507143\n\n\n14.185654\n\n\n\n\n2\n\n\n7.319939\n\n\n9.524231\n\n\n\n\n3\n\n\n5.986585\n\n\n6.672066\n\n\n\n\n4\n\n\n1.560186\n\n\n-3.358149\n\n\n\n\n\ndata[[\"x\", \"y\"]].shape\n(50, 2)\nWe can see here that when we asked the DataFrame for multiple columns by passing a list of column names it returns a two-dimensional object.\nIf we want to extract just one column but still maintain the dimensionality, we can pass a list with only one column name:\ndata[[\"x\"]].head()\n\n\n\n\n\n\n\n\nx\n\n\n\n\n\n\n0\n\n\n3.745401\n\n\n\n\n1\n\n\n9.507143\n\n\n\n\n2\n\n\n7.319939\n\n\n\n\n3\n\n\n5.986585\n\n\n\n\n4\n\n\n1.560186\n\n\n\n\n\nIf we check the shape and dimensionality of this, we see that it is a \\((50 \\times 1)\\) structure with two dimensions:\ndata[[\"x\"]].shape\n(50, 1)\ndata[[\"x\"]].ndim\n2\n\n\nFinal comparison\nFinally, to reiterate, the difference between\ndata[\"x\"].head()\n0    3.745401\n1    9.507143\n2    7.319939\n3    5.986585\n4    1.560186\nName: x, dtype: float64\nand\ndata[[\"x\"]].head()\n\n\n\n\n\n\n\n\nx\n\n\n\n\n\n\n0\n\n\n3.745401\n\n\n\n\n1\n\n\n9.507143\n\n\n\n\n2\n\n\n7.319939\n\n\n\n\n3\n\n\n5.986585\n\n\n\n\n4\n\n\n1.560186\n\n\n\n\n\nis not really in the data itself, but in the mathematical structure. One is a vector and and the other is a matrix. One is one-dimensional and the other is two-dimensional.\ndata[\"x\"].ndim\n1\ndata[[\"x\"]].ndim\n2"
  },
  {
    "objectID": "pages/answer_pixel_correlation.html",
    "href": "pages/answer_pixel_correlation.html",
    "title": "Introduction to Data Anlysis in Python (Part 2)",
    "section": "",
    "text": "import numpy as np\nfrom pandas import DataFrame, Series\nfrom skimage import io\n\nphoto = io.imread(\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/97/Swallow-tailed_bee-eater_%28Merops_hirundineus_chrysolaimus%29.jpg/768px-Swallow-tailed_bee-eater_%28Merops_hirundineus_chrysolaimus%29.jpg\")\n\nphoto = np.array(photo, dtype=np.float64) / 255  # Scale values\nw, h, d = original_shape = tuple(photo.shape)  # Get the current shape\nimage_array = np.reshape(photo, (w * h, d))  # Reshape to to 2D\n\npixels = DataFrame(image_array, columns=[\"Red\", \"Green\", \"Blue\"])\npixels.corr()\n\n\n\n\n\n\n\n\nRed\n\n\nGreen\n\n\nBlue\n\n\n\n\n\n\nRed\n\n\n1.000000\n\n\n0.85383\n\n\n0.749801\n\n\n\n\nGreen\n\n\n0.853830\n\n\n1.00000\n\n\n0.781750\n\n\n\n\nBlue\n\n\n0.749801\n\n\n0.78175\n\n\n1.000000"
  },
  {
    "objectID": "pages/generate_moons.html",
    "href": "pages/generate_moons.html",
    "title": "Introduction to Data Anlysis in Python (Part 2)",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import make_moons\n\nX, y = make_moons(n_samples=500, noise=0.2, random_state=42)\nX = pd.DataFrame(X, columns=[\"x1\", \"x2\"])\n\nsns.scatterplot(data=X, x=\"x1\", y=\"x2\", hue=y, palette=\"Dark2\")\n&lt;Axes: xlabel='x1', ylabel='x2'&gt;\n\ndata = X.copy()\ndata[\"y\"] = y\ndata.to_csv(\"moons.csv\", index=False)\ncheck = pd.read_csv(\"moons.csv\")\npd.testing.assert_frame_equal(data, check)"
  },
  {
    "objectID": "pages/appendix_clustering.html",
    "href": "pages/appendix_clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is a process by which you collect a large number of data points into a smaller number of groups, based on the distances between them. It is useful in cases where the volumes of data are large and you want to extract some figures of interest. It is a type of unsupervised learning.\nA common use for clustering is identifying distinct subsets of a population, e.g. in a census.\nThere are a number of algorithms available for performing clustering but the simplest and most common is k-means clustering.\nIt works by taking the n-dimensional data provided, \\(X\\) and randomly places \\(k\\) seed points in the field which represent the centres of the initial clusters.\n\nIt then iterates over every data point in \\(X\\) and assigns each to be associated with whichever cluster centre is closest.\nOnce all points have been associated with a cluster, it then iterates over each cluster and calculates the new mean of the cluster to be the centroid of all the points assigned to it.\n\nSteps 1 and 2 are repeated until the algorithm converges on a result.\n\n\nA simple example\nLet’s start by using scikit-learn to provide us with some randonly generated data points. It provides a function called make_blobs() which creates a number of gaussian clusters.\nWe’ll ask it to create 500 points in 4 clusters. We set random_state=6 to ensure that this example will always generate the same points for reproducibility.\nfrom sklearn.datasets import make_blobs\n\ndata, true_labels = make_blobs(n_samples=500, centers=4, random_state=6)\nWe then put the data into a pandas DataFrame to give us a nicer API for working with it. We plot it to see what it looks like, colouring each point according to what cluster is was generated from.\n%matplotlib inline\n\nimport pandas as pd\n\npoints = pd.DataFrame(data, columns=[\"x1\", \"x2\"])\npoints.plot.scatter(\"x1\", \"x2\")\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe524046e50&gt;\n\nWe can see here that these clusters are very distinct. This is a very good situation to use k-means clustering in and it will give a useful result.\nWe initialise the KMeans object with the number of clusters we are looking for (n_clusters is a hyperparameter). This is important as k-means requires this decision to be made up-front. There are some clustering algorithms which can attempt to calculate the number of clusters for you but when using k-means you need to make that assessment yourself.\nThere are other hyperparameters that can be passed to KMeans which are explained in full in the documentation.\nPassing the data you want to fit to the fit() method will then actually perform the algormithm. You can pass in nested lists, numpy arrays (as long as they have the shape \\((N_{samples}, N_{features})\\)) or pandas DataFrames.\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=4, n_init=\"auto\").fit(points)\nNow that we have calculated the cluster centres, we can use the cluster_centers_ data attribute of our model to see what clusters it has decided on.\ncluster_centers = pd.DataFrame(kmeans.cluster_centers_, columns=[\"x1\", \"x2\"])\ncluster_centers\n\n\n\n\n\n\n\n\nx1\n\n\nx2\n\n\n\n\n\n\n0\n\n\n6.485156\n\n\n-9.212537\n\n\n\n\n1\n\n\n-7.857994\n\n\n1.892259\n\n\n\n\n2\n\n\n0.485425\n\n\n-1.628580\n\n\n\n\n3\n\n\n7.886559\n\n\n-3.337117\n\n\n\n\n\nComparing these \\(x\\) and \\(y\\) values against the plot above, we see that it seems to have placed the centres in the correct location. It’s better though to be able to see this directly, so let’s plot the centres on top of the original data.\nax = points.plot.scatter(\"x1\", \"x2\")\ncluster_centers.plot.scatter(\"x1\", \"x2\", ax=ax, c=\"red\", s=200, marker=\"x\")\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f75777df2b0&gt;\n\nThe other piece of data that we can retrieve from the model is which cluster it assigned to each data point. This is available as the labels_ data attribute and is an array with 500 entries, each being a number between 0 and 3. We can use it to colour our plot to see the clusters emerge.\npoints.plot.scatter(\"x1\", \"x2\", c=kmeans.labels_, colormap=\"Dark2\", colorbar=False)\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f75773114e0&gt;\n\n\nExercise 1\n\nRun the above example again but try setting different random_state values when creating the blobs. What happens when the generated clusters overlap a lot?\n\n\n\nExercise 2\n\nUsing the data provided by scikit-learn.datasets.load_iris, perform the clustering on the data.\nTip: load the data into a DataFrame with:\nfrom sklearn.datasets import load_iris\niris = pd.DataFrame(load_iris().data, columns=load_iris().feature_names)\nTip: Note that the Iris dataset has four input dimensions. You can pass all four columns into the k-means clustering algorithm with kmeans.fit(iris)\nanswer\n\n\n\nExercise 3 (optional)\n\nAnother data attribute of the model is inertia_ which gives you the sum of the squared distances of data points to their closest cluster center. It is this attribute which the algorithm uses to decide whether it has converged. In general a smaller number represents a better fit.\nWrite a loop which performs the k-means fit over the blob-generated data with the number of clusters varying from 2 up to 7. For each of these fits, extract the value of the inertia_ attribute and draw a plot of inertia against number of clusters.\nanswer\n\n\n\n\nOther clustering algorithms\nThere are different clustering algorithms beyond k-means. scikit-learn come with many and you can see them all in the documentation.\nk-means famously has problems with clusters which are elongated in some direction as it assumes sphericity. Look at the fourth row of this image (where the k-means we are using is the one in the first column):\n\nIt has failed to cluster the three groups in the way that we as humans would have done. Other algorithms (such as DBSCAN) perform better in that situation.\nOne solution with k-means is to transform the data in some way to make it more spherical and then apply clustering. This pre-processing can be done with something like PCA.\nThe other type of preprocessing you may like to do is to manually reparameterise the data. For example, the first row has two circular clusters which share a centre. Since k-means works on cluster-centres it can’t tell them apart. However, if that data was reparameterised into \\((r, \\theta)\\) then the two clusters would become distinct and k-means would work well.\nThis is why it’s always worth plotting your data and deciding how to process it before throwing machine learning algorithms at it."
  },
  {
    "objectID": "pages/appendix_scaling.html",
    "href": "pages/appendix_scaling.html",
    "title": "Feature scaling",
    "section": "",
    "text": "An important part of any data analysis task is preparing your data. Depending on the tak you are trying to do, and the algorithms you are using, the data preparation steps will vary. In this section we will learn about feature scaling and dimensionality reduction.\nLet’s look at some data representing wine. The data contains information on 178 samples of wine, measuring a number of things, including the alcoholic content, the amount of magnesium, the amount of proline and many more. Each sample also has a class associated with it, representing the variety of the grape used.\nfrom sklearn.datasets import load_wine\n\nX, y = load_wine(as_frame=True, return_X_y=True)\ny = y.astype(\"category\")  # This makes seaborn use the right colour palette\nX\n\n\n\n\n\n\n\n\nalcohol\n\n\nmalic_acid\n\n\nash\n\n\nalcalinity_of_ash\n\n\nmagnesium\n\n\ntotal_phenols\n\n\nflavanoids\n\n\nnonflavanoid_phenols\n\n\nproanthocyanins\n\n\ncolor_intensity\n\n\nhue\n\n\nod280/od315_of_diluted_wines\n\n\nproline\n\n\n\n\n\n\n0\n\n\n14.23\n\n\n1.71\n\n\n2.43\n\n\n15.6\n\n\n127.0\n\n\n2.80\n\n\n3.06\n\n\n0.28\n\n\n2.29\n\n\n5.64\n\n\n1.04\n\n\n3.92\n\n\n1065.0\n\n\n\n\n1\n\n\n13.20\n\n\n1.78\n\n\n2.14\n\n\n11.2\n\n\n100.0\n\n\n2.65\n\n\n2.76\n\n\n0.26\n\n\n1.28\n\n\n4.38\n\n\n1.05\n\n\n3.40\n\n\n1050.0\n\n\n\n\n2\n\n\n13.16\n\n\n2.36\n\n\n2.67\n\n\n18.6\n\n\n101.0\n\n\n2.80\n\n\n3.24\n\n\n0.30\n\n\n2.81\n\n\n5.68\n\n\n1.03\n\n\n3.17\n\n\n1185.0\n\n\n\n\n3\n\n\n14.37\n\n\n1.95\n\n\n2.50\n\n\n16.8\n\n\n113.0\n\n\n3.85\n\n\n3.49\n\n\n0.24\n\n\n2.18\n\n\n7.80\n\n\n0.86\n\n\n3.45\n\n\n1480.0\n\n\n\n\n4\n\n\n13.24\n\n\n2.59\n\n\n2.87\n\n\n21.0\n\n\n118.0\n\n\n2.80\n\n\n2.69\n\n\n0.39\n\n\n1.82\n\n\n4.32\n\n\n1.04\n\n\n2.93\n\n\n735.0\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n173\n\n\n13.71\n\n\n5.65\n\n\n2.45\n\n\n20.5\n\n\n95.0\n\n\n1.68\n\n\n0.61\n\n\n0.52\n\n\n1.06\n\n\n7.70\n\n\n0.64\n\n\n1.74\n\n\n740.0\n\n\n\n\n174\n\n\n13.40\n\n\n3.91\n\n\n2.48\n\n\n23.0\n\n\n102.0\n\n\n1.80\n\n\n0.75\n\n\n0.43\n\n\n1.41\n\n\n7.30\n\n\n0.70\n\n\n1.56\n\n\n750.0\n\n\n\n\n175\n\n\n13.27\n\n\n4.28\n\n\n2.26\n\n\n20.0\n\n\n120.0\n\n\n1.59\n\n\n0.69\n\n\n0.43\n\n\n1.35\n\n\n10.20\n\n\n0.59\n\n\n1.56\n\n\n835.0\n\n\n\n\n176\n\n\n13.17\n\n\n2.59\n\n\n2.37\n\n\n20.0\n\n\n120.0\n\n\n1.65\n\n\n0.68\n\n\n0.53\n\n\n1.46\n\n\n9.30\n\n\n0.60\n\n\n1.62\n\n\n840.0\n\n\n\n\n177\n\n\n14.13\n\n\n4.10\n\n\n2.74\n\n\n24.5\n\n\n96.0\n\n\n2.05\n\n\n0.76\n\n\n0.56\n\n\n1.35\n\n\n9.20\n\n\n0.61\n\n\n1.60\n\n\n560.0\n\n\n\n\n\n178 rows × 13 columns\n\n\nTo keep things easier to visualise, we’ll just grab two of the features:\nX_subset = X[[\"alcohol\", \"proline\"]]\nFirst, let’s have a look at the data and see how it’s distributed:\nimport seaborn as sns\nimport pandas as pd\n\nsns.relplot(data=X, x=\"alcohol\", y=\"proline\", hue=y, palette=\"Dark2\")\n&lt;seaborn.axisgrid.FacetGrid at 0x7fede99daf90&gt;\n\nThe classes look relatively distinct from each other so a k-nearest neighbours should do the trick. First we split our data in test and train (the _s suffix represents our 2-feature subset):\nfrom sklearn.model_selection import train_test_split\n\ntrain_X_s, test_X_s, train_y_s, test_y_s = train_test_split(X_subset, y, random_state=42)\nThen we fit and score our model, without any further tweaking or tuning:\nfrom sklearn.neighbors import KNeighborsClassifier\n\ndirect_knn = KNeighborsClassifier()\ndirect_knn.fit(train_X_s, train_y_s)\ndirect_knn.score(test_X_s, test_y_s)\n0.6666666666666666\nThat looks like it’s worked, but I would expect a higher score for such a simple data set. Getting a third of the data wrong seems high.\nLet’s use our visualisation function again to see of there’s anything obviously wrong:\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nDecisionBoundaryDisplay.from_estimator(direct_knn, X_subset, cmap=\"Pastel2\")\nsns.scatterplot(data=X_subset, x=\"alcohol\", y=\"proline\", hue=y, palette=\"Dark2\")\n&lt;Axes: xlabel='alcohol', ylabel='proline'&gt;\n\nWe can immediately see here that there’s some issue. The kNN algorithm should be creating nice smooth and round regions in the data but they seem here to be be very horizontally striped.\nOne potential reason for this could be that the number of neighbours is incorrect, so you could run a grid search to check the best value for the hyperparameter. In this case however, there’s a more fundamenetal issue with the data.\nIf you look at the absolute values on the \\(x_1\\) and \\(x_2\\) axes you’ll see that the ranges over which they vary are vastly different. The alcohol data goes from 11 to 15, but the proline data goes from 300 to 1700. This causes a problem with algorithms like kNN as the metric that they use to classify is based on euclidean distance. That means that it assumes that a distance of \\(\\Delta x_1\\) has the same importance as a distance \\(\\Delta x_2\\). If we plot our data the way that kNN sees it, it looks like:\nsns.relplot(data=X, x=\"alcohol\", y=\"proline\", hue=y, palette=\"Dark2\").set(aspect=\"equal\", xlim=(-800, 800), ylim=(200, 1800))\n&lt;seaborn.axisgrid.FacetGrid at 0x7fede5db1d50&gt;\n\nThere’s such a vast difference in the scales that they look like they’re all in a straight line! It’s clear that the values in the two directions are not equally weighted, so we need to do something about it.\nThe standard way is to take the values in each feature, \\(x\\) and calculate their mean, \\(\\mu\\), and standard deviation, \\(\\sigma\\) and scale the values such that \\(x^{\\prime} = \\frac{x - \\mu}{\\sigma}\\).\nscikit-learn provides a scaler called StandardScaler which can do this for you. It works like any other scikit-learn model and so has a fit() method:\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(X)\n\n\n\nStandardScaler()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n  StandardScaler?Documentation for StandardScaleriFitted\n\nStandardScaler()\n\n\n\n\n\nAt this point, the scaler has calculated the \\(\\mu\\) and \\(\\sigma\\) for each feature (you can check with scaler.mean_ and scaler.var_). We can ask it to actually perform the scaling by calling the transform() method. This returns a numpy array, so if we want to keep it as as a pandas Dataframe, we need to explicitly convert it back:\nX_scaled_raw = scaler.transform(X)\nX_scaled = pd.DataFrame(X_scaled_raw, columns=X.columns)\nPlotting the result shows a nice balanced spread of data (note the new scales on the axes):\nsns.relplot(data=X_scaled, x=\"alcohol\", y=\"proline\", hue=y, palette=\"Dark2\")\n&lt;seaborn.axisgrid.FacetGrid at 0x7fede5364690&gt;\n\n\nPipelines\nTo incorporate this scaling process into our model, we need to make sure that: 1. the scaling factors are calculated once and are not changed from that point on, 2. the scaling is applied to the training data, 3. the scaling is applied to the test data, 4. the scaling is applied to any data used in a prediction.\nThe easiest way to ensure this is to use a pipeline. This lets us assemble multiple steps together and scikit-learn knows how to pass data between them.\nfrom sklearn.pipeline import make_pipeline\n\nscaled_knn = make_pipeline(\n    StandardScaler(),\n    KNeighborsClassifier()\n)\nscaled_knn\n\n\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('kneighborsclassifier', KNeighborsClassifier())])\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n\n  Pipeline?Documentation for PipelineiNot fitted\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('kneighborsclassifier', KNeighborsClassifier())])\n\n\n\n\n\n\n StandardScaler?Documentation for StandardScaler\n\nStandardScaler()\n\n\n\n\n\n KNeighborsClassifier?Documentation for KNeighborsClassifier\n\nKNeighborsClassifier()\n\n\n\n\n\n\n\nA pipeline acts like any other scikit-learn model so we can still use our fit, score and predict methods. Any data we pass in will be passed through all the steps in the correct way:\nscaled_knn.fit(train_X_s, train_y_s)\nscaled_knn.score(test_X_s, test_y_s)\n0.8666666666666667\nOur training data has been used to calculate the scaling parameters, has been transformed and then used to fit the kNN model. The test data was then passed through to be scaled and scored using the kNN model.\nWe can plot our decision boundary again and now it’s looking much more reasonable:\nDecisionBoundaryDisplay.from_estimator(scaled_knn, X_subset, cmap=\"Pastel2\")\nsns.scatterplot(data=X_subset, x=\"alcohol\", y=\"proline\", hue=y, palette=\"Dark2\")\n&lt;Axes: xlabel='alcohol', ylabel='proline'&gt;\n\n\n\nPrincipal component analysis\nIn our data so far, we have used just two of the feature columns in our data. We primarily did this to make plotting the data easier to understand. There’s nothing stopping us from passing all our features to our pipeline:\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, random_state=42)  # re-split using all columns\nscaled_knn_all = make_pipeline(\n    StandardScaler(),\n    KNeighborsClassifier()\n)\nscaled_knn_all\n\n\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('kneighborsclassifier', KNeighborsClassifier())])\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n\n  Pipeline?Documentation for PipelineiNot fitted\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('kneighborsclassifier', KNeighborsClassifier())])\n\n\n\n\n\n\n StandardScaler?Documentation for StandardScaler\n\nStandardScaler()\n\n\n\n\n\n KNeighborsClassifier?Documentation for KNeighborsClassifier\n\nKNeighborsClassifier()\n\n\n\n\n\n\n\nscaled_knn_all.fit(train_X, train_y)\nscaled_knn_all.score(test_X, test_y)\n0.9555555555555556\nSo, adding more features does seem to have improved our score. However, many algorithms (including kNN) are disproportionaly negatively affected by an increase in the number of features. This is due to something called the curse of dimensionality. Our data points become much further apart in n-dimensional space and so the definition of a neighbourhood becomes harder to measure.\nThe solution to this is dimensionality reduction which aims to identify either the most important features, or to find useful combinations of features, which still retain enough of the information in the system.\nThe most common of these is principal component analysis which makes linear combinations of the features into new features. For example, instead of selecting two features (alcohol and proline) to input into the model, we can use PCA to calculate the two most important principal components by combining all the available features.\nThis step can be included in our pipeline, just after the StandardScaler:\nfrom sklearn.decomposition import PCA\n\nscaled_pca_knn = make_pipeline(\n    StandardScaler(),\n    PCA(n_components=2),  # PCA with 2 components\n    KNeighborsClassifier()\n)\nscaled_pca_knn\n\n\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('pca', PCA(n_components=2)),\n                ('kneighborsclassifier', KNeighborsClassifier())])\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n\n  Pipeline?Documentation for PipelineiNot fitted\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('pca', PCA(n_components=2)),\n                ('kneighborsclassifier', KNeighborsClassifier())])\n\n\n\n\n\n\n StandardScaler?Documentation for StandardScaler\n\nStandardScaler()\n\n\n\n\n\n PCA?Documentation for PCA\n\nPCA(n_components=2)\n\n\n\n\n\n KNeighborsClassifier?Documentation for KNeighborsClassifier\n\nKNeighborsClassifier()\n\n\n\n\n\n\n\nscaled_pca_knn.fit(train_X, train_y)\nscaled_pca_knn.score(test_X, test_y)\n0.9777777777777777\nThat seems to have helped the model.\nIf we want to plot our decision boundary, we need to do a little more work to pull out the transformation steps from the final kNN step. You can access the steps in a pipeline like a Python list:\ntransformer_steps = scaled_pca_knn[:-1]  # all except the last step\nknn_step = scaled_pca_knn[-1]  # only the last step\nso then we can transform the data and pass it, along with the kNN step, into our plotting function:\ntransformed_X = pd.DataFrame(transformer_steps.transform(X))\n\nDecisionBoundaryDisplay.from_estimator(knn_step, transformed_X, cmap=\"Pastel2\")\nsns.scatterplot(data=transformed_X, x=0, y=1, hue=y, palette=\"Dark2\")\n&lt;Axes: xlabel='0', ylabel='1'&gt;\n\nThat data is looking nicely separated by class and the decision boundaries are smooth.\n\nNumber of principal components\nIn the example above, I chose n_components=2 so that we’d be able to plot the decision boundary easily. There is however, no reason to limit ourselves to this. Every additional principal component we include increases the amount of information that’s included. We can see how much of the variance in the data is explained by each component:\nscaled_pca_knn[\"pca\"].explained_variance_ratio_\narray([0.3639525 , 0.18617733])\nSo the first provides 36% and the second 19%. It’s more useful to look at the sum of the included components (as you include more components, the number will get closer to 100%):\nsum(scaled_pca_knn[\"pca\"].explained_variance_ratio_)\n0.5501298349276385\nSo only about half the variance was explained by the first two components, but it’s sufficient to have got a very good score previously.\nWe can use the GridSearchCV tool to try different values and see which works best:\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\n\nscaled_pca_knn_cv = GridSearchCV(\n    make_pipeline(\n        StandardScaler(),\n        PCA(),\n        KNeighborsClassifier()\n    ),\n    {\n        \"pca__n_components\" : range(1, 5),\n    }\n)\nscaled_pca_knn_cv\n\n\n\nGridSearchCV(estimator=Pipeline(steps=[('standardscaler', StandardScaler()),\n                                       ('pca', PCA()),\n                                       ('kneighborsclassifier',\n                                        KNeighborsClassifier())]),\n             param_grid={'pca__n_components': range(1, 5)})\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n\n  GridSearchCV?Documentation for GridSearchCViNot fitted\n\nGridSearchCV(estimator=Pipeline(steps=[('standardscaler', StandardScaler()),\n                                       ('pca', PCA()),\n                                       ('kneighborsclassifier',\n                                        KNeighborsClassifier())]),\n             param_grid={'pca__n_components': range(1, 5)})\n\n\n\n\n\n\n\n\nestimator: Pipeline\n\nPipeline(steps=[('standardscaler', StandardScaler()), ('pca', PCA()),\n                ('kneighborsclassifier', KNeighborsClassifier())])\n\n\n\n\n\n\n\n\n StandardScaler?Documentation for StandardScaler\n\nStandardScaler()\n\n\n\n\n\n PCA?Documentation for PCA\n\nPCA()\n\n\n\n\n\n KNeighborsClassifier?Documentation for KNeighborsClassifier\n\nKNeighborsClassifier()\n\n\n\n\n\n\n\n\n\n\n\n\nscaled_pca_knn_cv.fit(train_X, train_y)\nscaled_pca_knn_cv.score(test_X, test_y)\n0.9777777777777777\nscaled_pca_knn_cv.best_estimator_[\"pca\"].n_components_\n3\n\n\nExercise\nTake the Iris data set from sklearn and make a classifier which can predict the species. Try building up your solution in the following steps: 1. First use a kNN by itself 2. Add in a feature-scaling step 3. Add in a PCA step\nYou can load the data with\nfrom sklearn.datasets import load_iris\n\nX, y = load_iris(as_frame=True, return_X_y=True)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Trainer notes\n\n\n\n\n\nThis course is intended to be split in three sessions comprising:\n\nGetting familiar with the working framework\n\nInstall Jupyter Notebooks\nWrite interactive data analysis with python code, markdown and inline visualizations\n\nUnderstand the process of training a machine learning model\n\nModel fitting\nTrain and test sets split\nHyperparameter optimization\nEvaluation\n\nWork on field-specific examples\n\nThe course can be delivered in-person and on-line, but it involves a strong component of self-led learning.\n\n\n\nThis course is aimed at the Python developer who wants to learn how to do useful data analysis tasks. Over the years, Python has become a very popular tool for analysing data. These days it comes with support from many tools to do machine learning, data querying, neural networks and exploratory analysis.\nIn this course we will investigate the use of scikit-learn for machine learning to discover things about whatever data may come across your desk. We will see in detail how to building and evaluate machine learning models to make data-driven decisions.\nFor the purpose of this course we will be using a free tool called Jupyter Notebooks which provides you with a local editor and Python terminal in your web browser. Setting up instructions can be found here.\n\nIntended learning outcomes\nBy the end of this course, you will:\n\nKnow how to use Jupyter Notebooks.\nBe familiar with scikit-learn, pandas and seaborn.\nUnderstand the machine learning\n\nSelect suitable models\nSplit Data into training and test sets\nFit models to data\nHyperparameter tuning using grid search\nUnderstand how to evaluate model performance\n\n\nLet’s embark on this exciting journey into the world of data analysis with Python!",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "pages/999-contributors.html",
    "href": "pages/999-contributors.html",
    "title": "Contributors",
    "section": "",
    "text": "This course was originally written by Matt Williams, see https://milliams.com/courses/intermediate_python/.\nThe course has since been modified by the Jean Golding Institute team:\n\nPau Erola\nDan Lawson\nJames Thomas\nLeo Gorman\n\n\nResearch advocates\n\nZhiyuan Xu\nZhengzhe Peng\nWinfred Gatua\nVaishnudebi Dutta\nRuolin Wu\nCatherine Upex\nBryony Clifton\nBoyi Li",
    "crumbs": [
      "About",
      "Contributors"
    ]
  },
  {
    "objectID": "pages/answer_iris_correlation.html",
    "href": "pages/answer_iris_correlation.html",
    "title": "Introduction to Data Anlysis in Python (Part 2)",
    "section": "",
    "text": "from pandas import DataFrame\nfrom sklearn.datasets import load_iris\n\niris, iris_target = load_iris(as_frame=True, return_X_y=True)\niris.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\n\n\nsepal width (cm)\n\n\npetal length (cm)\n\n\npetal width (cm)\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\n\n\n2\n\n\n4.7\n\n\n3.2\n\n\n1.3\n\n\n0.2\n\n\n\n\n3\n\n\n4.6\n\n\n3.1\n\n\n1.5\n\n\n0.2\n\n\n\n\n4\n\n\n5.0\n\n\n3.6\n\n\n1.4\n\n\n0.2\n\n\n\n\n\ncorr = iris.corr()\n%matplotlib inline\n\nimport seaborn as sns\n\nsns.heatmap(corr, vmin=-1.0, vmax=1.0, cmap=\"RdBu\")\n&lt;AxesSubplot: &gt;\n\nfrom pandas.plotting import scatter_matrix\n\na = scatter_matrix(iris, figsize=(16, 16), c=iris_target)"
  },
  {
    "objectID": "pages/999-how-to.html",
    "href": "pages/999-how-to.html",
    "title": "How to read",
    "section": "",
    "text": "How to read this documentation\nIn this documentation, any time that we are seeing a small snippet of Python code, we’ll see it written in a grey box like the following:\nprint(\"Hello, Python\")\nIf the commands are executed by the machine we will see the output of them below enclosed on a vertical purple line:\n\nprint(\"Hello, Python!\")\n\nHello, Python!\n\n\nBy contrast, you will see larger peces of code as scripts with a given name, e.g. script.py, in a code block with darker header:\n\n\nscript.py\n\ngreeting = \"Hello\"\nname = input(\"What is your name? \")\nprint(greeting, name)\n\nWe may ask you to run a script using the Command Prompt (Windows) or Terminal (Mac and Linux). We will show you what commands to run and will look like this:\n\n\nTerminal/Command Prompt\n\npython script.py\n\nPlease note that sometimes we will skip showing the execution of scripts on the Terminal/Command Prompt box, but we will assume you to run the script on your.\nIn some cases we will introduce general programming concepts and structures using pseudocode, a high-level, easy-to-read syntax close to natural language. This should not be confused with Python code and cannot be executed on your machine, but it is useful to describe how your code should behave. Here there is an example:\nFOR EACH sample IN my_study\n    IF (sample.value &gt; 100)\n        DO SOMETHING\n    OTHERWISE\n        DO SOMETHING ELSE\nEach section will start with a learning outcomes box that highlight the important activities in the section, like this one:\n\n\n\n\n\n\nIn this section…\n\n\n\nThese are important concepts and technical notes.\n\n\n\n\n\n\n\n\nTroubleshooting…\n\n\n\nThese highlight common issues and how to solve them.\n\n\nThere are some exercises along this course, and it is important you try to answer them yourself to understand how Python works. Exercises are shown in blue boxes followed by a yellow box that contains the answer of each exercise. We recommend you to try to answer each exercise yourself before looking at the solution.\n\n\n\n\n\n\nExercise\n\n\n\nThis is an exercise. You will need to click in the below box to see the answer.\n\n\n\n\n\n\n\n\nAnswer (click to open)\n\n\n\n\n\nThis is the answer.\n\n\n\nLast, some notes for the trainers are provided in red boxes like the one below. The notes are hidden by default but can be read by all.\n\n\n\n\n\n\nTrainer notes\n\n\n\n\n\nThis are some suggestions on how to deliver the section and some key points.",
    "crumbs": [
      "About",
      "How to read"
    ]
  },
  {
    "objectID": "pages/answer_cluster_bars.html",
    "href": "pages/answer_cluster_bars.html",
    "title": "Introduction to Data Anlysis in Python (Part 2)",
    "section": "",
    "text": "import numpy as np\nfrom pandas import DataFrame, Series\nfrom skimage import io\nfrom sklearn.cluster import KMeans\n\nphoto = io.imread(\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/97/Swallow-tailed_bee-eater_%28Merops_hirundineus_chrysolaimus%29.jpg/768px-Swallow-tailed_bee-eater_%28Merops_hirundineus_chrysolaimus%29.jpg\")\n\nphoto = np.array(photo, dtype=np.float64) / 255  # Scale values\nw, h, d = original_shape = tuple(photo.shape)  # Get the current shape\nimage_array = np.reshape(photo, (w * h, d))  # Reshape to to 2D\n\npixels = DataFrame(image_array, columns=[\"Red\", \"Green\", \"Blue\"])\n\npixels_sample = pixels.sample(frac=0.05)\n\nkmeans = KMeans(n_clusters=10, n_init=\"auto\").fit(pixels_sample[[\"Red\", \"Green\", \"Blue\"]])\n\nlabels = kmeans.predict(pixels[[\"Red\", \"Green\", \"Blue\"]])\n%matplotlib inline\n\nSeries(labels).value_counts(sort=False).plot.bar(color=kmeans.cluster_centers_)\n&lt;Axes: &gt;"
  },
  {
    "objectID": "pages/generate_overfit.html",
    "href": "pages/generate_overfit.html",
    "title": "Introduction to Data Anlysis in Python (Part 2)",
    "section": "",
    "text": "%matplotlib inline\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.colors import to_rgb, rgb_to_hsv, hsv_to_rgb\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\n\nplt.xkcd(scale=0.5)\n\np = sns.color_palette(\"colorblind\")\n\nTRUE_COLOR = p[1]\nSAMPLES_COLOR = p[0]#\"steelblue\"\nPOLY_3_COLOR = p[2]#\"darkorange\"\nPOLY_15_COLOR = p[3]#\"darkgreen\"\nTRAIN_COLOR = p[4]#\"crimson\"\nTEST_COLOR = p[5]#\"darkolivegreen\"\n\ndef true_fun(x):\n    return np.cos(1.5 * np.pi * x)\n\nrng = np.random.RandomState(0)\n\nn_samples = 30\n\nx = np.sort(rng.rand(n_samples))\nX = x[:, np.newaxis]\ny = true_fun(x) + rng.randn(n_samples) * 0.1\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, random_state=0)\n\ndef poly_fit(X, y, degree):\n    pipeline = make_pipeline(\n        PolynomialFeatures(degree=degree, include_bias=False),\n        LinearRegression(),\n    )\n    pipeline.fit(X, y)\n    return pipeline\n\ndef start_plot():\n    fig, ax = plt.subplots(constrained_layout=True)\n    return fig, ax\n\ndef finalise_plot(ax):\n    ax.set_xticks(())\n    ax.set_yticks(())\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n    ax.set_xlim((0, 1))\n    ax.set_ylim((-2, 5))\n    ax.legend(loc=\"best\")\n    \ndef plot_model(ax, model, color, label=\"Model\"):\n    X_test = np.linspace(0, 1, 500)[:, np.newaxis]\n    y_pred = model.predict(X_test)\n    ax.plot(X_test, y_pred, label=label, color=color, linestyle=\"--\")\n\ndef plot_true(ax):\n    X_test = np.linspace(0, 1, 100)[:, np.newaxis]\n    ax.plot(X_test, true_fun(X_test), label=\"True function\", color=TRUE_COLOR)\n    \ndef plot_samples(ax, X, y, label, color):\n    edgecolor = hsv_to_rgb(rgb_to_hsv(to_rgb(color)) * [1.0, 0.9, 1.0])\n    ax.scatter(X, y, edgecolor=edgecolor, facecolor=color, s=50, label=label, zorder=100)\n\nTrue function to samples\nimport logging\n# Hide the \"findfont: Font family\" warnings\nlogging.getLogger('matplotlib.font_manager').disabled = True\nfig, ax = start_plot()\nplot_true(ax)\nfinalise_plot(ax)\nfig.savefig(\"overfit_just_model.svg\")\n\nfig, ax = start_plot()\nplot_true(ax)\nplot_samples(ax, X, y, \"Samples\", SAMPLES_COLOR)\nfinalise_plot(ax)\nfig.savefig(\"overfit_model.svg\")\n\nfig, ax = start_plot()\nplot_samples(ax, X, y, \"Samples\", SAMPLES_COLOR)\nfinalise_plot(ax)\nfig.savefig(\"overfit_samples.svg\")\n\n\n\nFitting with different degrees\nmodel = poly_fit(X, y, 3)\n\nfig, ax = start_plot()\nplot_samples(ax, X, y, \"Samples\", SAMPLES_COLOR)\nplot_model(ax, model, POLY_3_COLOR, \"$3^{rd}$ degree\")\nfinalise_plot(ax)\nfig.savefig(\"overfit_3.svg\")\n\nmodel = poly_fit(X, y, 14)\n\nfig, ax = start_plot()\nplot_samples(ax, X, y, \"Samples\", SAMPLES_COLOR)\nplot_model(ax, model, POLY_15_COLOR, \"$15^{th}$ degree\")\nfinalise_plot(ax)\nfig.savefig(\"overfit_15.svg\")\n\nmodel_3 = poly_fit(X, y, 3)\nmodel_15 = poly_fit(X, y, 14)\n\nfig, ax = start_plot()\nplot_samples(ax, X, y, \"Samples\", SAMPLES_COLOR)\nplot_model(ax, model_15, POLY_15_COLOR, \"$15^{th}$ degree\")\nplot_model(ax, model_3, POLY_3_COLOR, \"$3^{rd}$ degree\")\nfinalise_plot(ax)\nfig.savefig(\"overfit_3_15.svg\")\n\n\n\nTrain test split\nfig, ax = start_plot()\nplot_samples(ax, train_X, train_y, \"Training data\", TRAIN_COLOR)\nplot_samples(ax, test_X, test_y, \"Test data\", TEST_COLOR)\nfinalise_plot(ax)\nfig.savefig(\"overfit_split.svg\")\n\n\n\n15th order overfits\nmodel = poly_fit(train_X, train_y, 14)\n\nfig, ax = start_plot()\nplot_samples(ax, train_X, train_y, \"Training data\", TRAIN_COLOR)\nplot_model(ax, model, POLY_15_COLOR, \"$15^{th}$ degree\")\nfinalise_plot(ax)\nfig.savefig(\"overfit_15_train.svg\")\n\nmodel = poly_fit(train_X, train_y, 14)\n\nfig, ax = start_plot()\nplot_samples(ax, train_X, train_y, \"Training data\", TRAIN_COLOR)\nplot_samples(ax, test_X, test_y, \"Test data\", TEST_COLOR)\nplot_model(ax, model, POLY_15_COLOR, \"$15^{th}$ degree\")\nfinalise_plot(ax)\nfig.savefig(\"overfit_15_split.svg\")\n\nmodel = poly_fit(train_X, train_y, 14)\n\nfig, ax = start_plot()\nplot_samples(ax, test_X, test_y, \"Test data\", TEST_COLOR)\nplot_model(ax, model, POLY_15_COLOR, \"$15^{th}$ degree\")\nfinalise_plot(ax)\nfig.savefig(\"overfit_15_test.svg\")\n\n\n\n3rd order does not overfit\nmodel = poly_fit(train_X, train_y, 3)\n\nfig, ax = start_plot()\nplot_samples(ax, train_X, train_y, \"Training data\", TRAIN_COLOR)\nplot_samples(ax, test_X, test_y, \"Test data\", TEST_COLOR)\nplot_model(ax, model, POLY_3_COLOR, \"$3^{rd}$ degree\")\nfinalise_plot(ax)\nfig.savefig(\"overfit_3_split.svg\")\n\nmodel = poly_fit(train_X, train_y, 3)\n\nfig, ax = start_plot()\nplot_samples(ax, test_X, test_y, \"Test data\", TEST_COLOR)\nplot_model(ax, model, POLY_3_COLOR, \"$3^{rd}$ degree\")\nfinalise_plot(ax)\nfig.savefig(\"overfit_15_test.svg\")\n\n\n\nCompare 3rd and 15th orders directly\nmodel_3 = poly_fit(train_X, train_y, 3)\nmodel_15 = poly_fit(train_X, train_y, 14)\n\nfig, ax = start_plot()\nplot_samples(ax, train_X, train_y, \"Training data\", TRAIN_COLOR)\nplot_samples(ax, test_X, test_y, \"Test data\", TEST_COLOR)\nplot_model(ax, model_15, POLY_15_COLOR, \"$15^{th}$ degree\")\nplot_model(ax, model_3, POLY_3_COLOR, \"$3^{rd}$ degree\")\nfinalise_plot(ax)\nfig.savefig(\"overfit_3_15_split.svg\")\n\nmodel_3 = poly_fit(train_X, train_y, 3)\nmodel_15 = poly_fit(train_X, train_y, 14)\n\nfig, ax = start_plot()\nplot_samples(ax, test_X, test_y, \"Test data\", TEST_COLOR)\nplot_model(ax, model_15, POLY_15_COLOR, \"$15^{th}$ degree\")\nplot_model(ax, model_3, POLY_3_COLOR, \"$3^{rd}$ degree\")\nfinalise_plot(ax)\nfig.savefig(\"overfit_3_15_split.svg\")"
  },
  {
    "objectID": "pages/600-correlation.html",
    "href": "pages/600-correlation.html",
    "title": "Correlation",
    "section": "",
    "text": "When presented with a new collection of data, one of the first questions you may ask is how they are related to each other. This can involve deep study of how one parameter is likely to vary as you change another but the simplest start is to look a the linear correlation between them.\nCorrelation is usually taught as being the degree to which two variables are linearly related, that is as one increases, on average how much does the other one increase. This is a useful measure because it’s easy to calculate and most data only have either linear relationships or no relationship at all.\n\nHowever, correlation is a much broader idea than that and when doing machine learning, it’s worth understanding the bigger picture. At its core, correlation is a measure of how related two data sets are. The way I like to think of it is, if I know the value of one of the two ariables, how much information do I have about the value of the other.\nTo highlight this, consider the following two variables, \\(x\\) and \\(y\\):\n\nThey have a linear correlation of zero (on average as \\(x\\) increases, \\(y\\) stays the same) but if you know the value of \\(X\\), you clearly have information about what the value of \\(y\\) is likely to be.\nThe other way to think about it is in terms of mutual information. \\(y\\) is clearly sharing information with \\(x\\), otherwise there would be no visible pattern.\n\nMultiple cross-correlation\nIt’s very common when working on real data that you have more than two figures of interest.\nTo get a sense of some real data, let’s look at a housing dataset provided by scikit-learn.\n\nfrom sklearn.datasets import fetch_california_housing\n\nhousing, target = fetch_california_housing(as_frame=True, return_X_y=True)\n\n\nhousing.head()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n\n\n\n\n\n\n\nIt has a row for each census block and a column for each feature, e.g. “median income of the block”, “average house age of the block” etc.\nTo get the linear correlation between all these features, we call the corr() method on the DataFrame:\n\nhousing.corr()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\n\n\n\n\nMedInc\n1.000000\n-0.119034\n0.326895\n-0.062040\n0.004834\n0.018766\n-0.079809\n-0.015176\n\n\nHouseAge\n-0.119034\n1.000000\n-0.153277\n-0.077747\n-0.296244\n0.013191\n0.011173\n-0.108197\n\n\nAveRooms\n0.326895\n-0.153277\n1.000000\n0.847621\n-0.072213\n-0.004852\n0.106389\n-0.027540\n\n\nAveBedrms\n-0.062040\n-0.077747\n0.847621\n1.000000\n-0.066197\n-0.006181\n0.069721\n0.013344\n\n\nPopulation\n0.004834\n-0.296244\n-0.072213\n-0.066197\n1.000000\n0.069863\n-0.108785\n0.099773\n\n\nAveOccup\n0.018766\n0.013191\n-0.004852\n-0.006181\n0.069863\n1.000000\n0.002366\n0.002476\n\n\nLatitude\n-0.079809\n0.011173\n0.106389\n0.069721\n-0.108785\n0.002366\n1.000000\n-0.924664\n\n\nLongitude\n-0.015176\n-0.108197\n-0.027540\n0.013344\n0.099773\n0.002476\n-0.924664\n1.000000\n\n\n\n\n\n\n\nHere we see the features in our data set along both the rows and the columns. The correlation between each pair is given as a number between -1.0 and 1.0 where -1.0 is absolute inverse linear correlation, 1.0 is absolute positive linear correlation and zero is no linear correlation.\nWe see the the 1.0 occuring on the diagonal (because a variable is always completely correlated with itself) and a whole range of values between -1.0 and 1.0 off-diagonal.\nIf we want the correlation between two specific columns then we can request it from this object:\n\ncorr = housing.corr()\ncorr[\"MedInc\"][\"AveRooms\"]\n\nnp.float64(0.32689543164129786)\n\n\n\n\n\n\n\n\nExercise\n\n\n\nLook through the table manually and see if you can find the most negative and most positive correlations.\nBonus: Try to automate that search using Python code. - Hint: To find the minimum, use the min() and idxmin() methods. To find the maximum, hide the diagonals first using np.fill_diagonal(corr.values, np.nan)\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nLet’s find the most negative and the most positive (ignoring self-correlation) values\nfrom pandas import DataFrame\nfrom sklearn.datasets import fetch_california_housing\n\nhousing_data = fetch_california_housing()\nhousing = DataFrame(housing_data.data, columns=housing_data.feature_names)\n\ncorr = housing.corr()\n\ncorr\n\n\n\n\n\n\n\n\nMedInc\n\n\nHouseAge\n\n\nAveRooms\n\n\nAveBedrms\n\n\nPopulation\n\n\nAveOccup\n\n\nLatitude\n\n\nLongitude\n\n\n\n\n\n\nMedInc\n\n\n1.000000\n\n\n-0.119034\n\n\n0.326895\n\n\n-0.062040\n\n\n0.004834\n\n\n0.018766\n\n\n-0.079809\n\n\n-0.015176\n\n\n\n\nHouseAge\n\n\n-0.119034\n\n\n1.000000\n\n\n-0.153277\n\n\n-0.077747\n\n\n-0.296244\n\n\n0.013191\n\n\n0.011173\n\n\n-0.108197\n\n\n\n\nAveRooms\n\n\n0.326895\n\n\n-0.153277\n\n\n1.000000\n\n\n0.847621\n\n\n-0.072213\n\n\n-0.004852\n\n\n0.106389\n\n\n-0.027540\n\n\n\n\nAveBedrms\n\n\n-0.062040\n\n\n-0.077747\n\n\n0.847621\n\n\n1.000000\n\n\n-0.066197\n\n\n-0.006181\n\n\n0.069721\n\n\n0.013344\n\n\n\n\nPopulation\n\n\n0.004834\n\n\n-0.296244\n\n\n-0.072213\n\n\n-0.066197\n\n\n1.000000\n\n\n0.069863\n\n\n-0.108785\n\n\n0.099773\n\n\n\n\nAveOccup\n\n\n0.018766\n\n\n0.013191\n\n\n-0.004852\n\n\n-0.006181\n\n\n0.069863\n\n\n1.000000\n\n\n0.002366\n\n\n0.002476\n\n\n\n\nLatitude\n\n\n-0.079809\n\n\n0.011173\n\n\n0.106389\n\n\n0.069721\n\n\n-0.108785\n\n\n0.002366\n\n\n1.000000\n\n\n-0.924664\n\n\n\n\nLongitude\n\n\n-0.015176\n\n\n-0.108197\n\n\n-0.027540\n\n\n0.013344\n\n\n0.099773\n\n\n0.002476\n\n\n-0.924664\n\n\n1.000000\n\n\n\n\n\n\nMost negative correlation\nFind the most negative correlation for each column:\ncorr.min()\nMedInc       -0.119034\nHouseAge     -0.296244\nAveRooms     -0.153277\nAveBedrms    -0.077747\nPopulation   -0.296244\nAveOccup     -0.006181\nLatitude     -0.924664\nLongitude    -0.924664\ndtype: float64\nFind the column which has the lowest correlation:\ncorr.min().idxmin()\n'Latitude'\nExtract the Latitude column and get the index of the most negative value in it:\ncorr[corr.min().idxmin()].idxmin()\n'Longitude'\nThe most negative correlation is therefore between:\ncorr.min().idxmin(), corr[corr.min().idxmin()].idxmin()\n('Latitude', 'Longitude')\nwith the value:\ncorr.min().min()\n-0.9246644339150366\n\n\nMost positive correlation\nFirst we need to remove the 1.0 values on the diagonal:\nimport numpy as np\n\nnp.fill_diagonal(corr.values, np.nan)\ncorr\n\n\n\n\n\n\n\n\nMedInc\n\n\nHouseAge\n\n\nAveRooms\n\n\nAveBedrms\n\n\nPopulation\n\n\nAveOccup\n\n\nLatitude\n\n\nLongitude\n\n\n\n\n\n\nMedInc\n\n\nNaN\n\n\n-0.119034\n\n\n0.326895\n\n\n-0.062040\n\n\n0.004834\n\n\n0.018766\n\n\n-0.079809\n\n\n-0.015176\n\n\n\n\nHouseAge\n\n\n-0.119034\n\n\nNaN\n\n\n-0.153277\n\n\n-0.077747\n\n\n-0.296244\n\n\n0.013191\n\n\n0.011173\n\n\n-0.108197\n\n\n\n\nAveRooms\n\n\n0.326895\n\n\n-0.153277\n\n\nNaN\n\n\n0.847621\n\n\n-0.072213\n\n\n-0.004852\n\n\n0.106389\n\n\n-0.027540\n\n\n\n\nAveBedrms\n\n\n-0.062040\n\n\n-0.077747\n\n\n0.847621\n\n\nNaN\n\n\n-0.066197\n\n\n-0.006181\n\n\n0.069721\n\n\n0.013344\n\n\n\n\nPopulation\n\n\n0.004834\n\n\n-0.296244\n\n\n-0.072213\n\n\n-0.066197\n\n\nNaN\n\n\n0.069863\n\n\n-0.108785\n\n\n0.099773\n\n\n\n\nAveOccup\n\n\n0.018766\n\n\n0.013191\n\n\n-0.004852\n\n\n-0.006181\n\n\n0.069863\n\n\nNaN\n\n\n0.002366\n\n\n0.002476\n\n\n\n\nLatitude\n\n\n-0.079809\n\n\n0.011173\n\n\n0.106389\n\n\n0.069721\n\n\n-0.108785\n\n\n0.002366\n\n\nNaN\n\n\n-0.924664\n\n\n\n\nLongitude\n\n\n-0.015176\n\n\n-0.108197\n\n\n-0.027540\n\n\n0.013344\n\n\n0.099773\n\n\n0.002476\n\n\n-0.924664\n\n\nNaN\n\n\n\n\n\ncorr.max().idxmax(), corr[corr.max().idxmax()].idxmax()\n('AveRooms', 'AveBedrms')\ncorr.max().max()\n0.8476213257130424\n\n\n\n\n\n\nPlotting the correlation\nViewing the correlation coefficients as a table is useful if you want the precise value of the correlation but often you want a visual overview which can give you the information you want at a glance.\nThe easiest way to view it is as a heat map where each cell has a colour showing the value of the correlation using Seaborn which is a visualisation library that provides a higher-level interface to Matplotlib.\n\nimport seaborn as sns\n\nsns.heatmap(corr, vmin=-1.0, vmax=1.0, square=True, cmap=\"RdBu\")\n\n\n\n\n\n\n\n\nThis gives us a sense of which parameters are strongly correlated with each other. Very blue squares are positively correlated, for example the average number of rooms and the average number of bedrooms. That correlation makes sense as they definitely have mutual information.\nOthers perhaps make less sense at a glance. We see that the latitude is very strongly negatively correlated with the longitude. Why on earth should there be any relationship between those two? Let’s take a look at another view on the data to see if we can discover why.\n\n\nMulti-variable scatter matrix\nPandas also provides a quick method of looking at a large number of data parameters at once and looking visually at which might be worth investigating. If you pass any pandas DataFrame to the scatter_matrix() function then it will plot all the pairs of parameters in the data.\nThe produced graph has a lot of information in it so it’s worth taking some time to make sure you understand these plots. The plot is arranged with all the variables of interest from top to bottom and then repeated from left to right so that any one square in the grid is defined by the intersection of two variables.\nEach box that is an intersection of a variable with another (e.g. row three, column one is the intersection between “AveRooms” and “MedInc”) shows the scatter plot of how the values of those variables relate to each other. If you see a strong diagonal line it means that those variables are correlated in this data set. It it’s more of a blob or a flat horizontal or vertical line then that suggests a low correlation.\nThe top-right triangle of the plot is a repeat of the bottom-left triangle, just with the items in the pair reversed (i.e. row one, column three is the intersection between “MedInc” and “AveRooms”).\nThe square boxes along the diagonal from the top-left to the bottom-right are those intersections of a variable with itself and so are used, not to show correlation, but to show the distribution of values of each single variable as a histogram.\n\nfrom pandas.plotting import scatter_matrix\n\na = scatter_matrix(housing, figsize=(16, 16))\n\n\n\n\n\n\n\n\nIn general, when calculating a regression, you want your features to be as uncorrelated with each other as possible. This is because if two features, \\(x_1\\) and \\(x_2\\) are strongly correlated with each other then it’s possible to predict the value of \\(x_2\\) from the value of \\(x_1\\) with high confidence. This means that \\(x_2\\) is not providing any additional predictive power.\nIn some cases this is not a problem as adding one extra variable does not slow down or harm the algorithm used but some methods benefit from choosing carefully the parameters which are being fitted over.\nIt’s also possible in some cases to transform the data in some way to reduce the correlation between variables. One example of a method which does this is principle component analysis (PCA).\nOn the other hand, you do want correlation between \\(X\\) and \\(y\\) as if there is no mutual information then there is no predictive power.\n\n\n\n\n\n\nExercise\n\n\n\nTry running through the above step using a different dataset from sklearn. You can find them listed at https://scikit-learn.org/stable/datasets/toy_dataset.html. Iris is a classic dataset used in machine learning which it is worth being aware of.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nfrom pandas import DataFrame\nfrom sklearn.datasets import load_iris\n\niris, iris_target = load_iris(as_frame=True, return_X_y=True)\niris.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\n\n\nsepal width (cm)\n\n\npetal length (cm)\n\n\npetal width (cm)\n\n\n\n\n\n\n0\n\n\n5.1\n\n\n3.5\n\n\n1.4\n\n\n0.2\n\n\n\n\n1\n\n\n4.9\n\n\n3.0\n\n\n1.4\n\n\n0.2\n\n\n\n\n2\n\n\n4.7\n\n\n3.2\n\n\n1.3\n\n\n0.2\n\n\n\n\n3\n\n\n4.6\n\n\n3.1\n\n\n1.5\n\n\n0.2\n\n\n\n\n4\n\n\n5.0\n\n\n3.6\n\n\n1.4\n\n\n0.2\n\n\n\n\n\ncorr = iris.corr()\n%matplotlib inline\n\nimport seaborn as sns\n\nsns.heatmap(corr, vmin=-1.0, vmax=1.0, cmap=\"RdBu\")\n&lt;AxesSubplot: &gt;\n\nfrom pandas.plotting import scatter_matrix\n\na = scatter_matrix(iris, figsize=(16, 16), c=iris_target)"
  },
  {
    "objectID": "pages/answer_diabetes_regression.html",
    "href": "pages/answer_diabetes_regression.html",
    "title": "Introduction to Data Anlysis in Python (Part 2)",
    "section": "",
    "text": "from sklearn.datasets import load_diabetes\nfrom sklearn.linear_model import LinearRegression\n\nX, y = load_diabetes(as_frame=True, return_X_y=True)\n\nX.head()\n\n\n\n\n\n\n\n\nage\n\n\nsex\n\n\nbmi\n\n\nbp\n\n\ns1\n\n\ns2\n\n\ns3\n\n\ns4\n\n\ns5\n\n\ns6\n\n\n\n\n\n\n0\n\n\n0.038076\n\n\n0.050680\n\n\n0.061696\n\n\n0.021872\n\n\n-0.044223\n\n\n-0.034821\n\n\n-0.043401\n\n\n-0.002592\n\n\n0.019907\n\n\n-0.017646\n\n\n\n\n1\n\n\n-0.001882\n\n\n-0.044642\n\n\n-0.051474\n\n\n-0.026328\n\n\n-0.008449\n\n\n-0.019163\n\n\n0.074412\n\n\n-0.039493\n\n\n-0.068332\n\n\n-0.092204\n\n\n\n\n2\n\n\n0.085299\n\n\n0.050680\n\n\n0.044451\n\n\n-0.005670\n\n\n-0.045599\n\n\n-0.034194\n\n\n-0.032356\n\n\n-0.002592\n\n\n0.002861\n\n\n-0.025930\n\n\n\n\n3\n\n\n-0.089063\n\n\n-0.044642\n\n\n-0.011595\n\n\n-0.036656\n\n\n0.012191\n\n\n0.024991\n\n\n-0.036038\n\n\n0.034309\n\n\n0.022688\n\n\n-0.009362\n\n\n\n\n4\n\n\n0.005383\n\n\n-0.044642\n\n\n-0.036385\n\n\n0.021872\n\n\n0.003935\n\n\n0.015596\n\n\n0.008142\n\n\n-0.002592\n\n\n-0.031988\n\n\n-0.046641\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, random_state=42)\n\nmodel = LinearRegression(fit_intercept=True)\nmodel.fit(train_X[[\"bmi\"]], train_y)\n\n\n\nLinearRegression()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n  LinearRegression?Documentation for LinearRegressioniFitted\n\nLinearRegression()\n\n\n\n\n\nmodel.score(test_X[[\"bmi\"]], test_y)\n0.3172099449537781\nimport pandas as pd\n\npred = pd.DataFrame({\"bmi\": [X[\"bmi\"].min(), X[\"bmi\"].max()]})\npred[\"y\"] = model.predict(pred)\nimport seaborn as sns\n\nsns.relplot(data=X, x=\"bmi\", y=y)\nsns.lineplot(data=pred, x=\"bmi\", y=\"y\", c=\"red\", linestyle=\":\")\n&lt;Axes: xlabel='bmi', ylabel='target'&gt;"
  },
  {
    "objectID": "pages/generate_kmeans_animation.html",
    "href": "pages/generate_kmeans_animation.html",
    "title": "Introduction to Data Anlysis in Python (Part 2)",
    "section": "",
    "text": "from pandas import DataFrame\nfrom sklearn.datasets import make_blobs\n\nN_CLUSTERS = 3\n\ndata, true_labels = make_blobs(n_samples=500, centers=N_CLUSTERS, random_state=9)\n\npoints = DataFrame(data, columns=[\"x1\", \"x2\"])\npoints.plot.scatter(\"x1\", \"x2\")\n&lt;AxesSubplot:xlabel='x1', ylabel='x2'&gt;\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib import animation\nfrom sklearn.cluster import KMeans\n\n# First set up the figure, the axis, and the plot element we want to animate\nwith plt.xkcd():\n    fig, ax = plt.subplots(constrained_layout=True)\n#fig.set_constrained_layout_pads(w_pad=10/72, h_pad=10/72)\nax.tick_params(\n    axis='both',\n    which='both',\n    bottom=False,\n    left=False,\n    labelbottom=False,\n    labelleft=False,\n)\n#ax.set_xlabel(\"x1\")\n#ax.set_ylabel(\"x2\")\n\n# Here we create the initial path objects which will be altered each frame.\np = ax.scatter(points[\"x1\"], points[\"x2\"], edgecolor=\"none\", s=60, alpha=0.8)\nx = ax.scatter([], [], c=\"red\", s=200, marker=\"x\")\n\ndef animate(i):\n    kmeans = KMeans(n_clusters=N_CLUSTERS, n_init=1, max_iter=i+1, random_state=1, init=\"random\").fit(points)\n    p.set_array(kmeans.labels_)  # Update the colours\n    x.set_offsets(kmeans.cluster_centers_)  # Update the positions\n    return p,\n\nanim = animation.FuncAnimation(fig, animate, frames=30, interval=400, blit=True)\n\n#anim.save(\"kmeans.mp4\", extra_args=[\"-vcodec\", \"libx264\"])\nanim.save(\"kmeans.gif\", writer=\"imagemagick\")\n\n!gifsicle -b -O3 kmeans.gif\ngifsicle:kmeans.gif: warning: too many colors, using local colormaps\n  (You may want to try ‘--colors 256’.)"
  },
  {
    "objectID": "pages/generate_quadratic_plot.html",
    "href": "pages/generate_quadratic_plot.html",
    "title": "Introduction to Data Anlysis in Python (Part 2)",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\nplt.xkcd()\n\nN_POINTS = 50\nx = np.linspace(0, 10, N_POINTS)\nrng = np.random.RandomState(42)\ny = x*2\n\nx = x + (rng.normal(size=N_POINTS) * 0.25)\ny = y + (rng.normal(size=N_POINTS) * 0.25)\n\nfig, ax = plt.subplots(constrained_layout=True)\nfig.set_constrained_layout_pads(w_pad=10/72, h_pad=10/72)\nax.scatter(x, y, alpha=0.8)\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\n\nfig.savefig(\"linear.svg\")\n\ny = -((x-5)**2) + 25\n\nx = x + (rng.normal(size=N_POINTS) * 0.25)\ny = y + (rng.normal(size=N_POINTS) * 0.25)\n\nfig, ax = plt.subplots(constrained_layout=True)\nfig.set_constrained_layout_pads(w_pad=10/72, h_pad=10/72)\nax.scatter(x, y, alpha=0.8)\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\n\nfig.savefig(\"quadratic.svg\")"
  },
  {
    "objectID": "pages/102-nearest-neighbours.html",
    "href": "pages/102-nearest-neighbours.html",
    "title": "Fitting data: classification",
    "section": "",
    "text": "In this section…\n\n\n\n\nUse scikit-learn to train a classification model\nUnderstand how hyperparameters may influence the model output\nVisualize model results using seaborn\n\n\n\n\n\n\n\n\n\nTrainer notes\n\n\n\n\n\nRecommended time: 30 minutes\nThis sections introduces another machine learning model, but for classification in this case.\nCommon issues: - Understanding the method of K-nearest neighbors\n\n\n\nIn the first chapters we were using linear regression which is a supervised regression technique. We’re going to carry on with supervised techniques but look instead at how we can classify or categorise data using an algorithm called K-nearest neighbours.\nWhen you ask for a prediction from k-NN for a data point \\(x\\), the algorithm looks at all the training data that was passed in and finds the \\(k\\) nearest (e.g. the 5 nearest) data points. The label of \\(x\\) will then be based on whichever label was found most frequently within the \\(k\\) neighbours.\n\nWe’ll start by grabbing some data which represents a interesting case for some classification methods. Start by loading the data file:\nimport pandas as pd\n\ndata = pd.read_csv(\"https://bristol-training.github.io/data-analysis-python-2/data/moons.csv\")\nIf we look at the first few rows of this data, we can see it has two features (x1 and x2) and one target column (y). The target column is an integer, which suggests it will work well with a classification algorithm:\n\ndata.head()\n\n\n\n\n\n\n\n\nx1\nx2\ny\n\n\n\n\n0\n0.830858\n-0.334342\n1\n\n\n1\n0.991710\n0.879000\n0\n\n\n2\n1.107245\n-0.470344\n1\n\n\n3\n-0.140899\n1.033148\n0\n\n\n4\n0.405592\n1.328529\n0\n\n\n\n\n\n\n\nLet’s also have a look at the data visually to see what we’re working with:\n\nimport seaborn as sns\n\nsns.scatterplot(data=data, x=\"x1\", y=\"x2\", hue=\"y\", palette=\"Dark2\")\n\n\n\n\n\n\n\n\nWe can grab out the features and target parts now to use in scikit-learn shortly:\n\nX = data[[\"x1\", \"x2\"]]\ny = data[\"y\"]\n\nAs ever, we need to split our data into a training data set and a test data set:\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y)\n\nNow we have our data and it’s all in the correct format (\\(X\\) is a 2D table of data and \\(y\\) is a single column of labels), we can go ahead and use our model.\nAs usual it works by importing the model, KNeighborsClassifier, making an instance of it (setting the hyperparameters) and then fitting it by passing it \\(X\\) and \\(y\\) for the training data set.\nThe most important hyperparameter for k-NN is \\(k\\), or the number of neighbours. The model defaults to 5 but you can set it to any integer you wish.\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier(n_neighbors=5)\nmodel.fit(train_X, train_y)\n\nKNeighborsClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifier?Documentation for KNeighborsClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_neighbors \n5\n\n\n\nweights \n'uniform'\n\n\n\nalgorithm \n'auto'\n\n\n\nleaf_size \n30\n\n\n\np \n2\n\n\n\nmetric \n'minkowski'\n\n\n\nmetric_params \nNone\n\n\n\nn_jobs \nNone\n\n\n\n\n            \n        \n    \n\n\nAt this point, our model is ready to use. I’ll point out one important difference between k-NN and other algorithms and that is how it stores the information you have given it.\nThinking back to the example of the linear regression from the first chapter, in that case we gave the model some data (50 \\(x\\),\\(y\\) values) and based on those it calculated two parameters of interest, the gradient and the y-intercept. No matter how many training examples we give it, it will always generalise those data down to two parameters.\nK-nearest neighbours is different in that it is a non-generalising learning algorithm (also referred to as instance-based learning). It doesn’t simplify down the training data we pass in, it actually stores all of it internally. Thinking about it, this makes sense as when we ask it to make a prediction it needs to actually go and find the data points that are near the prediction site. This means that if we train a model on more data, the model becomes more heavyweight (i.e. may use more memory) and will likely become slower (as it needs to check more points to find the neighbours).\nIn general, this will not cause a problem but it’s something that you should be aware of.\nWe can use our model in the same way as in the past to, for example, check the performance against the test data set:\n\nmodel.score(test_X, test_y)\n\n0.984\n\n\nThat looks like a very good score indeed. Is that believable or do you think we’ve done something wrong?\nLet’s take a look at the distribution of predictions compared to the input data.\nWe’ll use a built-in function from sklearn called DecisionBoundaryDisplay.from_estimator to plot the predictions of the model, compared to the input data.:\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nDecisionBoundaryDisplay.from_estimator(model, X, cmap=\"PRGn\")\nsns.scatterplot(data=X, x=\"x1\", y=\"x2\", hue=y, palette=\"Dark2\")\n\n\n\n\n\n\n\n\nHere we can see that all of the green points are sitting in the one background area and the orange points are all in their area. In this case, it makes sense that it’s got a very good score since the data are not overlapping much.\n\n\n\n\n\n\nExercise\n\n\n\nRun the code above and make sure you see the same output.\nExperiment with different values of n_neighbors when creating the model (between 1 and 200).\nHow does varying this value affect the prediction map or the model score?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThis answer page shows the results of trying different values of noise and n_neighbors when fitting k-NN to a dummy data set. For you to complete the exercise I would just expect you to maually change the values and rerun the cells to look at the differences. On this page I have done something a little more complicated in order to visualise all the combinations in one plot.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.inspection import DecisionBoundaryDisplay\nWe’ll loop over a range of neighbour counts:\nneighbours = [1, 5, 10, 100, 150]\nStart by grabbing the data:\ndata = pd.read_csv(\"https://bristol-training.github.io/applied-data-analysis-in-python/data/moons.csv\")\nX = data[[\"x1\", \"x2\"]]\ny = data[\"y\"]\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, random_state=42)\nWe then loop over the range of values we want, fit the model and plot the result for each.\n# We'll plot the results in a grid of subplots\nfig, axs = plt.subplots(\n    nrows=len(neighbours),\n    ncols=1,\n    figsize=(8, 30),\n    constrained_layout=True,\n    sharex=True,\n    sharey=True\n)\n\nfor row, n_neighbors in enumerate(neighbours):\n    # Fit and score the model (uses the `n_neighbors` variable)\n    model = KNeighborsClassifier(n_neighbors=n_neighbors).fit(train_X, train_y)\n    score = model.score(test_X, test_y)\n\n    # Plot the results in the grid of subplots\n    ax = axs[row]\n    ax.set_xlim(-2, 3)\n    ax.set_ylim(-1.5, 2)\n    ax.set_title(f\"k: {n_neighbors}, score={score:.2}\")\n    DecisionBoundaryDisplay.from_estimator(model, X, cmap=\"PRGn\", ax=ax)\n    sns.scatterplot(data=X, x=\"x1\", y=\"x2\", hue=y, ax=ax, palette=\"Dark2\")\n    ax.get_legend().set_visible(False)",
    "crumbs": [
      "Model fitting",
      "Fitting data: classification"
    ]
  },
  {
    "objectID": "pages/200-validation.html",
    "href": "pages/200-validation.html",
    "title": "Testing your model",
    "section": "",
    "text": "In this section…\n\n\n\n\nImplement train-test splits to evaluate model performance\nUnderstand the importance of data splitting in preventing overfitting\n\n\n\n\n\n\n\n\n\nTrainer notes\n\n\n\n\n\nRecommended time: 30 minutes\nThis sections…\nCommon issues:\n\n\n\nSo far we have been fitting our model to the data and assuming that it is doing a good job. We have not had any method for analysing the quality of the fit. In our example so far we first looked at the fit with our eyes and judged it sufficient. We then compared the output parameters with our ground-truth and judged it to be “close enough”.\nTo truly judge how good the model is, we need to compare it with some data and see how well it aligns (i.e. how well it would be able to predict it).\nNaïvely we might think to compare our model against the same data we used to fit it. However, this is a dangerous thing to do as it encourages you to tweak your model to best fit the data that you have in hand rather than trying to make a model which can predict things about the process which generated your data. Making your model fit your local subset well, at the expense of the global superset is known as overfitting.\nFor example, imagine we have a true physical model:\n\nIf we want to understand the underlying model, we can make measurements of it:\n\nHowever, we cannot see the underlying model directly so all that we see is:\n\nWe can fit this model with perhaps varying degrees of polynomial. Mathematically, if we increase the degree of polynomial far enough we can fit any function. For example, fitting the data with a 15th-order polynomial creates a model which goes through most of the data points but clearly represent the underlying model badly:\n\nHere we can see that the model is not doing a good job of representing the underlying function (because we saw it above) but in the real world you do not usually have the underlying model available. In those cases overfitting is harder to see as it just manifests as a “well-performing” model. Seen in isolation, this model looks like it is performing quite well, whereas a 3rd-order polynomial looks slightly worse (as fewer of the bumps are accounted for):\n\nThe simplest solution to overfitting is to fit your model with one subset of data and then assess its quality with another subset. If those two subsets are independent then any specific features in the former which your model might try to overfit to will not be present in the latter and so it will be judged poorly.\n\nBringing up our data from the last chapter again:\nimport pandas as pd\n\ndata = pd.read_csv(\"https://bristol-training.github.io/data-analysis-python-2/data/linear.csv\")\n\nX = data[[\"x\"]]\ny = data[\"y\"]\nscikit-learn provides a built-in function, train_test_split, to split your data into a subset of data to fit with and a subset of data to test against:\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, random_state=42)\n\ntrain_test_split will split your data randomly and so to get a reproducible “random” split we set the random_state argument.\nTo see that train and test are taken from the same distribution let’s plot them:\n\nimport seaborn as sns\n\n# Label the original DataFrame with the test/train split\n# This is just used for plotting purposes\ndata.loc[train_X.index, \"train/test\"] = \"train\"\ndata.loc[test_X.index, \"train/test\"] = \"test\"\n\nsns.relplot(data=data, x=\"x\", y=\"y\", hue=\"train/test\")\n\n\n\n\n\n\n\n\nNow that we have train and test we should only ever pass train to the fit function:\n\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression(fit_intercept=True)\nmodel.fit(train_X, train_y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\nfit_intercept \nTrue\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n\n            \n        \n    \n\n\nTo find out how good the fit was, we can call the score method on the model. It is important here that we pass in our test data set as we expect that to provide an independent validation of the model.\n\nmodel.score(test_X, test_y)\n\n0.9676069631786152\n\n\nA score of \\(1.0\\) is a perfect match and anything less than that is less-well performing. A score of \\(0.97\\) suggests we have a very good model.\nGoing back to our example from the start, we can see that when we compare our 3rd- and 15th-order polynomials against the test data, the 3rd-order score will be much better:\n\n\n\n\n\n\n\nExercise\n\n\n\n\nLoad the diabetes dataset from the scikit-learn collection. You can load the dataset into pandas with:\nfrom sklearn.datasets import load_diabetes\n\nX, y = load_diabetes(as_frame=True, return_X_y=True)\nSplit the data into train and test subsets\nFit and plot the linear relationship between the “bmi” column (Body mass index) and the “target” column (quantitative measure of disease progression one year after baseline).\nCalculate the score of the model against the test data set.\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.linear_model import LinearRegression\n\nX, y = load_diabetes(as_frame=True, return_X_y=True)\n\nX.head()\n\n\n\n\n\n\n\n\nage\n\n\nsex\n\n\nbmi\n\n\nbp\n\n\ns1\n\n\ns2\n\n\ns3\n\n\ns4\n\n\ns5\n\n\ns6\n\n\n\n\n\n\n0\n\n\n0.038076\n\n\n0.050680\n\n\n0.061696\n\n\n0.021872\n\n\n-0.044223\n\n\n-0.034821\n\n\n-0.043401\n\n\n-0.002592\n\n\n0.019907\n\n\n-0.017646\n\n\n\n\n1\n\n\n-0.001882\n\n\n-0.044642\n\n\n-0.051474\n\n\n-0.026328\n\n\n-0.008449\n\n\n-0.019163\n\n\n0.074412\n\n\n-0.039493\n\n\n-0.068332\n\n\n-0.092204\n\n\n\n\n2\n\n\n0.085299\n\n\n0.050680\n\n\n0.044451\n\n\n-0.005670\n\n\n-0.045599\n\n\n-0.034194\n\n\n-0.032356\n\n\n-0.002592\n\n\n0.002861\n\n\n-0.025930\n\n\n\n\n3\n\n\n-0.089063\n\n\n-0.044642\n\n\n-0.011595\n\n\n-0.036656\n\n\n0.012191\n\n\n0.024991\n\n\n-0.036038\n\n\n0.034309\n\n\n0.022688\n\n\n-0.009362\n\n\n\n\n4\n\n\n0.005383\n\n\n-0.044642\n\n\n-0.036385\n\n\n0.021872\n\n\n0.003935\n\n\n0.015596\n\n\n0.008142\n\n\n-0.002592\n\n\n-0.031988\n\n\n-0.046641\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, random_state=42)\n\nmodel = LinearRegression(fit_intercept=True)\nmodel.fit(train_X[[\"bmi\"]], train_y)\n\n\n\nLinearRegression()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n  LinearRegression?Documentation for LinearRegressioniFitted\n\nLinearRegression()\n\n\n\n\n\nmodel.score(test_X[[\"bmi\"]], test_y)\n0.3172099449537781\nimport pandas as pd\n\npred = pd.DataFrame({\"bmi\": [X[\"bmi\"].min(), X[\"bmi\"].max()]})\npred[\"y\"] = model.predict(pred)\nimport seaborn as sns\n\nsns.relplot(data=X, x=\"bmi\", y=y)\nsns.lineplot(data=pred, x=\"bmi\", y=\"y\", c=\"red\", linestyle=\":\")\n&lt;Axes: xlabel='bmi', ylabel='target'&gt;",
    "crumbs": [
      "Model fitting",
      "Testing your model"
    ]
  },
  {
    "objectID": "pages/generate_blobs.html",
    "href": "pages/generate_blobs.html",
    "title": "Introduction to Data Anlysis in Python (Part 2)",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import make_blobs\n\nX, y = make_blobs(n_samples=500, centers=4, cluster_std=2.5, random_state=42)\nX = pd.DataFrame(X, columns=[\"x1\", \"x2\"])\n\nsns.scatterplot(data=X, x=\"x1\", y=\"x2\", hue=y, palette=\"Dark2\")\n&lt;Axes: xlabel='x1', ylabel='x2'&gt;\n\ndata = X.copy()\ndata[\"y\"] = y\ndata.to_csv(\"blobs.csv\", index=False)\ncheck = pd.read_csv(\"blobs.csv\")\npd.testing.assert_frame_equal(data, check)"
  },
  {
    "objectID": "pages/answer_colour_space_inertia.html",
    "href": "pages/answer_colour_space_inertia.html",
    "title": "Introduction to Data Anlysis in Python (Part 2)",
    "section": "",
    "text": "First we get the RGB data set:\nimport numpy as np\nfrom pandas import Series, DataFrame\nimport pandas as pd\nfrom skimage import io\nfrom sklearn.cluster import KMeans\n\nphoto = io.imread(\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/97/Swallow-tailed_bee-eater_%28Merops_hirundineus_chrysolaimus%29.jpg/768px-Swallow-tailed_bee-eater_%28Merops_hirundineus_chrysolaimus%29.jpg\")\n\nphoto = np.array(photo, dtype=np.float64) / 255  # Scale values\nw, h, d = original_shape = tuple(photo.shape)  # Get the current shape\nimage_array = np.reshape(photo, (w * h, d))  # Reshape to to 2D\n\npixels = DataFrame(image_array, columns=[\"Red\", \"Green\", \"Blue\"])\n\npixels_sample = pixels.sample(frac=0.05)\nThen we compute the L*a*b* dataset:\nfrom skimage.color import rgb2lab\n\nphoto_lab = rgb2lab(photo)  # This is where we convert colour space\nw, h, d = original_shape = tuple(photo_lab.shape)\nimage_array_lab = np.reshape(photo_lab, (w * h, d))\n\npixels_lab = DataFrame(image_array_lab, columns=[\"L\", \"a\", \"b\"])\n\npixels_sample_lab = pixels_lab.sample(frac=0.05)\nThen we normalise the two inertia values so we can compare them alongside each other:\nkmeans = KMeans(n_clusters=1, n_init=\"auto\").fit(pixels_sample[[\"Red\", \"Green\", \"Blue\"]])\nkmeans_lab = KMeans(n_clusters=1, n_init=\"auto\").fit(pixels_sample_lab[[\"L\", \"a\", \"b\"]])\n\nnorm = kmeans_lab.inertia_ / kmeans.inertia_\nThen we loop over the number of clusters and calculate the inertia of each:\n%matplotlib inline\n\ninertia_values = []\nr = pd.RangeIndex(2, 10)\nfor n_clusters in r:\n    kmeans = KMeans(n_clusters=n_clusters, n_init=\"auto\").fit(pixels_sample[[\"Red\", \"Green\", \"Blue\"]])\n    kmeans_lab = KMeans(n_clusters=n_clusters, n_init=\"auto\").fit(pixels_sample_lab[[\"L\", \"a\", \"b\"]])\n    inertia_values.append((kmeans.inertia_, kmeans_lab.inertia_ / norm))\n\ninertia = DataFrame(inertia_values, columns=[\"RGB\", \"L*a*b*\"], index=r)\ninertia.plot()\n&lt;Axes: &gt;"
  },
  {
    "objectID": "pages/answer_no_y_intercept.html",
    "href": "pages/answer_no_y_intercept.html",
    "title": "Introduction to Data Anlysis in Python (Part 2)",
    "section": "",
    "text": "import pandas as pd\n\ndata = pd.read_csv(\"https://bristol-training.github.io/applied-data-analysis-in-python/data/linear.csv\")\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression(fit_intercept=False)\nX = data[[\"x\"]]\ny = data[\"y\"]\nmodel.fit(X, y)\n\n\n\nLinearRegression(fit_intercept=False)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n  LinearRegression?Documentation for LinearRegressioniFitted\n\nLinearRegression(fit_intercept=False)\n\n\n\n\n\npred = pd.DataFrame({\"x\": [0, 10]})\npred[\"y\"] = model.predict(pred)\nimport seaborn as sns\n\nsns.relplot(data=data, x=\"x\", y=\"y\")\nsns.lineplot(data=pred, x=\"x\", y=\"y\", c=\"red\", linestyle=\":\")\n&lt;Axes: xlabel='x', ylabel='y'&gt;\n\nprint(\" Model gradient: \", model.coef_[0])\nprint(\"Model intercept:\", model.intercept_)\n Model gradient:  1.1985226874421444\nModel intercept: 0.0"
  },
  {
    "objectID": "pages/aside_generate_linear_data.html",
    "href": "pages/aside_generate_linear_data.html",
    "title": "Introduction to Data Anlysis in Python (Part 2)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\n\nrng = np.random.RandomState(42)\n\nnumber_of_points = 50\nx_scale = 10\ngradient = 2\ny_intercept = -5\n\nx = x_scale * rng.rand(number_of_points)\ny = gradient * x + y_intercept + rng.normal(size=number_of_points)\n\ndata = pd.DataFrame({\"x\": x, \"y\": y})\n\ndata.to_csv(\"linear.csv\", index=False)"
  },
  {
    "objectID": "pages/answer_negative_correlation.html",
    "href": "pages/answer_negative_correlation.html",
    "title": "Introduction to Data Anlysis in Python (Part 2)",
    "section": "",
    "text": "import numpy as np\nfrom pandas import Series, DataFrame\n\na = np.arange(100)\nb = np.arange(100) * -2\ndf = DataFrame({\"a\": a, \"b\": b})\n\ndf.corr()\n\n\n\n\n\n\n\n\na\n\n\nb\n\n\n\n\n\n\na\n\n\n1.0\n\n\n-1.0\n\n\n\n\nb\n\n\n-1.0\n\n\n1.0"
  },
  {
    "objectID": "pages/appendix_clustering_images.html",
    "href": "pages/appendix_clustering_images.html",
    "title": "Clustering images",
    "section": "",
    "text": "As well as abstract data parameters such as created by the make_blobs() function and physical measurements as seen in the iris exercise, clustering can also be used on images.\nThe classic use for this is to reduce the number of colours used in an image for either compression or artistic purposes.\nAs with most machine learning algorithms working on images, the first step is to understand how an image is represented on a computer and to convert that into a format that the algorithm can understand.\nIn it simplest form, an image is represented as a 3-dimensional array of numbers. Two of those dimensions represent the width and height of the image and the third is the colour dimension. The colour dimension usually only has three values in it, one for how green that pixel is, one for how red it is and one for how blue it is. Each of these values will usually be an integer between 0 and 255 (one byte per colour channel).\nThis means that a colour image which is 15 pixels wide and 10 pixels high will have \\(15 \\times 10 \\times 3 = 450\\) numbers used to describe it.\nLet’s start by loading a photo from the internet using scikit-image’s imread() function.\nLooking at the shape we see that it is 480 pixels square and it has 3 colour chanels.\nfrom skimage import io\n\nphoto = io.imread(\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/97/Swallow-tailed_bee-eater_%28Merops_hirundineus_chrysolaimus%29.jpg/480px-Swallow-tailed_bee-eater_%28Merops_hirundineus_chrysolaimus%29.jpg\")\n\nprint(\"Shape is\", photo.shape)\nprint(\"Size is\", photo.size)\nShape is (480, 480, 3)\nSize is 691200\n%matplotlib inline\nio.imshow(photo)\n&lt;matplotlib.image.AxesImage at 0x7f222bb59810&gt;\n\n(Swallow-tailed bee-eater by Charles J Sharp, CC BY-SA 4.0)\nIt’s often more useful for machine learning to scale the values of the image to be between \\(0\\) and \\(1\\) rather than \\(0\\) and \\(255\\).\nAlso, for the purpose of clustering the colours in an image, we don’t care what positions the pixels have, only their values. To this end, we flatten the (480, 480, 3) 3D array into a 2D array of shape (480 × 480, 3) = (691200, 3)\nimport numpy as np\n\nphoto = np.array(photo, dtype=np.float64) / 255  # Scale values\nw, h, d = original_shape = tuple(photo.shape)  # Get the current shape\nimage_array = np.reshape(photo, (w * h, d))  # Reshape to to 2D\nNow that we have our 2D image array, we put it into a pandas DataFrame for easier plotting and processing:\nfrom pandas import DataFrame\n\npixels = DataFrame(image_array, columns=[\"Red\", \"Green\", \"Blue\"])\n\nExercise\n\nUsing pixels, find the pixel with the highest blue value (tip: use idxmax())\nCan you work out its original (x, y) position in the photo? (tip: use // and %) answer\n\n\n\nExploring the pixel data\nBefore applying the clustering algorithm to the data, it’s useful to plot the data.\nSince RGB pixels are a 3D dataset, we will plot three, 2D plots of the pairs red/green, red/blue and green/blue. To make the plots visually useful we will also colour each point in the plot with the colour of the pixel it came from.\nWe create a new column which contains a label which matplotlib will be able to understand to make each point the correct colour:\nfrom matplotlib import colors\n\npixels[\"colour\"] = [colors.to_hex(p) for p in image_array]\nSince we have \\(480 \\times 480 = 691200\\) pixels, both plotting the data and running the algorithm may be quite slow. To speed things up, we will run the algorithm fit over a random subset of the data. Pandas provides a method for doing just this, sample(). We tell it what fraction of the data we want to look at, here we specify 5%.\npixels_sample = pixels.sample(frac=0.05)\nTo make out lives easier, we define a function plot_colours() which will plot the three pairs of columns against each other\nimport matplotlib.pyplot as plt\n\n\ndef plot_colours(df, c1, c2, c3):\n    \"\"\"\n    Given a DataFrame and three column names,\n    plot the pairs against each other\n    \"\"\"\n    fig, ax = plt.subplots(1, 3)\n    fig.set_size_inches(18, 6)\n    df.plot.scatter(c1, c2, c=df[\"colour\"], alpha=0.3, ax=ax[0])\n    df.plot.scatter(c1, c3, c=df[\"colour\"], alpha=0.3, ax=ax[1])\n    df.plot.scatter(c2, c3, c=df[\"colour\"], alpha=0.3, ax=ax[2])\n\n\nplot_colours(pixels_sample, \"Red\", \"Green\", \"Blue\")\n\nWe see here that there is a strong brown line through the middle caused by the background with the colourful bird feathers are around the edge. In general, k-means clustering struggles to cope well with elongated features like this so we may need a larger number of clusters in order to pick out all the colours we want.\nSometimes viewing the data in 3D can help since planar projections can lose some nuances of the data. Display 3D plots using the mplot3d package.\nfrom mpl_toolkits import mplot3d\nfig = plt.figure()\nax = plt.axes(projection='3d')\nax.set_xlabel(\"Red\")\nax.set_ylabel(\"Green\")\nax.set_zlabel(\"Blue\")\nax.scatter(pixels_sample[\"Red\"], pixels_sample[\"Green\"], pixels_sample[\"Blue\"], c=pixels_sample[\"colour\"])\n&lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f222337eb00&gt;\n\nYou can make the 3D plot (and any other in fact) interactive by putting %matplotlib notebook at the top of the cell. This makes the change globally so make sure that you then have another cell which does %matplotlib inline to reset it back to the default static style.\n%matplotlib inline\n\nExercise\n\nCalculate the correlation between then red, green and blue channels. Does it match what you see in the plots? Do you think you want strong or weak correlation between your data variables? answer\n\nNow we get on to the actual work of running the clustering. We do it in the same way as before by first specifying the number of clusters and then passing it the data.\nWe need to specify that the data is pixels_sample[[\"Red\", \"Green\", \"Blue\"]] in order to pick out just those three columns as you should remember that we added in a fourth column containing the encoded colour string.\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=10, n_init=\"auto\").fit(pixels_sample[[\"Red\", \"Green\", \"Blue\"]])\nFinally, we display the chosen cluster centres which we shall use as our representative colours for the image:\nplt.imshow([kmeans.cluster_centers_])\n&lt;matplotlib.image.AxesImage at 0x7f222055e260&gt;\n\n\n\n\nAssigning points to clusters\nOnce we have calculated the midpoints of each of the clusters, we can then go back through the points in the original data set and assign each point to the cluster centre it is nearest to.\nIn the dummy examples from the previous section we could just use the labels_ data attribute on the model to do this but since we’ve only fit the data over a subset of the full data set, the labels_ attribute will similarly only contain those data points.\nThe KMeans model provides a predict() method which, given the calculated cluster centres can assign a cluster to each data point passed in. This allows you to use k-means clutering as a predictive classification tool.\nlabels = kmeans.predict(pixels[[\"Red\", \"Green\", \"Blue\"]])\nlabels\narray([5, 2, 2, ..., 5, 5, 5], dtype=int32)\n\nExercise (optional)\n\nPlot a bar graph of the counts of the number of pixels in each cluster. Try to colour each bar by the colour of the cluster centre. answer\n\nGiven our list of labels we then loop though it, replacing each cluster index with the values from the corresponding cluster centre. We then reshape the array to make it 3D again (width × height × colour channels).\nreduced = np.array([kmeans.cluster_centers_[p] for p in labels]).reshape(original_shape)\nWe can then plot the reduced image against the original to see the differences.\nf, axarr = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(18, 9))\naxarr[0].imshow(photo)\naxarr[0].set_title(\"Original\")\naxarr[1].imshow(reduced)\naxarr[1].set_title(\"RGB clustered\")\nText(0.5, 1.0, 'RGB clustered')\n\nYou’ll see here that it’s managed to pick out some of the larger blocks of colour but the majority of the image is still brown.\nThere are three main ways we can immediately try to improve this:\n\nIncrease the number of clusters the algorithm is fitting\nTransform the data to reduce the elongated nature of the clusters\nTry a different clustering algorithm (https://scikit-learn.org/stable/modules/clustering.html)\n\nWe’re going to go ahead with method 2 but feel free to play around with changing the number of clusters. Maybe even try plotting the inertia graph like we did in the last section.\n\n\n\nDifferent colour space\nSo far we’ve treated each pixel in the images as being defined by their RGB value, that is their total amount of red, green and blue in each pixel. This is not the only way to describe the colour of a pixel and over the decades, different schemes have emerged. RGB is popular and useful as it is the closes to how a computer monitor or phone screen works where physical red, green and blue lights create the picture.\nThe next most commonly used colour space is probably HSV where the three numbers represent the hue (like on a colour wheel), saturation (the scale from grey to bright) and value (the scale from black to colourful).\nFor the purposes of separating out the visual colour space, there is another colour space called L*a*b* (often just referred to as Lab) which expresses colour as three numerical values, L* for the lightness and a* and b* for the green–red and blue–yellow colour components respectively. Unlike RGB, it is designed to better represent human vision and gives a more uniform representation of brightness.\nDue to how it arranges colours, a plot of a* against b* will often give a useful spread of colours and is probably most likely to show clusters.\nscikit-image provides functions for converting images between different colour spaces, including rgb2lab() and lab2rgb().\nLet’s go through the same steps as before, the only difference being that we start by passing the image though the rgb2lab() function:\nfrom skimage.color import rgb2lab, lab2rgb\n\nphoto_lab = rgb2lab(photo)  # This is where we convert colour space\nw, h, d = original_shape = tuple(photo_lab.shape)\nimage_array_lab = np.reshape(photo_lab, (w * h, d))\n\npixels_lab = DataFrame(image_array_lab, columns=[\"L\", \"a\", \"b\"])\n\npixels_lab[\"colour\"] = [colors.to_hex(p) for p in image_array]\npixels_sample_lab = pixels_lab.sample(frac=0.05)\n\nplot_colours(pixels_sample_lab, \"L\", \"a\", \"b\")\n\nImmediately we see a difference from when we did this plot with RGB. We no longer have the strong diagonal line though the image (though there is still a prominent horizontal line). Looking in the third plot, (a* against b*) we can see distinctly the blue, green and yellow areas.\nPassing this converted data set to KMeans gives us our new set of cluster centres which we can view by converting them back from L*a*b* to RGB.\nkmeans_lab = KMeans(n_clusters=10, n_init=\"auto\").fit(pixels_sample_lab[[\"L\", \"a\", \"b\"]])\nplt.imshow(lab2rgb([kmeans_lab.cluster_centers_]))\n&lt;matplotlib.image.AxesImage at 0x7f220def2d10&gt;\n\nlabels_lab = kmeans_lab.predict(pixels_lab[[\"L\", \"a\", \"b\"]])  # Assign pixels to the cluster centre\ncenters_lab = lab2rgb([kmeans_lab.cluster_centers_])[0]  # Get the RGB of the cluster centres\nreduced_lab = np.array([centers_lab[p] for p in labels_lab]).reshape(original_shape)  # Map and reshape\nio.imshow(reduced_lab)\n&lt;matplotlib.image.AxesImage at 0x7f220dd5d2d0&gt;\n\n\n\nComparing the results\nNow that we have run the analysis with the raw data and the transformed data, we should compare the outcomes.\nf, axarr = plt.subplots(1, 3, sharex=True, sharey=True, figsize=(18, 6))\naxarr[0].imshow(photo)\naxarr[0].set_title(\"Original\")\naxarr[1].imshow(reduced)\naxarr[1].set_title(\"RGB clustered\")\naxarr[2].imshow(reduced_lab)\naxarr[2].set_title(\"Lab clustered\")\nText(0.5, 1.0, 'Lab clustered')\n\nPlotting the three next to each other, we see some differences. The L*a*b* data seems to have picked out more of the different colours of the bird at the expense of some of the brown shades.\nIt is possible that another colour space or a different transform may perform better but under the limit of 10 clusters, it seems that L*a*b* performs a little better.\n\nExercise\n\nExperiment with different numbers of clusters. Is there a point where the two colour space methods become indistinguishable?\nPlot on the same graph the inertia against number of clusters for the two colour spaces. Does one drop faster than the other? (tip: you may need to normalise them against each other) answer\nTry the process with another image. Try to keep the image small (~500 pixels in each dimension) or reduce the frac when sample()ing.\nTry adding in the HSV or HSL colour spaces or any others that scikit-image supports.\n\nThat’s all of the exercises for today. Move on to the next section for some pointers on what you may want to look into next and some book recomendations."
  },
  {
    "objectID": "pages/100-fitting-data.html",
    "href": "pages/100-fitting-data.html",
    "title": "Fitting data: regression",
    "section": "",
    "text": "In this section…\n\n\n\n\nLearn how to use scikit-learn to define a model and train it\nUnderstand how to interpret the model parameters and coefficients\n\n\n\n\n\n\n\n\n\nTrainer notes\n\n\n\n\n\nRecommended time: 30 minutes\nThis section introduces model selection and training with linear regression.\nCommon issues: - Not being able to read the file because it is not in the same folder, or right folder. - Libraries (pandas, scikit-learn) not imported.\n\n\n\nThe process of extracting information from data using computers is called machine learning.\nMachine learning a very large field and covers a whole host of techniques. In this course we will be discovering a few of them but let’s first start with the simplest form of machine learning, the linear fit or linear regression.\nPlease, remember to load your data as we did in previous sections:\n\nSetting up our model\nFor this and for the other machine learning techniques in this course, we will be using scikit-learn. It provides a whole host of tools for studying data. You may also want to investigate statsmodels which also provides a large number of tools for statistical exploration.\nscikit-learn provides a number of models which you can use to study your data. Each model is a Python class which can be imported and used. The usual process for using a model is:\n\nImport the model you want to use\nCreate an instance of that model and set any hyperparameters you want\nFit the model to the data, this computes the parameters of the model using machine learning\nPredict new information using the model\n\nAs we saw by plotting the data, the relationship between \\(x\\) and \\(y\\) is linear. In scitkit-learn, linear regression is available as scikit-learn.linear_model.LinearRegression.\nWe import the model and create an instance of it. By default the LinearRegression model will fit the y-intercept, but since we don’t want to make that assumption we explicitly pass fit_intercept=True. fit_intercept is an example of a hyperparameter, which are variables or options in a model which you set up-front rather than letting them be learned from the data.\n\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression(fit_intercept=True)\nmodel\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniNot fitted\n        \n            \n                Parameters\n                \n\n\n\n\nfit_intercept \nTrue\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n\n            \n        \n    \n\n\nWhen running a Jupyter Notebook, you will see this model summary box appear every time a cell evaluates to the model itself. Here, writing model causes this to happen explicitly but it will happen any time the last line in a cell has the return value of the model too.\n\n\nFitting the data\nOnce we have created our model, we can fit it to the data by calling the fit() method on it. This takes two arguments:\n\nThe input data as a two-dimensional structure of the size \\((N_{samples}, N_{features})\\).\nThe labels or targets of the data as a one-dimensional data structure of size \\((N_{samples})\\).\n\nIn our case we only have one feature, \\(x\\), and 50 data points so it should be in the shape \\((50, 1)\\). If we just request data[\"x\"] then that will be a 1D array (actually a pandas Series) of shape \\((50)\\) so we must request the data with data[[\"x\"]] (which returns it as a single-column, but still two-dimensional, DataFrame). For a more thorough explanation of this dimensionality difference, see this explanation.\nIf you’re using pandas to store your data (as we are) then just remember that the first argument should be a DataFrame and the second should be a Series.\n\nX = data[[\"x\"]]\ny = data[\"y\"]\n\n\nmodel.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\nfit_intercept \nTrue\n\n\n\ncopy_X \nTrue\n\n\n\ntol \n1e-06\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\n\n            \n        \n    \n\n\nIt is when you call this function that scikit-learn will go away and perform the machine learning algorithm. In our case it takes a fraction of a second but more complex models could take hours to compute.\nBy the time that this function returns, the model object will have had it’s internal parameters set as best the algorithm can do in order to predict \\(y\\) from \\(x\\).\nWe see the summary box appear here too as the fit method returns the model that it was called on, allowing you to chain together methods if you want.\n\n\nMaking predictions using the model\nOnce we’ve performed the fit, we can use it to predict the value of new data points which weren’t part of the original data set.\nWe can use this to plot the fit over the original data to compare the result. By getting the predicted \\(y\\) values for the minimum and maximum \\(x\\) values, we can plot a straight line between them to visualise the model.\nThe predict() function takes an array of the same shape as the original input data (\\((N_{samples}, N_{features})\\)) so we put our list of \\(x\\) values into a DataFrame before passing it to predict().\nWe then plot the original data in the same way as before and draw the prediction line in the same plot.\n\npred = pd.DataFrame({\"x\": [0, 10]})  # Make a new DataFrame containing the X values\npred[\"y\"] = model.predict(pred)  # Make a prediction and add that data into the table\npred\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n0\n-4.903311\n\n\n1\n10\n14.873255\n\n\n\n\n\n\n\n\nimport seaborn as sns\nsns.relplot(data=data, x=\"x\", y=\"y\")\nsns.lineplot(data=pred, x=\"x\", y=\"y\", c=\"red\", linestyle=\":\")\n\n\n\n\n\n\n\n\nAs well as plotting the line in a graph, we can also extract the calculated values of the gradient and y-intercept. The gradient is available as a list of values, model.coef_, one for each dimension or feature. The intercept is available as model.intercept_:\n\nprint(\" Model gradient: \", model.coef_[0])\nprint(\"Model intercept:\", model.intercept_)\n\n Model gradient:  1.9776566003853107\nModel intercept: -4.903310725531115\n\n\nThe equation that we have extracted can therefore be represented as:\n\\[y = 1.97 x - 4.90\\]\nThe original data was produced (with random wobble applied) from a straight line with gradient \\(2\\) and y-intercept of \\(-5\\). Our model has managed to predict values very close to the original.\n\n\n\n\n\n\nExercise\n\n\n\n\nRun the above data reading and model fitting. Ensure that you get the same answer we got above.\nTry fitting without allowing the y-intercept to vary. How does it affect the prediction of the gradient?\n\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nimport pandas as pd\n\ndata = pd.read_csv(\"https://bristol-training.github.io/applied-data-analysis-in-python/data/linear.csv\")\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression(fit_intercept=False)\nX = data[[\"x\"]]\ny = data[\"y\"]\nmodel.fit(X, y)\n\n\n\nLinearRegression(fit_intercept=False)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n\n  LinearRegression?Documentation for LinearRegressioniFitted\n\nLinearRegression(fit_intercept=False)\n\n\n\n\n\npred = pd.DataFrame({\"x\": [0, 10]})\npred[\"y\"] = model.predict(pred)\nimport seaborn as sns\n\nsns.relplot(data=data, x=\"x\", y=\"y\")\nsns.lineplot(data=pred, x=\"x\", y=\"y\", c=\"red\", linestyle=\":\")\n&lt;Axes: xlabel='x', ylabel='y'&gt;\n\nprint(\" Model gradient: \", model.coef_[0])\nprint(\"Model intercept:\", model.intercept_)\n Model gradient:  1.1985226874421444\nModel intercept: 0.0",
    "crumbs": [
      "Model fitting",
      "Fitting data: regression"
    ]
  },
  {
    "objectID": "pages/001-workspace-setup.html",
    "href": "pages/001-workspace-setup.html",
    "title": "Workspace Setup",
    "section": "",
    "text": "Please use our project setup guide to set up your computer for the course. If you have followed this guide, you will also be able to install additional packages needed for this course:\nuv add scikit-learn seaborn pandas openpyxl numpy openpyxl",
    "crumbs": [
      "Working framework",
      "Workspace Setup"
    ]
  },
  {
    "objectID": "extra/pca-morphological-traits.html",
    "href": "extra/pca-morphological-traits.html",
    "title": "PCA of morphological traits",
    "section": "",
    "text": "Introduction\nPCA is a dimensionality reduction and machine learning method used to simplify a large data set into a smaller set while still maintaining significant patterns and trends. PCA converts a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. These represent the underlying structure in the data - the dimensions in which there is most variance (most difference) in the data.\nIt is widely used in paleobiology and evolutionary biology while scientists study the disparity of the clade of interest, for example, exploring the disparity of Cambrian and modern arthropods. In this kind of study, scientists are seeking to distill the fundamental variance in the anatomy of Cambrian and living species given a set of observations of the presence and absence of anatomical structures. The PCA will allow us to identify the dimensions in which these species exhibit the most variance. In effect, it allows us to create a multidimensional universe of ‘design space’, ‘morphological disparity’, or ‘morphospace’ in which similar organisms are grouped together and distanced from dissimilar organisms.\nHere, we will try first do a case study on flower - the modern Iris Dataset. And if you are interested in PCA in paleo dataset, after this case study, you can explore the web ClustVis and try for a real dataset from Wills (1994) which characterizes the anatomy of Cambrian Burgess Shale and Modern arthropods (comprised of a suite of characters, like compound eyes, jointed appendages, antennae, etc, most of which are scored as either present or absent, or as different states describing the different manifestations of such structures).\n\n\nPCA on Iris Dataset\nThis example shows PCA on the Iris dataset.\nThis dataset is made of 4 features: sepal length, sepal width, petal length, petal width. We use PCA to project this 4 feature space into a three dimensional space.\n\nLoading the Iris dataset\nThe Iris dataset is directly available as part of scikit-learn. It can be loaded using the sklearn.datasets.load_iris function (i.e., the load_iris function within the sklearn.datasets module).\nWith the default parameters, a class object is returned, containing the data, the target values, the feature names, and the target names.\n\nfrom sklearn.datasets import load_iris\n\niris = load_iris(as_frame=True)\nprint(iris.keys())\n\ndict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n\n\n\n\nPlot of pairs of features of the Iris dataset\nLet’s first explore the relationships between the features of the Iris dataset by plotting pairwise scatter plots. This helps us visually assess the distribution of the data and the potential for dimensionality reduction before applying PCA.\n\nimport seaborn as sns\n\n# Rename classes using the iris target names\niris.frame[\"target\"] = iris.target_names[iris.target]\n_ = sns.pairplot(iris.frame, hue=\"target\")\n\n\n\n\n\n\n\n\nEach data point on each scatter plot refers to one of the 150 iris flowers in the dataset, with the color indicating their respective type (Setosa, Versicolor, and Virginica).\nYou can already see a pattern regarding the Setosa type, which is easily identifiable based on its short and wide sepal. Only considering these two dimensions, sepal width and length, there’s still overlap between the Versicolor and Virginica types.\nThe diagonal of the plot shows the distribution of each feature. We observe that the petal width and the petal length are the most discriminant features for the three types.\n\n\nPlot a PCA representation\nNow that we’ve seen the original feature relationships, let’s reduce the dimensionality of the dataset using PCA.\nWe’ll apply PCA to the Iris dataset and project the data onto its first three principal components.\nThis 3D scatter plot allows us to visualize how well the three Iris species can be separated in the reduced space.\n\nimport matplotlib.pyplot as plt\n\n# unused but required import for doing 3d projections with matplotlib &lt; 3.2\nimport mpl_toolkits.mplot3d  # noqa: F401\n\nfrom sklearn.decomposition import PCA\n\nfig = plt.figure(1, figsize=(8, 6))\nax = fig.add_subplot(111, projection=\"3d\", elev=-150, azim=110)\n\nX_reduced = PCA(n_components=3).fit_transform(iris.data)\nscatter = ax.scatter(\n    X_reduced[:, 0],\n    X_reduced[:, 1],\n    X_reduced[:, 2],\n    c=iris.target,\n    s=40,\n)\n\nax.set(\n    title=\"First three PCA dimensions\",\n    xlabel=\"1st Eigenvector\",\n    ylabel=\"2nd Eigenvector\",\n    zlabel=\"3rd Eigenvector\",\n)\nax.xaxis.set_ticklabels([])\nax.yaxis.set_ticklabels([])\nax.zaxis.set_ticklabels([])\n\n# Add a legend\nlegend1 = ax.legend(\n    scatter.legend_elements()[0],\n    iris.target_names.tolist(),\n    loc=\"upper right\",\n    title=\"Classes\",\n)\nax.add_artist(legend1)\n\nplt.show()\n\n\n\n\n\n\n\n\nPCA will create 3 new features that are a linear combination of the 4 original features. In addition, this transformation maximizes the variance. With this transformation, we see that we can identify each species using only the first feature (i.e., first eigenvector).",
    "crumbs": [
      "Discussion",
      "PCA of morphological traits"
    ]
  },
  {
    "objectID": "extra/medical-image-clustering.html",
    "href": "extra/medical-image-clustering.html",
    "title": "Medical image clustering",
    "section": "",
    "text": "To teach the concept of image clustering for analysis of medical images using a publicly available data set. The course will introduce ideas on data optimisation techniques with a focus on data augmentation, to improve machine learning in small imaging datasets.",
    "crumbs": [
      "Discussion",
      "Medical image clustering"
    ]
  },
  {
    "objectID": "extra/medical-image-clustering.html#the-basics-of-image-clustering",
    "href": "extra/medical-image-clustering.html#the-basics-of-image-clustering",
    "title": "Medical image clustering",
    "section": "The basics of image clustering",
    "text": "The basics of image clustering\nMachine learning can be used on images as well as numerical data. One method is clustering. Clustering is a form of unsupervised learning in which you separate a large set of data into smaller groups based on distances between them. In terms of images, clustering can be used to reduce the number of colours used in an image.\nIn this exercise we will demonstrate how clustering can be used to measure the size of different dermatological features.\nMake sure to download our data from here:\n\nPicture of mole 1\nPicture of mole 2\nPicture of mole 3\n\nLet’s start by loading our first image from our directory and examining it’s features:\n\nfrom PIL import Image\nimport numpy as np\nfrom skimage import io\nimport matplotlib.pyplot as plt\n\n# Make sure to specify your own working directory\nimg_handle  = Image.open(\"extra/data/mole.jpg\") \n# We need to first convert the image to a numpy array to ensure we can specify its dimensions \nimg = np.array(img_handle)\nprint('Shape is', img.shape)\nprint(\"Size is\", img.size)\n\nShape is (450, 600, 3)\nSize is 810000\n\n\n\nio.imshow(img)\nplt.show()\n\n/tmp/ipykernel_2809/1806673016.py:1: FutureWarning: `imshow` is deprecated since version 0.25 and will be removed in version 0.27. Please use `matplotlib`, `napari`, etc. to visualize images.\n  io.imshow(img)\n\n\n\n\n\n\n\n\n\nAt the moment, the image is in 3 dimensions: 450 (height) x 600 (width) x 3 (colour). Color is 3 corresponding to RGB (red, green, blue). We want to flatten the image to two dimensional array (pixel x colour).\nWe then want to put this into a pandas data frame for better plotting and processing. The data frame will have four columns: Red, Green, Blue and Colour - indicating the hex code of the pixel.\n\nimg = np.array(img, dtype=np.float64) / 255  # Scale values\nw, h, d = original_shape = tuple(img.shape)  # Get the current shape\nimg_array = np.reshape(img, (w * h, d))  # Reshape to to 2D\n\n#put into pandas data frame \nfrom pandas import DataFrame\n\npixels = DataFrame(img_array, columns=[\"Red\", \"Green\", \"Blue\"])\nfrom matplotlib import colors\n\npixels[\"colour\"] = [colors.to_hex(p) for p in img_array]\nprint(pixels)\n\n             Red     Green      Blue   colour\n0       0.949020  0.662745  0.698039  #f2a9b2\n1       0.964706  0.678431  0.713725  #f6adb6\n2       0.964706  0.678431  0.713725  #f6adb6\n3       0.964706  0.686275  0.717647  #f6afb7\n4       0.968627  0.690196  0.705882  #f7b0b4\n...          ...       ...       ...      ...\n269995  0.847059  0.572549  0.611765  #d8929c\n269996  0.850980  0.576471  0.607843  #d9939b\n269997  0.847059  0.572549  0.603922  #d8929a\n269998  0.847059  0.564706  0.607843  #d8909b\n269999  0.850980  0.568627  0.611765  #d9919c\n\n[270000 rows x 4 columns]\n\n\n\nExploring the data\nIt is useful to plot the data before running the clustering.\nSince we have a high amount of pixels (810000) running the algorithm may be slow. Therefore we can run it over a small subset of data.\n\npixels_sample = pixels.sample(frac=0.05) #look at 5% of the data\n\nSince RGB pixels are a 3D dataset, we will plot three 2D plots of the pairs red/green, red/blue and green/blue.\nTo make our lives easier, we can define a function plot_colours() which will plot the three pairs of columns against each other. To make the plots visually useful we will also colour each point in the plot with the colour of the pixel it came from using our fourth column, ‘Colour’.\n\ndef plot_colours(df, c1, c2, c3):\n    \"\"\"\n    Given a DataFrame and three column names,\n    plot the pairs against each other\n    \"\"\"\n    fig, ax = plt.subplots(1, 3)\n    fig.set_size_inches(18, 6)\n    df.plot.scatter(c1, c2, c=df[\"colour\"], alpha=0.3, ax=ax[0])\n    df.plot.scatter(c1, c3, c=df[\"colour\"], alpha=0.3, ax=ax[1])\n    df.plot.scatter(c2, c3, c=df[\"colour\"], alpha=0.3, ax=ax[2])\n\nplot_colours(pixels_sample, \"Red\", \"Green\", \"Blue\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThe 1st plot: Red vs green, represents a non-linear relationship between red and green, red increases and green increases at a different rate.\nThe 2nd plot: Red vs blue, a curved upward trend where red values have higher blue values. However, the spread is more eminent at lower levels of red intensity.\nThe 3rd plot: Green vs blue, a curved relationship with strong positive correlation with limited spread. Meaning as green increases blue increases.\n\nOverall this shows we have a strong correlation between colour channels and the colours are highly dependent as shown by high correlation.\nSometimes viewing the data in 3D can help since planar projections can lose some nuances of the data. We can display 3D plots using the mplot3d package.\n\nfrom mpl_toolkits import mplot3d\n\nfig = plt.figure(figsize=(10,8))\nax = plt.axes(projection='3d')\n\nax.set_xlabel(\"Red\")\nax.set_ylabel(\"Green\")\nax.set_zlabel(\"Blue\",labelpad=-2,rotation=90) # Rotate the z-axis label to fit\n\nax.scatter(pixels_sample[\"Red\"], pixels_sample[\"Green\"], pixels_sample[\"Blue\"], c=pixels_sample[\"colour\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAssigning points to clusters\nNow we can begin the clustering. We will be using a method called k-means clustering. It works by initialising K cluster centres, assigning each data point (pixel in this case) to the nearest centre. It then works through the data set, assigning each pixel and updating the clusters if needed. The process continues until no more updates to the clusters are required.\nWe can run the clustering and display our cluster centres using the code below:\n\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=5, n_init=\"auto\").fit(pixels_sample[[\"Red\", \"Green\", \"Blue\"]])\nplt.imshow([kmeans.cluster_centers_])\nplt.show()\n\n\n\n\n\n\n\n\nNow we have our clusters which we can use as our representative clusters for the image. We can now go through the original image and assign each pixel to a cluster that it is nearest to.\n\nlabels = kmeans.predict(pixels[[\"Red\", \"Green\", \"Blue\"]])\nlabels\n\narray([1, 1, 1, ..., 4, 4, 4], shape=(270000,), dtype=int32)\n\n\n To quantify these predictions we can plot a bar graph to show how many pixels are in each cluster\n\nfrom pandas import DataFrame, Series\n\nSeries(labels).value_counts(sort=False).plot.bar(color=kmeans.cluster_centers_)\nplt.show()\n\n\n\n\n\n\n\n\nTo assess how well the algorithm has clustered the images, we can plot the clustered pixels next to the original image to observe the differences.\nFirst we need to convert our clustered pixels back to a 3D array (width x height x colour):\n\nreduced = np.array([kmeans.cluster_centers_[p] for p in labels]).reshape(original_shape)\n\nWe can then plot the images next to each other.\n\nf, axarr = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(18, 9))\naxarr[0].imshow(img)\naxarr[0].set_title(\"Original\")\naxarr[1].imshow(reduced)\naxarr[1].set_title(\"RGB clustered\")\nplt.show()\n\n\n\n\n\n\n\n\n Here it has done a pretty good job at identifying the main darker spots, but finer features such as hair are undefined. We could try to improve this by increasing the number of clusters.\n\nWhat number of clusters do you think are sufficient to define all features?\nSet the number of clusters to 2. Does this sufficiently distinguish between mole and not mole?\n\n\n\n\n\n\n\nOptional Exercise\n\n\n\nTake a look at this other image clustering exercise. It explains different methods for colouring space. Have a go at using and comparing the different methods.",
    "crumbs": [
      "Discussion",
      "Medical image clustering"
    ]
  },
  {
    "objectID": "extra/medical-image-clustering.html#clustering-multiple-images",
    "href": "extra/medical-image-clustering.html#clustering-multiple-images",
    "title": "Medical image clustering",
    "section": "Clustering Multiple Images",
    "text": "Clustering Multiple Images\nIn the next step we are going to use this clustering technique to measure the surface area of dermatological features in a set of 3 images. This could be used as a technique to measure features in a large data set that are hard or tedious to quantify.\nFirst we need to load and inspect the images in the same way we did in the previous exercise\n\nfrom PIL import Image\nimport numpy as np\nfrom skimage import io\nimport matplotlib.pyplot as plt\n\n#image 1\nimg1  = Image.open(\"extra/data/mole.jpg\") #make sure to specify your own working directory\nimg1 = np.array(img1)\nprint('Image 1 shape is', img1.shape)\nprint(\"Image 1 size is\", img1.size)\n\nio.imshow(img1)\nplt.show()\n\n#image 2\nimg2  = Image.open(\"extra/data/mole2.jpg\") #make sure to specify your own working directory\nimg2 = np.array(img2)\nprint('Image 2 shape is', img2.shape)\nprint(\"Image 2 size is\", img2.size)\n\nio.imshow(img2)\nplt.show()\n\n#image 3\nimg3  = Image.open(\"extra/data/mole3.jpg\") #make sure to specify your own working directory\nimg3 = np.array(img3)\nprint('Image 3 shape is', img3.shape)\nprint(\"Image 3 size is\", img3.size)\n\nio.imshow(img3)\nplt.show()\n\nImage 1 shape is (450, 600, 3)\nImage 1 size is 810000\n\n\n/tmp/ipykernel_2809/795262317.py:12: FutureWarning: `imshow` is deprecated since version 0.25 and will be removed in version 0.27. Please use `matplotlib`, `napari`, etc. to visualize images.\n  io.imshow(img1)\n\n\n\n\n\n\n\n\n\nImage 2 shape is (450, 600, 3)\nImage 2 size is 810000\n\n\n/tmp/ipykernel_2809/795262317.py:21: FutureWarning: `imshow` is deprecated since version 0.25 and will be removed in version 0.27. Please use `matplotlib`, `napari`, etc. to visualize images.\n  io.imshow(img2)\n\n\n\n\n\n\n\n\n\nImage 3 shape is (450, 600, 3)\nImage 3 size is 810000\n\n\n/tmp/ipykernel_2809/795262317.py:30: FutureWarning: `imshow` is deprecated since version 0.25 and will be removed in version 0.27. Please use `matplotlib`, `napari`, etc. to visualize images.\n  io.imshow(img3)\n\n\n\n\n\n\n\n\n\nWe also need to flatten them and convert into a pandas data frames\n\nfrom pandas import DataFrame\nfrom matplotlib import colors\n\n#image 1\nimg1 = np.array(img1, dtype=np.float64) / 255  # Scale values\nw, h, d = original_shape = tuple(img1.shape)  # Get the current shape\nimg1_array = np.reshape(img1, (w * h, d))  # Reshape to to 2D\n\npixels1 = DataFrame(img1_array, columns=[\"Red\", \"Green\", \"Blue\"]) #convert to pandas\npixels1[\"colour\"] = [colors.to_hex(p) for p in img1_array] #add colours column \nprint(pixels1)\n\n#image 2\nimg2 = np.array(img2, dtype=np.float64) / 255  # Scale values\nw, h, d = original_shape = tuple(img2.shape)  # Get the current shape\nimg2_array = np.reshape(img2, (w * h, d))  # Reshape to to 2D\n\npixels2 = DataFrame(img2_array, columns=[\"Red\", \"Green\", \"Blue\"]) #convert to pandas\npixels2[\"colour\"] = [colors.to_hex(p) for p in img2_array] #add colours column \nprint(pixels2)\n\n#image 3\nimg3 = np.array(img3, dtype=np.float64) / 255  # Scale values\nw, h, d = original_shape = tuple(img3.shape)  # Get the current shape\nimg3_array = np.reshape(img3, (w * h, d))  # Reshape to to 2D\n\npixels3 = DataFrame(img3_array, columns=[\"Red\", \"Green\", \"Blue\"]) #convert to pandas\npixels3[\"colour\"] = [colors.to_hex(p) for p in img3_array] #add colours column \nprint(pixels3)\n\n             Red     Green      Blue   colour\n0       0.949020  0.662745  0.698039  #f2a9b2\n1       0.964706  0.678431  0.713725  #f6adb6\n2       0.964706  0.678431  0.713725  #f6adb6\n3       0.964706  0.686275  0.717647  #f6afb7\n4       0.968627  0.690196  0.705882  #f7b0b4\n...          ...       ...       ...      ...\n269995  0.847059  0.572549  0.611765  #d8929c\n269996  0.850980  0.576471  0.607843  #d9939b\n269997  0.847059  0.572549  0.603922  #d8929a\n269998  0.847059  0.564706  0.607843  #d8909b\n269999  0.850980  0.568627  0.611765  #d9919c\n\n[270000 rows x 4 columns]\n             Red     Green      Blue   colour\n0       0.890196  0.560784  0.650980  #e38fa6\n1       0.894118  0.564706  0.654902  #e490a7\n2       0.886275  0.556863  0.647059  #e28ea5\n3       0.874510  0.564706  0.639216  #df90a3\n4       0.878431  0.568627  0.654902  #e091a7\n...          ...       ...       ...      ...\n269995  0.843137  0.560784  0.647059  #d78fa5\n269996  0.823529  0.556863  0.639216  #d28ea3\n269997  0.839216  0.576471  0.643137  #d693a4\n269998  0.835294  0.572549  0.639216  #d592a3\n269999  0.835294  0.584314  0.647059  #d595a5\n\n[270000 rows x 4 columns]\n             Red     Green      Blue   colour\n0       0.905882  0.576471  0.627451  #e793a0\n1       0.913725  0.592157  0.639216  #e997a3\n2       0.913725  0.592157  0.639216  #e997a3\n3       0.925490  0.596078  0.647059  #ec98a5\n4       0.925490  0.596078  0.647059  #ec98a5\n...          ...       ...       ...      ...\n269995  0.827451  0.560784  0.525490  #d38f86\n269996  0.831373  0.564706  0.521569  #d49085\n269997  0.839216  0.580392  0.525490  #d69486\n269998  0.839216  0.580392  0.525490  #d69486\n269999  0.823529  0.576471  0.509804  #d29382\n\n[270000 rows x 4 columns]\n\n\n\n\n\n\n\n\nExercise\n\n\n\nAt the moment we are loading and editing each image seperately - this can be time consuming with large data sets. How can we make this process faster?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nimport os\n\n# Set a directory variable name containing images\nimage_dir = \"extra/data\"\n\n# List all image files in the directory\nimage_files = [file for file in os.listdir(image_dir) if file.endswith((\".jpg\", \".png\", \".jpeg\"))]\nprint(f'These are the image file names in the current working directory: {image_files}') # View file names\n\n# Loop through images\nfor file in image_files:\n    img_path = os.path.join(image_dir, file)\n    img = io.imread(img_path)  # Load the image\n    \n    # Normalize pixel values (scale between 0 and 1)\n    img = np.array(img, dtype=np.float64) / 255  \n\n    # Get the shape of the image\n    w, h, d = img.shape  \n\n    # Reshape into a 2D array of pixels\n    img_array = np.reshape(img, (w * h, d))  \n\n    # Convert to a Pandas DataFrame\n    pixels = DataFrame(img_array, columns=[\"Red\", \"Green\", \"Blue\"])  \n   \n    pixels[\"colour\"] = [colors.to_hex(p) for p in img_array]  # Convert RGB to HEX colors\n    print(pixels.head())\n\nThese are the image file names in the current working directory: ['mole.jpg', 'mole3.jpg', 'mole2.jpg']\n        Red     Green      Blue   colour\n0  0.949020  0.662745  0.698039  #f2a9b2\n1  0.964706  0.678431  0.713725  #f6adb6\n2  0.964706  0.678431  0.713725  #f6adb6\n3  0.964706  0.686275  0.717647  #f6afb7\n4  0.968627  0.690196  0.705882  #f7b0b4\n        Red     Green      Blue   colour\n0  0.905882  0.576471  0.627451  #e793a0\n1  0.913725  0.592157  0.639216  #e997a3\n2  0.913725  0.592157  0.639216  #e997a3\n3  0.925490  0.596078  0.647059  #ec98a5\n4  0.925490  0.596078  0.647059  #ec98a5\n        Red     Green      Blue   colour\n0  0.890196  0.560784  0.650980  #e38fa6\n1  0.894118  0.564706  0.654902  #e490a7\n2  0.886275  0.556863  0.647059  #e28ea5\n3  0.874510  0.564706  0.639216  #df90a3\n4  0.878431  0.568627  0.654902  #e091a7\n\n\n\n\n\nTo run the clustering on a set of images we need to stack the data frames into one. We will add a new column to indicate which image the data came from.\n\n# First we need to add a new column to our data sets to indicate which image they came from \nimport pandas as pd\n\npixels1['image'] = '1'\npixels2['image'] = '2'\npixels3['image'] = '3'\n\npixels = pd.concat([pixels1, pixels2, pixels3], ignore_index = True, axis = 0) #axis 0 indicates stacking verticallu\n\nprint(pixels)\n\n             Red     Green      Blue   colour image\n0       0.949020  0.662745  0.698039  #f2a9b2     1\n1       0.964706  0.678431  0.713725  #f6adb6     1\n2       0.964706  0.678431  0.713725  #f6adb6     1\n3       0.964706  0.686275  0.717647  #f6afb7     1\n4       0.968627  0.690196  0.705882  #f7b0b4     1\n...          ...       ...       ...      ...   ...\n809995  0.827451  0.560784  0.525490  #d38f86     3\n809996  0.831373  0.564706  0.521569  #d49085     3\n809997  0.839216  0.580392  0.525490  #d69486     3\n809998  0.839216  0.580392  0.525490  #d69486     3\n809999  0.823529  0.576471  0.509804  #d29382     3\n\n[810000 rows x 5 columns]\n\n\n\n\n\n\n\n\nOptional Exercise\n\n\n\nExplore the data in the same way as the previous exercise. Do you notice anything from the correlations?\n\n\nWe can now run K-means clustering in the same way as before. First by sampling a subset of pixels and assign our cluster centres. In order to compare the size of the dermatological features across the images, we want to measure the pixels that are and aren’t dark space. We could try to use 2 clusters for this.\n\npixels_sample = pixels.sample(frac=0.05) #look at 5% of the data\n\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=2, n_init=\"auto\").fit(pixels_sample[[\"Red\", \"Green\", \"Blue\"]])\nplt.imshow([kmeans.cluster_centers_])\nplt.show()\n\n\n\n\n\n\n\n\nWe can now run our clustering with our whole data set.\n\nlabels = kmeans.predict(pixels[[\"Red\", \"Green\", \"Blue\"]])\nlabels\n\narray([1, 1, 1, ..., 1, 1, 1], shape=(810000,), dtype=int32)\n\n\nNow we have used the clustering algorithm to cluster pixels into dark and light areas, we can manipulate our data frame to compare the size of the features in the images.\nFirst we want to add the clustering array to the pandas data frame\n\n# Add the clustering array as a new column \n\npixels['cluster'] = labels.tolist()\nprint(pixels)\n\n             Red     Green      Blue   colour image  cluster\n0       0.949020  0.662745  0.698039  #f2a9b2     1        1\n1       0.964706  0.678431  0.713725  #f6adb6     1        1\n2       0.964706  0.678431  0.713725  #f6adb6     1        1\n3       0.964706  0.686275  0.717647  #f6afb7     1        1\n4       0.968627  0.690196  0.705882  #f7b0b4     1        1\n...          ...       ...       ...      ...   ...      ...\n809995  0.827451  0.560784  0.525490  #d38f86     3        1\n809996  0.831373  0.564706  0.521569  #d49085     3        1\n809997  0.839216  0.580392  0.525490  #d69486     3        1\n809998  0.839216  0.580392  0.525490  #d69486     3        1\n809999  0.823529  0.576471  0.509804  #d29382     3        1\n\n[810000 rows x 6 columns]\n\n\nWe can then plot the amount of dark space for each image\n\n# Filter only rows where cluster == 0\npixels_fil = pixels[pixels[\"cluster\"] == 1]\n\n# Count occurrences of cluster 0 per image\ndark_counts= pixels_fil[\"image\"].value_counts()\n\n# Plot the bar chart\nplt.figure(figsize=(8, 5))\ndark_counts.plot(kind=\"bar\")\nplt.show()\n\n\n\n\n\n\n\n\nIn a real data set we would have lots of different images. Using clustering this way would save time in measuring images. Can you think of other applications relevant to your research?",
    "crumbs": [
      "Discussion",
      "Medical image clustering"
    ]
  },
  {
    "objectID": "extra/medical-image-clustering.html#data-augmentation",
    "href": "extra/medical-image-clustering.html#data-augmentation",
    "title": "Medical image clustering",
    "section": "Data augmentation",
    "text": "Data augmentation\nWe are moving on to a different aspect of clustering images addressing the problem of limited image data.\nMostly we have access to limited medical image data. This amount of data is insufficient to train a machine learning algorithm that is good enough to decipher between diseased and not diseased. Therefore, we will explore alternative strategies of creating more data from existing dataset to boost model performance.\nData augmentation is a technique of artificially increasing the training set by creating modified copies of a dataset using existing data. It is aimed at creating new data points, manipulating existing data to increase the size and diversity of a dataset. An example use case is in image analysis for health care where it helps improve diagnostic models that detect, recognize, and diagnose diseases based on images. The creation of an augmented image provides more training data for models, especially for rare diseases that lack source data variations. The production and use of synthetic patient data advances medical research while respecting all data privacy considerations.\n\n\n\n\n\n\nWhy is data augmentation important?\n\n\n\n\nEnhanced Model performance.\nData augmentation techniques help enrich datasets by creating many variations of existing data. This provides a larger dataset for training and enables a model to encounter more diverse features. The augmented data helps the model better generalize to unseen data and improve its overall performance in real-world environments.\nTo prevent models from overfitting.\nOverfitting is the undesirable ML behavior where a model can accurately provide predictions for training data but it struggles with new data. If a model trains only with a narrow dataset, it can become overfit and can give predictions related to only that specific data type.\nThe initial training set is too small.\nTo improve the model accuracy.\nIf you need to train a deep learning model on sensitive data, you can use augmentation techniques on the existing data to create synthetic data.\nTo reduce the operational cost of labeling and cleaning the raw dataset.\nData augmentation techniques increase the effectiveness of smaller datasets, vastly reducing the dependency on large datasets in training environments.\n\n\n\nHow do you achieve Image data augmentation?\n\nIn image processing, applying geomteric transformations like rotations, flips, or color adjustments to existing images can help models generalize better.\nOther transformations are:\n\nColor space transformations: randomly change RGB color channels, contrast, and brightness.\nKernel filters: randomly change the sharpness or blurring of the image.\nRandom erasing: delete some part of the initial image.\nMixing images: blending and mixing multiple images.\n\n\n\nGeometric Transformations\nIn this session we will focus on geometric transformations only including: rotating images, flipping and blurring the images.\n\n# Load the necessary libraries\nimport skimage\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport numpy as np\n\nfrom skimage import data   ## Inbuilt dataset within scikit-image\nfrom skimage import io, transform, filters, exposure\nmatplotlib.rcParams['font.size'] = 18\n\n\n# We are working with one file at a time for now.\n# Read in the image data\nimage = io.imread(\"extra/data/mole.jpg\") \n\n\nRotate the image\n\n# Rotate the image\nrotated = transform.rotate(image, 90)  # Rotate 90 degrees\nplt.imshow(rotated)\n\n\n\n\n\n\n\n\n\n\nFlip the image horizontally\n\n# Flip the image horizontally\nflipped_h = image[:, ::-1]\nplt.imshow(flipped_h) \n\n\n\n\n\n\n\n\n\n\nFlip the image vertically\n\nflipped_v = image[::-1, :]  \nplt.imshow(flipped_v)\n\n\n\n\n\n\n\n\n\n\nBlur the image\n\n# Blur the image\nblurred = filters.gaussian(image, sigma=2)\nplt.imshow(blurred)",
    "crumbs": [
      "Discussion",
      "Medical image clustering"
    ]
  },
  {
    "objectID": "extra/medical-image-clustering.html#visualise-all-images-including-the-original-image",
    "href": "extra/medical-image-clustering.html#visualise-all-images-including-the-original-image",
    "title": "Medical image clustering",
    "section": "Visualise all images including the original image",
    "text": "Visualise all images including the original image\n\n# Visualise the original and modified forms of the image all together\nfig, axes = plt.subplots(1, 5, figsize=(15, 5))\ntitles = [\"Original\", \"Rotated\", \"Flipped H\", \"Flipped V\", \"Blurred\"]\nimages = [image, rotated, flipped_h, flipped_v, blurred]\n\nfor ax, title, image in zip(axes, titles, images):\n    ax.imshow(image, cmap=\"gray\")\n    ax.set_title(title)\n    ax.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nWe have been reading in one image at a time and applying the transformations on each image at a time. Can you work out how to process mutiple images all at once? Consider using the three image files provided.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nimport os\nimport matplotlib.pyplot as plt\nfrom skimage import io, transform, filters\n\n# Set a directory variable name containing images\nimage_dir = \"extra/data/\"\n\n# List all image files in the directory\nimage_files = [file for file in os.listdir(image_dir) if file.endswith((\".jpg\", \".png\", \".jpeg\"))]\nprint(image_files) \n\n# Loop through images\nfor file in image_files:\n    img_path = os.path.join(image_dir, file)\n    img = io.imread(img_path)  # Load the image\n\n    # Apply transformations\n    rotated = transform.rotate(img, 90)\n    flipped_h = img[:, ::-1]  # Flip horizontally\n    flipped_v = img[::-1, :]  # Flip vertically\n    blurred = filters.gaussian(img, sigma=2)\n\n    # Visualize transformations\n    fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n    titles = [\"Original\", \"Rotated\", \"Flipped H\", \"Flipped V\", \"Blurred\"]\n    images = [img, rotated, flipped_h, flipped_v, blurred]\n\n    for ax, title, image in zip(axes, titles, images):\n        ax.imshow(image)\n        ax.set_title(title)\n        ax.axis(\"off\")\n\n    plt.suptitle(f\"Transformations for {file}\")\n    plt.show()\n\n['mole.jpg', 'mole3.jpg', 'mole2.jpg']",
    "crumbs": [
      "Discussion",
      "Medical image clustering"
    ]
  },
  {
    "objectID": "extra/medical-image-clustering.html#resources",
    "href": "extra/medical-image-clustering.html#resources",
    "title": "Medical image clustering",
    "section": "Resources",
    "text": "Resources\n\nScikit-image\nData augmentation using AWS",
    "crumbs": [
      "Discussion",
      "Medical image clustering"
    ]
  },
  {
    "objectID": "extra/prediction-alzheimer-disease.html",
    "href": "extra/prediction-alzheimer-disease.html",
    "title": "Prediction of Alzheimer’s disease",
    "section": "",
    "text": "You may need to install some libraries to run this practical. If you are using Anaconda the recommended way to install it is with\nconda install openpyxl\nAlternatively, you can use pip as in\npython -m pip install -U pip\npython -m pip install -U openpyxl",
    "crumbs": [
      "Discussion",
      "Prediction of Alzheimer's disease"
    ]
  },
  {
    "objectID": "extra/prediction-alzheimer-disease.html#goal",
    "href": "extra/prediction-alzheimer-disease.html#goal",
    "title": "Prediction of Alzheimer’s disease",
    "section": "Goal",
    "text": "Goal\nUse the Bader et al. 2020 dataset to train a machine learning model which can test whether NPTX2 (identified as AD biomarker in Shen et al. 2024) can be used to predict whether a patient has AD using CSF samples.",
    "crumbs": [
      "Discussion",
      "Prediction of Alzheimer's disease"
    ]
  },
  {
    "objectID": "extra/prediction-alzheimer-disease.html#background",
    "href": "extra/prediction-alzheimer-disease.html#background",
    "title": "Prediction of Alzheimer’s disease",
    "section": "Background",
    "text": "Background\nAlzheimer’s disease (AD) is a neurodegenerative disorder affecting millions of people worldwide. Approximately 1% of AD cases are classified as autosomal dominant Alzheimer’s disease (ADAD), meaning they are caused by mutations to the genes PSEN1, PSEN2, or APP. Almost all people with these mutations will go on to develop AD, typically with a younger onset than those with sporadic Alzheimer’s disease (sAD). With the genetic cause of ADAD generally understood but many causes of sAD unclear, studying the molecular basis of ADAD disease progression can inform how sAD may develop.\nProteomics is an approach which involves looking at the entire complement of proteins expressed within a cell, tissue, or organism. Proteomics experiments can generate powerful molecular insights, such as by showing which proteins are up- or downregulated in different diseases. These experiments often generate very large datasets, which can be overwhelming to analyse. However, data science approaches such as machine learning can greatly enhance our ability to extract useful information from these datasets.\nA recent paper showcases how machine learning can be applied to proteomics datasets https://doi.org/10.1016/j.cell.2024.08.049.\nUsing a cerebrospinal fluid (CSF) proteomics database taken from mutation carriers or non-carriers for ADAD, the authors sought to find biomarkers for the disease. The authors identify six proteins which can be used to predict with high accuracy whether a patient is a mutation carrier or a non-carrier for ADAD. One of these proteins is NPTX2.\nThe study also reveals that many ADAD-associated proteins are associated with sAD as well, which could represent a large step forwards in diagnosis of sAD (the more common form of AD).\nYour task is to test whether this proposed biomarker for ADAD can also predict whether a patient has developed sAD.",
    "crumbs": [
      "Discussion",
      "Prediction of Alzheimer's disease"
    ]
  },
  {
    "objectID": "extra/prediction-alzheimer-disease.html#questions",
    "href": "extra/prediction-alzheimer-disease.html#questions",
    "title": "Prediction of Alzheimer’s disease",
    "section": "Questions",
    "text": "Questions\nPatient X, aged 70, has visited their doctor with cognitive symptoms. After taking a CSF sample and performing mass spectrometry, the abundance of NPTX2 was detected as 321700.\n\nUsing the data from a publicly available database of CSF samples taken from healthy or AD patients, create an appropriate ML model to predict whether Patient X has Alzheimer’s Disease. Comment on why you chose this model and how reliable it is.\nPatient X does have AD. Did your model predict this? If not, why might this be?",
    "crumbs": [
      "Discussion",
      "Prediction of Alzheimer's disease"
    ]
  },
  {
    "objectID": "extra/prediction-alzheimer-disease.html#dataset",
    "href": "extra/prediction-alzheimer-disease.html#dataset",
    "title": "Prediction of Alzheimer’s disease",
    "section": "Dataset",
    "text": "Dataset\nThe complete data can be found in the supplementary files below:\n\nProteomics data\nPatient data\n\nThis data requires pre-processing. You may like to do this yourself as an exercise in tidying data, or you can use the processed form EMBO_Bader_2020_AD_proteomics_NPTX2_tidy.xlsx.\n\n\n\n\n\n\nPre-processing of the data\n\n\n\n\n\nThis was challenging for three main complexities intrinsic to the original Excel spreadsheets found in the paper:\n\nThe 1st spreadsheet in the supplementary info of the paper had all the protein abundances from the ~200 patients. However, there was no other information in this spreadsheet about the patients (not even disease status). Information about the patients was in a 2nd spreadsheet. This second spreadsheet contained all the information including age, disease status, collection site, and gender. However, some patients had been deleted so that this 2nd spreadsheet had fewer patients, requiring the patients to be matched using the ‘sample name’ column.\nMissing values - a couple of the patients had missing values for NPTX2 (written as ‘filtered’). For simplicity in this demonstration, I deleted these entries. However, this can introduce artifacts - see this course for more information on how to handle missing values:\nIn the 1st spreadsheet (proteomics dataset), 1000s proteins were all listed, with each protein in a separate row and each patient (with codified ‘sample name’) in a separate column. To tidy the data, I made a column for each variable, or feature (AD status, NPTX2 abundance, age, gender), with each patient, or observation, in a row.",
    "crumbs": [
      "Discussion",
      "Prediction of Alzheimer's disease"
    ]
  },
  {
    "objectID": "extra/prediction-alzheimer-disease.html#worked-example-of-analysis",
    "href": "extra/prediction-alzheimer-disease.html#worked-example-of-analysis",
    "title": "Prediction of Alzheimer’s disease",
    "section": "Worked example of analysis",
    "text": "Worked example of analysis\nSee below for a worked example, with additional exercises and questions to consider.\n\nImporting pandas and reading Excel file\n\nimport pandas as pd\n\nNPTX2_tidy = pd.read_excel(\"extra/data/EMBO_Bader_2020_AD_proteomics_NPTX2_tidy.xlsx\")\nNPTX2_tidy.head()\n\n\n\n\n\n\n\n\nPatient\nAge\nGender\nCollection site\nAD status\nNPTX2 abundance\nsample name\n\n\n\n\n0\nPatient 1\n71\nf\nSweden\ncontrol\n226437.734375\n20180618_QX0_JaBa_SA_LC12_5_CSF1_1_8-1xD1xS1fM...\n\n\n1\nPatient 2\n77\nm\nSweden\nAD\n123887.046875\n20180618_QX0_JaBa_SA_LC12_5_CSF1_1_8-1xD1xS1fM...\n\n\n2\nPatient 3\n75\nm\nSweden\nAD\n391305.937500\n20180618_QX0_JaBa_SA_LC12_5_CSF1_1_8-1xD1xS1fM...\n\n\n3\nPatient 4\n72\nf\nSweden\nAD\n296838.531250\n20180618_QX0_JaBa_SA_LC12_5_CSF1_1_8-1xD1xS1fM...\n\n\n4\nPatient 5\n63\nf\nSweden\nAD\n211497.703125\n20180618_QX0_JaBa_SA_LC12_5_CSF1_1_8-1xD1xS1fM...\n\n\n\n\n\n\n\n\n\nImporting seaborn and visualising data\n\nimport seaborn as sns\n\nTo compare NPTX2 abundance in control vs AD groups open the data in Jupyter Notebooks. Use a suitable plot to visualise the distribution of NPTX2 abundance between the control and AD patients. What do you notice and what does this indicate about the suitability of this protein as a biomarker?\n\nsns.catplot(data=NPTX2_tidy, x=\"AD status\", y=\"NPTX2 abundance\")\n\n\n\n\n\n\n\n\nFrom this initial glance, it is apparent that the two groups have similar distributions of NPTX2 abundance, which indicates that this may not be a good biomarker for AD status in this dataset, as we would expect AD patients to have higher levels of NPTX2.\n\n\nFurther data analysis\nHow are the different variables distributed (e.g. AD status, age, within different collection sites)?\n\nNPTX2_tidy.dtypes #Checking data types. Object = categorical\n\nx=NPTX2_tidy[\"AD status\"]\ny=NPTX2_tidy[\"NPTX2 abundance\"]\n\nNPTX2_tidy.value_counts(x) \n\nAD status\ncontrol    108\nAD          87\nName: count, dtype: int64\n\n\nThis reveals that there are more AD than control patients. Is this important?\n\nNPTX2_tidy.groupby(by=[\"AD status\"]).mean(numeric_only =True) #Grouping by AD status to see how all numerical conditions vary across AD vs control\n\n\n\n\n\n\n\n\nAge\nNPTX2 abundance\n\n\nAD status\n\n\n\n\n\n\nAD\n71.885057\n214289.209052\n\n\ncontrol\n64.407407\n217592.649342\n\n\n\n\n\n\n\nLooking at the mean ages and NPTX2 abundances shows that the NPTX2 is very similar between the 2 groups (as mentioned earlier). Also, there is difference in age between the two groups. How might this affect our interpretation of the data?\n\nsns.catplot(data=NPTX2_tidy, x=\"AD status\", y=\"Age\", hue=\"AD status\")\n\n\n\n\n\n\n\n\nThis scatterplot also shows that the control group has a population of younger patients not present in the AD group. Let’s see if that population clusters with another variable (e.g. collection site)\n\npd.crosstab(index=NPTX2_tidy[\"AD status\"], columns=NPTX2_tidy[\"Gender\"]) #The cross-tabulation allows you to look at the frequency distribution of multiple categorical variables, and see how they are related to one another\n\nsns.catplot(data=NPTX2_tidy, x=\"AD status\", y=\"Age\", hue=\"Collection site\")\n\npd.crosstab(index=NPTX2_tidy[\"AD status\"], columns=NPTX2_tidy[\"Collection site\"])\n\n\n\n\n\n\n\nCollection site\nBerlin\nKiel\nMagdeburg\nSweden\n\n\nAD status\n\n\n\n\n\n\n\n\nAD\n32\n0\n26\n29\n\n\ncontrol\n50\n16\n12\n30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis shows that all of the patients from the Kiel subset are control – there are no AD patients – and this cohort is disproportionately younger than the other chorts. How might this affect the dataset, or the way that we analyse it?\nAs an extension task, after you fit your model the first time, try it without the Kiel dataset. Does this affect your result?\n\ndf = pd.DataFrame(NPTX2_tidy)\ndf.head()\n\n\n\n\n\n\n\n\nPatient\nAge\nGender\nCollection site\nAD status\nNPTX2 abundance\nsample name\n\n\n\n\n0\nPatient 1\n71\nf\nSweden\ncontrol\n226437.734375\n20180618_QX0_JaBa_SA_LC12_5_CSF1_1_8-1xD1xS1fM...\n\n\n1\nPatient 2\n77\nm\nSweden\nAD\n123887.046875\n20180618_QX0_JaBa_SA_LC12_5_CSF1_1_8-1xD1xS1fM...\n\n\n2\nPatient 3\n75\nm\nSweden\nAD\n391305.937500\n20180618_QX0_JaBa_SA_LC12_5_CSF1_1_8-1xD1xS1fM...\n\n\n3\nPatient 4\n72\nf\nSweden\nAD\n296838.531250\n20180618_QX0_JaBa_SA_LC12_5_CSF1_1_8-1xD1xS1fM...\n\n\n4\nPatient 5\n63\nf\nSweden\nAD\n211497.703125\n20180618_QX0_JaBa_SA_LC12_5_CSF1_1_8-1xD1xS1fM...\n\n\n\n\n\n\n\n\n\nClassification\nHere we will use k-nearest neighbours for predicting categories of data (AD or healthy). To use it, we need to encode our data into a binary form which the algorithms can process:\n\ndf_encoded = pd.get_dummies(df[\"AD status\"])\ndf_encoded .head()\n\n\n\n\n\n\n\n\nAD\ncontrol\n\n\n\n\n0\nFalse\nTrue\n\n\n1\nTrue\nFalse\n\n\n2\nTrue\nFalse\n\n\n3\nTrue\nFalse\n\n\n4\nTrue\nFalse\n\n\n\n\n\n\n\nNow concatenating the two data frames:\n\nbinary = pd.concat([df_encoded, df], axis=1)\nbinary.head()\n\n\n\n\n\n\n\n\nAD\ncontrol\nPatient\nAge\nGender\nCollection site\nAD status\nNPTX2 abundance\nsample name\n\n\n\n\n0\nFalse\nTrue\nPatient 1\n71\nf\nSweden\ncontrol\n226437.734375\n20180618_QX0_JaBa_SA_LC12_5_CSF1_1_8-1xD1xS1fM...\n\n\n1\nTrue\nFalse\nPatient 2\n77\nm\nSweden\nAD\n123887.046875\n20180618_QX0_JaBa_SA_LC12_5_CSF1_1_8-1xD1xS1fM...\n\n\n2\nTrue\nFalse\nPatient 3\n75\nm\nSweden\nAD\n391305.937500\n20180618_QX0_JaBa_SA_LC12_5_CSF1_1_8-1xD1xS1fM...\n\n\n3\nTrue\nFalse\nPatient 4\n72\nf\nSweden\nAD\n296838.531250\n20180618_QX0_JaBa_SA_LC12_5_CSF1_1_8-1xD1xS1fM...\n\n\n4\nTrue\nFalse\nPatient 5\n63\nf\nSweden\nAD\n211497.703125\n20180618_QX0_JaBa_SA_LC12_5_CSF1_1_8-1xD1xS1fM...\n\n\n\n\n\n\n\nVisualising the data, plotting age against NPTX2 abundance\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\nsns.scatterplot(data=binary, x=\"Age\", y=\"NPTX2 abundance\", hue=\"AD\", palette=\"Dark2\")\n\nX = binary[[\"Age\", \"NPTX2 abundance\"]]\ny = binary[\"AD\"]\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y)\n\nmodel = KNeighborsClassifier(n_neighbors=10)\nmodel.fit(train_X, train_y)\n\nKNeighborsClassifier(n_neighbors=10)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifier?Documentation for KNeighborsClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_neighbors \n10\n\n\n\nweights \n'uniform'\n\n\n\nalgorithm \n'auto'\n\n\n\nleaf_size \n30\n\n\n\np \n2\n\n\n\nmetric \n'minkowski'\n\n\n\nmetric_params \nNone\n\n\n\nn_jobs \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\n\n\n\n\n\nI am asking the question “Is patient y 0 or 1 (negative or positive) for AD?”\n\npred = pd.DataFrame({\"Age\": [70], \"NPTX2 abundance\":[297500]})  # Make a new DataFrame containing the X values\npred[\"AD\"] = model.predict(pred)  # Make a prediction and add that data into the table\npred\n\n\n\n\n\n\n\n\nAge\nNPTX2 abundance\nAD\n\n\n\n\n0\n70\n297500\nTrue\n\n\n\n\n\n\n\nNow test the reliability of your model. What is your model score? Would you use this model on real patients? Comment on the quality of the data you used in this exercise.\nNow testing the model:\n\nmodel.score(test_X, test_y)\n\n0.5306122448979592",
    "crumbs": [
      "Discussion",
      "Prediction of Alzheimer's disease"
    ]
  }
]